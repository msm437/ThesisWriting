% !TEX root = ../thesis-example.tex
%
\section{Device- and System-independent Personal Touchless User Interface for Multi-Display}
\label{sec:4-IPCAI}
As the computing technologies develop, more and more information is digitized from the physical world, but the mainstream display technology is still 2D display.
Researchers work on how to enable the user to perceive and interact with the information naturally and effectively.
As shown in \figurename{\ref{fig:1-intro:ORScenario}}, in the modern day operating room the surgeon performs surgeries with the support of different medical systems that showcase patient information, physiological data, and medical images. 
Surgeons rely on these medical systems to capture, browse, and manipulate patient information, physiological data, and medical images, but the current control input of these systems are still mouse, keyboard, touch screen, and joystick. 
To retrieve the desired information, the user has to press either a physical/virtual button using their own finger or a visual cursor controlled by another device. 
Yet, one of the most important rules in the OR is to maintain a strict boundary between what is sterile and what is not \cite{OHara2014a}.

In this section, a novel user interface is developed that allows the surgeon to personally perform touchless interaction with the various medical systems, switch effortlessly among them, all of this without modifying the systems' software and hardware.
To achieve this, a wearable RGB-D sensor is mounted on the surgeon's head for inside-out tracking of their finger with any of the medical systems' displays. Android devices with a special application are connected to the computers on which the medical systems are running, simulating a normal USB mouse and keyboard. When the surgeon performs interaction using pointing gestures, the desired cursor position in the targeted medical system display, and gestures, are transformed into general events and then sent to the corresponding Android device. Finally, the application running on the Android devices generates the corresponding mouse or keyboard events according to the targeted medical system.
To simulate an operating room setting, our unique user interface was tested by 7 medical participants who performed several interactions with {the visualization of CT, MRI, and Fluoroscopy images} at varying distances from them. Results from the System Usability Scale and NASA-TLX workload index indicated a strong acceptance of our proposed user interface.

\subsection{Design and methods} \label{sec:IPCAI:Methods}
Based on the pointing gesture recovery in Section \ref{section:4-PAST}, we propose a solution to give surgeons direct control of the various medical systems' displays inside the OR using only a single user interface via personalized pointing gesture. The collaboration of the Android devices and software allow the combined system to act as a usual mouse or keyboard and facilitates working with any device that does not see a difference in its input/output.

%. gives surgeons direct control over screens and medical systems in their view
\subsubsection{Hardware setup}
We introduce a wearable RGB-D sensor and a wearable computing device for each surgeon, and an Android device for each medical system. 
The proposed hardware setup with one surgeon and one medical system is shown in  \figurename{\ref{fig:Hardware}}.
The wearable RGB-D sensor is connected to the wearable computing device to recover the pointing gesture, and the Android device is connected to the medical system simulating a normal USB mouse and keyboard. The wearable computing and Android devices exchange information via a WiFi router. To simplify the display tracking task, a 2D marker plane is attached to each display, which is connected to different medical system.
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=\textwidth]{figures/4-PointingOR/images/Hardware}
	% figure caption is below the figure
	\caption{Hardware setup. The proposed system include a wearable RGB-D sensor, a wearable computing device, a medical system with one display, and an Android device.}
	\label{fig:Hardware}       % Give a unique label
\end{figure}

\subsubsection{Calibration} \label{sec:OR:calibration}
For the setup there are two parts that require calibration: i) the transformation between the display and its marker, and (ii) the user specific pointing gesture. The calibration methods are shown in \figurename{\ref{fig:Calibration}} and presented below.

\paragraph{Display with markers}
Without modifying any software/hardware of the existing medical systems, this calibration is performed with a marker printed on an A4 paper, as shown in  \figurename{\ref{fig:Calibration}}. 
The A4 paper is affixed closely next to the top-left corner of the display. As the paper is very thin, the printed marker and the display are supposed to be coplanar. With the tracking information of the markers, the transformation matrix of the display relative to the display marker can be calculated. Since the size of the A4 paper and the display are known, the translation of the printed marker relative to the display screen can be calculated. This one-time calibration process is performed offline for each display in the operating room. Then a configuration file is created for each display marker to describe which display it is attached to and which medical system it is connected to this display. 

\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.7\textwidth]{figures/4-PointingOR/Calibration}
	% figure caption is below the figure
	\caption{Calibration methods: (a) A printed marker (1) is placed next to the right-top corner of the display to calculate the transformation matrix between the display marker (2) and the display. }
	%(b) Calibration of the user specific pointing gesture: the user is asked to point at several tracked targets $T_i$ using one finger and the pointing lines $L_i$ which pass through the fingertip $F_i$ and $T_i$ are collected.
	\label{fig:Calibration}       % Give a unique label
\end{figure}
\paragraph{User specific pointing gesture}
In section \ref{sec:4:PAST}, the objective is to calibrate and recover the user specific pointing geometry. The result of the calibration is the position of a virtual eye center $E$ for the user.
As shown in \figurename{ \ref{fig:past:calibration}, the target, $T_i$, is shown somewhere in front of the user, who is then asked to perform a natural pointing gesture towards $T_i$ using one finger. The 3D positions of the fingertips $F_i$ and $T_i$ are collected through the wearable RGB-D sensor. The pointing line $L_i$ is defined such that it passes through $F_i$ and $T_i$. After sufficient pointing lines are collected, the intersection of the lines is calculated defining the user specific virtual eye-center $E$. After calibration the pointing geometry is recovered as $L_{i}$, going through $E$ and  $F_i$, when the pointing gesture is performed.

\subsubsection{System framework}
\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{figures/4-PointingOR/images/Workflow}
	\caption{The framework of the proposed system. Tracking information of objects in the OR after processing is used to control the medical systems. }
	\label{fig:Workflow}       % Give a unique label
\end{figure}
The system framework is shown in \figurename{\ref{fig:Workflow}}.
The left side shows the usual elements in the OR and the right side shows the data-flow of our user interface paradigm. When a surgeon performs interaction with a medical system, the display marker and fingertip can always be detected via the RGB-D sensor as it has a similar view as the surgeon. 
Through the following processing: \textit{Tracking Info}, \textit{Pointing Geometry}, and \textit{Interaction Events}, mouse and keyboard events are generated to control the medical systems.

\paragraph{Tracking Info} 
All the images are perceived via the wearable RGB-D sensor. Therefore, this is an inside-out tracking and does not require an external reference. The fingertip is detected from the depth image using 3D blob extraction and the display marker position is calculated via the color image. The display's position can be calculated based on the calibration result in Section \ref{sec:OR:calibration}.

\paragraph{Pointing Geometry}
Based on the calibration of ``User specific pointing gesture'' and the tracking information,
the pointing geometry between the pointing direction and the display screen plane is recovered. The target position in real world and the desired cursor position on the screen can be calculated. We can also detect some predefined finger and hand gestures. Finally, all these informations are transformed into general events and then sent to the corresponding Android device through UDP (User Datagram Protocol) connections.  

\paragraph{Interaction Events}
Two virtual devices are created in a Linux kernel to simulate USB mouse and keyboard when the Android device is connected to a computer. The two devices send normal initial signals to the host computer and are recognized as USB mouse and keyboard. The special application running on the Android system builds up a UDP connection with the computing devices to receive general events transformed from the wearable computing device and generates the corresponding mouse or keyboard events according to the current medical system. In our general design, all the interactions are implemented via the Android mouse or keyboard as the medical system is taken as a black box. However, the wearable computing device could directly communicate with the medical system when this is allowed. 
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/4-PointingOR/images/MultiUserMultiSystem}
	\caption{A setup with Multi-user Multi-system and the Android system structure. In this setup, there are two surgeons and two medical systems, one of which has two displays. }
	\label{fig:multiUserMultiSystem}       % Give a unique label
\end{figure}

\paragraph{Multi-user Multi-system Scenario}
There can be more than one surgeon and more than one medical system in the operating room scenario, and the proposed setup is shown in \figurename{\ref{fig:multiUserMultiSystem}}. In the Pointing Geometry stage, a personal configuration file should be loaded to implement the multi-user case and personalized figure-pointing gestures. In the configuration file, each member of the surgical team is assigned a priority level according to their responsibility. When Android devices receive information from more than one wearable computing device at the same time, they implement the mouse and keyboard event according to the surgeon with higher priority level. This can solve the issues which arise when these systems are used in the context of an actual surgical procedure, including collaboration, engagement and disengagement. When there are more than one displays connected to a medical system, it still works well as the Android device knows which display is the desired one. Even when there is no display for a medical system, a display marker still can be positioned somewhere to simulate a virtual display, which is logically connected to the medical system. In this case, the surgeon can point at the display marker and perform interaction with the corresponding system.

\paragraph{Personalized Pointing Gesture} 
Performing interactions requires two steps: i) choosing the target, and (ii) releasing the command. Through pointing gestures the surgeon can choose the target naturally via the pointing and immediately perform a finger or hand gesture to release the command. During the calibration procedure, the calibration of user pointing provides the characteristics of a user specific pointing geometry. In addition, since the pointing gesture is personal, pointing characteristics could be saved for each surgeon in order to recognize their gesture and map it to different commands.

\subsection{Evaluation \& results}
The main goal of the user study is to answer the following questions: 
\begin{description}
	\item[1.] To what degree does the surgeon accept the interaction with multiple medical systems via the proposed user interface?
	\item[2.] Does the surgeon find the system a viable alternative to the traditional indirect control of the medical systems and their displays in the modern day operation room?
\end{description}
Here we note that we are not evaluating the gesture accuracy, but we are assessing the Android-based communication with the different systems.

% A \textbf{supplementary video} is uploaded together with this submission that illustrates the concepts of our proposed user interface paradigm.
\subsubsection{Implementation}
\paragraph{Proposed system} 
During the user study, the Intel Realsense Creative camera was chosen as the wearable camera and Microsoft Surface 2 as the computing device, which was put in a backpack. The special Android application was developed and installed on two Nexus 7 devices to simulate the USB mouse and keyboard for each medical system. 
 
\paragraph{Patient scenario}}Anonymous patient is wheeled inside the emergency room following a car accident. A CT was taken after neck trauma to rule out fractures. As no fractures were found but instability due to disc ligament tear was suspected, an MRI was carried out that confirmed it. Anonymous patient then went to surgery where C-arm fluoroscopy images were taken intra-operatively to assist the surgeon during intervention.

\paragraph{Simulated medical systems}
The operating room was simulated in our laboratory with three displays showing CT, MRI, and C-arm fluoroscopy images. 
\textit{MicroDicom}\cite{Tangcharoen2011} was used to visualize the C-arm fluoroscopy image in \textbf{display A}, \textit{InVesalius}\cite{Moraes2012} was used to visualize the CT volume in \textbf{display B}, and MRI images were loaded directly in \textbf{display C}. The sizes of A, B, and C are 24 $''$, 42$''$ and 22$''$ and were placed at different locations and distances from the surgeon, as shown in \figurename{\ref{fig:ORSEnvironment}}. The interactions to be performed with these medical images were defined similar as those performed inside the modern day operating rooms:
\begin{description}
	[align=right,style=nextline,leftmargin= 0.3\linewidth,labelsep=\parindent,font=\normalfont]
	\item [Fluoroscopy image:] up-down flip , left-right flip
	\item [CT \& MRI images:] go through the images in axial, sagittal, coronal\\ zoom in/out in axial, sagittal, coronal
\end{description}
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=\textwidth]{figures/4-PointingOR/images/Displays}
	% figure caption is below the figure
	\caption{Simulated OR. There are three displays in the simulated scenario, where the C-arm fluoroscopy , CT, and MRI images are shown in display A, B, and C, respectively.}
	\label{fig:ORSEnvironment}       % Give a unique label
\end{figure}
\paragraph{Mouse and keyboard events} 
Going through the images in axial, sagittal, and coronal in \textit{InVesalius}\cite{Moraes2012} is done by clicking at the image window and rotating the mouse wheel. Zooming is triggered by pressing the right mouse button down and moving the mouse up or down. To flip images using \textit{MicroDicom} [19], it is necessary to trigger it by pressing buttons `Ctrl' with `F' or `H' at the same time. There is a  medical system configuration file for each Android application, and it declares how to generate the mouse and keyboard events to perform desired function.

\textit{Proposed interaction}- The intersection between the pointing ray and the screen plane is calculated as the cursor position, and a mouse click function is generated when the surgeon points to a new image window via his or her finger. Mouse wheel events are generated to go through the CT \& MRI data when the surgeon is pointing towards the \textit{InVesalius} image window and moving their finger left or right in the middle of the window. For zooming, the `right mouse button pressed down' event is triggered when moving their finger up or down. The button event `Ctrl+H' is generated for left-right flipping of the C-arm fluoroscopy image when the surgeon is pointing towards the \textit{MicroDicom} and moving their finger left or right. Similarly, the `Ctrl+F' button event is triggered to flip up-down the C-arm fluoroscopy images when surgeons move their finger up or down on the right side of the window.

\subsubsection{User study design}
In order to answer the above questions, a qualitative user study was conducted with 7 participants (3 expert surgeons and 4 final year medical students). First, we introduced the concepts, user interface system, types of interactions, and experimental setting to the participants. Second, each participant put on the wearable RGB-D sensor and underwent a test period lasting 3 minutes to get comfortable with the initial calibration step and subsequent interactions. The distance of the displays to the participants are: \textbf{display A}-1.5 meters, \textbf{display B}-3 meters, and \textbf{display C}-1 meter.

The interaction sequence with the three different displays and data is as follows. Each participant was asked to visualize the CT images for neck fracture(s), browsing through the slices and magnifying specific regions within the slice in \textbf{display B}. Then, the participants were asked to either rotate their head or body to face \textbf{display C} to confirm the site of the ligament tear in the MRI images by browsing through the images and magnifying them. Finally, the participants looked at the C-arm fluoroscopy \textbf{display A} to flip the X-ray images correctly (Please check the demo video here\footnote{\url{https://youtu.be/FLKBZOG26XY}}).

\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.7\textwidth]{figures/4-PointingOR/images/NASATLX}
	% figure caption is below the figure
	\caption{The results of NASA-TLX cognitive workload.}
	\label{fig:NASATLX}       % Give a unique label
\end{figure}
\subsubsection{Metrics}
The following metrics were measured during the course of the experiments. First, we measured the total time in seconds for the participants to interact with the three displays. Then, following the user study, we had a short interview with each of the participants and they were asked to complete an adapted System Usability Scale (SUS) \cite{Brooke1996b} questionnaire using a Five-point Likert scale (\textit{ 1-Strongly disagree, 2-Disagree, 3-Neither agree nor disagree, 4-Agree, 5-Strongly agree}). Lastly, the participants completed the NASA-TLX \cite{Hart2006}, which is a multi-dimensional scale ranked on 100 designed to obtain cognitive workload estimates from one or more operators while they are performing a task or immediately afterwards. The total workload is divided into six subscales that are represented as: \textit{Mental Demand, Physical Demand, Temporal Demand, Performance Effort, and Frustration}. The higher value from NASA-TLX means a higher cognitive workload.
\subsubsection{Results}
Although we did not have the same number of surgeons as students the measured average time to finish all the basic interactions with the three displays was 114$\pm$13 seconds for the medical students and 115$\pm$15 seconds for the experts. 
\begin{table}
	\caption{The Likert scale results of the usability questionnaire}
	\label{tb:questionnaire}
	\scriptsize
	\begin{center}
		\begin{tabular}{p{10cm}|p{1.2cm}}
			Questions & Scale \\
			\hline
			\textit{Q1:} The pointing gesture is intuitive for interaction. &  $4.1\pm0.7$ \\
			\textit{Q2:} It is very smooth to switch between different medical system. & $4.3\pm0.8$ \\
			\textit{Q3:} It is not complicated to get used to the system. & $4.1\pm0.7$ \\
			\textit{Q4:} The system feels responsive. & $3.4\pm0.7$\\
			\textit{Q5:} I  think the system is consistent. & $4.0\pm0.6$ \\
			\textit{Q6:} I prefer the proposed system instead of indirect interaction. & $4.4\pm0.8$ \\
			\textit{Q7:} The system is easy to use.& $3.9\pm0.9$ \\
			\textit{Q8:} I think that I would like to use this system. & $4.3\pm0.6$ \\
			\textit{Q9:} I would imagine that most people would learn to use this system very quickly. & $4.3\pm1.0$ \\
			\textit{Q10:} I felt very confident using the system. & $3.9\pm0.5$ \\
			\textit{Q11:} I didn't need to learn a lot of things before I could get going with this system & $4.7\pm0.4$
		\end{tabular}
	\end{center}
\end{table}
The results for the adapted SUS questionnaire are shown in \tablename{\ref{tb:questionnaire}}.
All responses were positive. The highest scores were credited to questions \#6 and \#11  indicating that the participants would prefer our user interface over the existing ones in the operating rooms (4.4 $\pm$ 0.8), and that such user interface did not require a significant amount of learning (4.7 $\pm$ 0.4). The lowest scores were attributed to questions \#4 and \#10 which indicate that participants did not feel confident (3.9 $\pm$ 0.5) which did not allow for a subsequent smooth interaction. Finally, the result of NASA-TLX cognitive workload is shown in \figurename{\ref{fig:NASATLX}}. The highest workload occurred for \textit{Effort} and \textit{Performance} with average scores of 48\% and 49\% respectively. The participants did not have high cognitive workload for the remaining subscales with values ranging between 20\% and 33\%.

\subsection{Conclusion}
%In conclusion, we have presented a novel user interface paradigm that gives the surgeon direct control of the interaction of different medical systems while maintaining sterility within the operating room.
Via a user study, we proposed and evaluated a new UI paradigm that gives the surgeon direct control of the interaction of different medical systems. There is no need to modify the software/hardware of the existing medical systems, and our solution can integrate pointing with personal gesture commands, while enabling switching smoothly among the different medical systems and operators. Our system can also be used for multi-user multi-system platforms. In future work, we plan to design a full user study and compare the proposed method to the traditional interfaces.