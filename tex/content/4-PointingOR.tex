% !TEX root = ../thesis-example.tex
%
\section{Device and System Independent Personal Touchless User Interface for Multi-Display}
\label{sec:4-IPCAI}
In the modern day operating room the surgeon performs surgeries with the support of different medical systems that showcase patient information, physiological data, and medical images. It is generally accepted that numerous interactions must be performed by the surgical team to control the corresponding medical system to retrieve the desired information. Joysticks and {physical} keys are still present in the operating room due to the disadvantages of mouses, and surgeons often communicate instructions to the surgical team when {requiring} information from a specific medical system. In this section, a novel user interface is developed that allows the surgeon to personally perform touchless interaction with the various medical systems, switch effortlessly among them, all of this without modifying the systems' software and hardware.
To achieve this, a wearable RGB-D sensor is mounted on the surgeon's head for inside-out tracking of {his/her} finger with any of the medical systems' displays. Android devices with a special application are connected to the computers on which the medical systems are running, simulating a normal USB mouse and keyboard. When the surgeon performs interaction using pointing gestures, the desired cursor position in the targeted medical system display, and gestures, are transformed into general events and then sent to the corresponding Android device. Finally, the application running on the Android devices generates the corresponding mouse or keyboard events according to the targeted medical system.
To simulate an operating room setting, our unique user interface was tested by 7 medical participants who performed several interactions with {the visualization of CT, MRI, and Fluoroscopy images} at varying distances from them. Results from the System Usability Scale and NASA-TLX workload index indicated a strong acceptance of our proposed user interface.

\subsection{Design and methods} \label{sec:IPCAI:Methods}
Inspired by our previous work \cite{ma2015ismar}, we propose a solution to give surgeons direct control of the various medical systems' displays inside the OR using only a single user interface via personal pointing gesture. The collaboration of the Android devices and software allow the combined system to act as a usual mouse or keyboard and facilitates working with any device that does not see a difference in its input/output.
%. gives surgeons direct control over screens and medical systems in their view
\subsubsection{Hardware setup}
We introduce a wearable RGB-D sensor and a wearable computing device for each surgeon, and an Android device for each medical system. 
The proposed hardware setup with one surgeon and one medical system is shown in  \figurename{\ref{fig:Hardware}}.
The wearable RGB-D sensor is connected to the computing device and the Android device is connected to the medical system simulating a normal USB mouse and keyboard. The computing {and Android} devices exchange information via a WiFi router. To simplify the display tracking task, a 2D marker plane is attached to each display.
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.75\textwidth]{figures/4-PointingOR/images/Hardware}
	% figure caption is below the figure
	\caption{The proposed hardware setup with a wearable RGB-D sensor, a wearable computing device, a medical system display, and an Android device.}
	\label{fig:Hardware}       % Give a unique label
\end{figure}
\subsubsection{Calibration}
For our setup there are two parts that require calibration: i) the transformation between the display and its marker, and (ii) the user specific pointing gesture. The calibration methods are shown in \figurename{\ref{fig:Calibration}} and presented below.
\paragraph{Display with markers}
Without modifying any software/hardware of the existing medical systems, this calibration is performed with a marker printed on an A4 paper, as shown in  \figurename{\ref{fig:Calibration}(a)}. 
The A4 paper is affixed closely next to the top-left corner of the display. Since the size of the A4 paper and the display are known, the translation of the printed marker relative to the display screen can be calculated. As the paper is very thin, the printed marker and the display are supposed to be coplanar. With the tracking information of the markers, the transformation matrix of the display relative to the display marker can be calculated. This one-time calibration process is performed offline for each display in the operating room. Then a configuration file is created for each display marker to describe which display it is attached to and which medical system it is connected to this display. 
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.9\textwidth]{figures/4-PointingOR/images/Calibration}
	% figure caption is below the figure
	\caption{Calibration methods: (a) A printed marker (1) is placed next to the right-top corner of the display to calculate the transformation matrix between the display marker (2) and the display. (b) Calibration of the user specific pointing gesture: the user is asked to point at several tracked targets $T_i$ using one finger and the pointing lines $L_i$ which pass through the fingertip $F_i$ and $T_i$ are collected.}
	\label{fig:Calibration}       % Give a unique label
\end{figure}
\paragraph{User specific pointing gesture}
In section \ref{section:4-PAST}, the objective is to calibrate and recover the user specific pointing geometry. The result of the calibration is the position of a virtual eye center $E$ for the user.
As shown in \figurename{ \ref{fig:Calibration}(b)}, the target, $T_i$, is shown somewhere in front of the user, who is then asked to perform a natural pointing gesture towards $T_i$ using one finger. The 3D positions of the fingertips $F_i$ and $T_i$ are collected through the wearable RGB-D sensor. The pointing line $L_i$ is defined such that it passes through $F_i$ and $T_i$. After sufficient pointing lines are collected, the intersection of the lines is calculated defining the user specific virtual eye-center $E$. After calibration the pointing geometry is recovered as $L_{i}$, going through $E$ and  $F_i$, when the pointing gesture is performed.

\subsubsection{System framework}
\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{figures/4-PointingOR/images/Workflow}
	\caption{The framework of the proposed system. Tracking information of objects in the OR after processing is used to control the medical systems. }
	\label{fig:Workflow}       % Give a unique label
\end{figure}
The system framework is shown in \figurename{\ref{fig:Workflow}}.
The left side shows the usual elements in the OR and the right side shows the data-flow of our user interface paradigm. When a surgeon performs interaction with a medical system, the display marker and fingertip can always be detected via the RGB-D sensor as it has a similar view as the surgeon. 
Through the following processing: \textit{Tracking Info}, \textit{Pointing Geometry}, and \textit{Interaction Events}, mouse and keyboard events are generated to control any medical systems.
\paragraph{Tracking Info} 
All the images are perceived via the wearable RGB-D sensor. Therefore, this is an inside-out tracking and does not require an external reference. The fingertip is detected from the depth image using 3D blob extraction and the display marker position is calculated via the color image.
\paragraph{Pointing Geometry}
Based on the calibration of ``User specific pointing gesture" and the tracking information,
the pointing geometry between the pointing direction and the display screen plane is recovered. The target position in real world and the desired cursor position on the screen can be calculated. We can also detect some predefined finger and hand gestures. Finally, all these information are transformed into general events and then sent to the corresponding Android device through UDP (User Datagram Protocol) connections. 
\paragraph{Interaction Events}
Two virtual devices are created in a Linux kernel to simulate USB mouse and keyboard when the Android device is connected to a computer. The two devices send normal initial signals to the host computer and are recognized as USB mouse and keyboard. The special application running on the Android system builds up a UDP connection with the computing devices to receive general events transformed from the pointing geometry and generates the corresponding mouse or keyboard events according to the current medical system. In our general design, all the interactions are implemented via the Android mouse or keyboard as the medical system is taken as a black box. However, the computing device could directly communicate with the medical system when this is allowed. 
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/4-PointingOR/images/MultiUserMultiSystem}
	\caption{A setup with Multi-user Multi-system and the Android system structure. In this setup, there are two surgeons and two medical systems, one of which has two displays. }
	\label{fig:multiUserMultiSystem}       % Give a unique label
\end{figure}
During the Pointing Geometry stage, a personal configuration file could also be loaded to implement the multi-user case and personalized figure-pointing gestures. 
\paragraph{Multi-user Multi-system Scenario}
From \figurename{\ref{fig:multiUserMultiSystem}}, there can be more than one surgeon and more than one medical system in the operating room scenario. In the configuration file, each member of the surgical team is assigned a priority level according to his or her responsibility. When Android devices receive information from more than one computing device at the same time, they implement the mouse and keyboard event of the surgeon with higher priority level. This can solve the issues which arise when these systems are used in the context of an actual surgical procedure, including collaboration, engagement and disengagement. When there are more than one displays connected to a medical system, it still works well as the Android device knows which display is the desired one. Even when there is no display for a medical system, a display marker still can be positioned somewhere to simulate a virtual display, which is logically connected to the medical system. In this case, the surgeon can point at the display marker and perform interaction with the corresponding system.
\paragraph{Personalized Pointing Gesture} 
Performing interactions requires two steps: i) choosing the target, and (ii) releasing the command. Through pointing gestures the surgeon can choose the target naturally via the pointing and immediately perform a finger or hand gesture to release the command. During the calibration procedure, the calibration of user pointing provides the characteristics of a user specific pointing geometry. In addition, since the pointing gesture is personal, pointing characteristics could be saved for each surgeon in order to recognize their gesture and map it to different commands.

\subsection{Evaluation \& results}
The main goal of the user study is to answer the following questions: 
\begin{description}
	\item[1.] To what degree does the surgeon accept the interaction with multiple medical systems via our novel user interface?
	\item[2.] Does the surgeon find the system a viable alternative to the traditional indirect control of the medical systems and their displays in the modern day operation room?
\end{description}
Please note that we are not evaluating the gesture accuracy, but we are assessing the Android-based communication with the different systems.

% A \textbf{supplementary video} is uploaded together with this submission that illustrates the concepts of our proposed user interface paradigm.
\subsubsection{Implementation}
\paragraph{\underline{Proposed system}} 
During the user study, the Intel Realsense Creative camera was chosen as the wearable camera and Microsoft Surface 2 as the computing device, which was put in a backpack. The special Android application was installed on two Nexus 7 devices to simulate the USB mouse and keyboard for each medical system. 
\paragraph{\underline{Patient scenario}}Anonymous patient is wheeled inside the emergency room following a car accident. A CT was taken after neck trauma to rule out fractures. As no fractures were found but instability due to disc ligament tear was suspected, an MRI was carried out that confirmed it. Anonymous patient then went to surgery where C-arm fluoroscopy images were taken intraoperatively to assist the surgeon during intervention.
\paragraph{\underline{Simulated medical systems}}
The operating room was simulated in our laboratory with three displays showing CT, MRI, and C-arm fluoroscopy images. 
MicroDicom\cite{Tangcharoen2011} was used to visualize the C-arm fluoroscopy image in \textbf{display A}, InVesalius\cite{Moraes2012} was used to visualize the CT volume in \textbf{display B}, and MRI images were loaded directly in \textbf{display C}. The sizes of A, B, and C are 24 $''$, 42$''$ and 22$''$ and were placed at different locations and distances from the surgeon, as shown in \figurename{\ref{fig:ORSEnvironment}}. The interactions to be performed with these medical images were defined similar as those performed inside the modern day operating rooms:
\begin{description}
	[align=right,style=nextline,leftmargin= 0.3\linewidth,labelsep=\parindent,font=\normalfont]
	\item [Fluoroscopy image:] up-down flip , left-right flip
	\item [CT \& MRI images:] go through the images in axial, sagittal, coronal\\ zoom in/out in axial, sagittal, coronal
\end{description}
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.75\textwidth]{figures/4-PointingOR/images/Displays}
	% figure caption is below the figure
	\caption{There are three displays in the simulated scenario, where the C-arm fluoroscopy , CT, and MRI images are shown in display A, B, and C, respectively.}
	\label{fig:ORSEnvironment}       % Give a unique label
\end{figure}
\paragraph{\underline{Mouse and keyboard events}} 
Going through the images in axial, sagittal, and coronal in InVesalius\cite{Moraes2012} is done by clicking at the image window and rotating the mouse wheel. Zooming is triggered by pressing the right mouse button down and moving the mouse up or down. To flip images using MicroDicom [19], it is necessary to trigger it by pressing buttons `Ctrl' with `F' or `H' at the same time. 

\textit{Proposed interaction}- The intersection between the pointing direction and the screen plane is calculated as the cursor position, and a mouse click function is generated when the surgeon points to a new image window via his or her finger. Mouse wheel events are generated to go through the CT \& MRI data when the surgeon is pointing towards the InVesalius image window and moving their finger left or right in the middle of the window. For zooming, the `right mouse button pressed down' event is triggered when moving their finger up or down. The button event `Ctrl+H' is generated for left-right flipping of the C-arm fluoroscopy image when the surgeon is pointing towards the MicroDicom and moving their finger left or right. Similarly, the `Ctrl+F' button event is triggered to flip up-down the C-arm fluoroscopy images when surgeons move their finger up or down on the right side of the window.

\subsubsection{User study design}
In order to answer the above questions, a qualitative user study was conducted with 7 participants (3 expert surgeons and 4 final year medical students). First, we introduced the concepts, user interface system, types of interactions, and experimental setting to the participants. Second, each participant put on the wearable RGB-D sensor and underwent a test period lasting 3 minutes to get comfortable with the initial calibration step and subsequent interactions. The distance of the displays to the participants are: \textbf{display A}-1.5 meters, \textbf{display B}-3 meters, and \textbf{display C}-1 meter.

The interaction sequence with the three different displays and data is as follows. Each participant was asked to visualize the CT images for neck fracture(s), browsing through the slices and magnifying specific regions within the slice in \textbf{display B}. Then, the participants were asked to either rotate their head or body to face \textbf{display C} to confirm the site of the ligament tear in the MRI images by browsing through the images and magnifying them. Finally, the participants looked at the C-arm fluoroscopy \textbf{display A} to flip the X-ray images correctly.
\subsubsection{Metrics}
The following metrics were measured during the course of the experiments. First, we measured the total time in seconds for the participants to interact with the three displays. Then, following the user study, we had a short interview with each of the participants and they were asked to complete an adapted System Usability Scale (SUS) \cite{Brooke1996b} questionnaire using a Five-point Likert scale (\textit{ 1-Strongly disagree, 2-Disagree, 3-Neither agree nor disagree, 4-Agree, 5-Strongly agree}). Lastly, the participants completed the NASA-TLX \cite{Hart2006}, which is a multi-dimensional scale ranked on 100 designed to obtain cognitive workload estimates from one or more operators while they are performing a task or immediately afterwards. The total workload is divided into six subscales that are represented as: \textit{Mental Demand, Physical Demand, Temporal Demand, Performance Effort, and Frustration}. The higher value from NASA-TLX means a higher cognitive workload.
\subsubsection{Results}
Although we did not have the same number of surgeons as students the measured average time to finish all the basic interactions with the three displays was 114$\pm$13 seconds for the medical students and 115$\pm$15 seconds for the experts. 
The results for the adapted SUS questionnaire are shown in \tablename{\ref{tb:questionnaire}}.
All responses were positive. The highest scores were credited to questions \#6 and \#11 which indicate that the participants would prefer our user interface over the existing ones in the operating rooms (4.4 $\pm$ 0.8), and that such user interface did not require a significant amount of learning (4.7 $\pm$ 0.4). The lowest scores were attributed to questions \#4 and \#10 which indicate that participants did not feel confident (3.9 $\pm$ 0.5) which did not allow for a subsequent smooth interaction. Finally, the result of NASA-TLX cognitive workload is shown in \figurename{\ref{fig:NASATLX}}. The highest workload occurred for \textit{Effort} and \textit{Performance} with average scores of 48\% and 49\% respectively. The participants did not have high cognitive workload for the remaining subscales with values ranging between 20\% and 33\%.
\begin{table}
	\caption{The Likert scale results of the usability questionnaire}
	\label{tb:questionnaire}
	\scriptsize
	\begin{center}
		\begin{tabular}{p{10cm}|p{1.2cm}}
			Questions & Scale \\
			\hline
			\textit{Q1:} The pointing gesture is intuitive for interaction. &  $4.1\pm0.7$ \\
			\textit{Q2:} It is very smooth to switch between different medical system. & $4.3\pm0.8$ \\
			\textit{Q3:} It is not complicated to get used to the system. & $4.1\pm0.7$ \\
			\textit{Q4:} The system feels responsive. & $3.4\pm0.7$\\
			\textit{Q5:} I  think the system is consistent. & $4.0\pm0.6$ \\
			\textit{Q6:} I prefer the proposed system instead of indirect interaction. & $4.4\pm0.8$ \\
			\textit{Q7:} The system is easy to use.& $3.9\pm0.9$ \\
			\textit{Q8:} I think that I would like to use this system. & $4.3\pm0.6$ \\
			\textit{Q9:} I would imagine that most people would learn to use this system very quickly. & $4.3\pm1.0$ \\
			\textit{Q10:} I felt very confident using the system. & $3.9\pm0.5$ \\
			\textit{Q11:} I didn't need to learn a lot of things before I could get going with this system & $4.7\pm0.4$
		\end{tabular}
	\end{center}
\end{table}
\begin{figure}
	\centering
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.7\textwidth]{figures/4-PointingOR/images/NASATLX}
	% figure caption is below the figure
	\caption{The results of NASA-TLX cognitive workload.}
	\label{fig:NASATLX}       % Give a unique label
\end{figure}

\subsection{Discussion \& Outlook}
%Analyze the results and the responding reasons:
%
%All the user agree our system is very interesting and would be very useful. However, some users can not perform the interaction very well as the finger tracking is not perfect and the marker tracing is very challenged as the wear sensor has only 30 frames per second. 
%
%Comparing with the traditional method and the normal gesture control methods, the proposed framework is very general.
%here the current system is just to  present the main concept, not that functions is professionally correct. 
%the gesture control and user interface could be improved later.  
%
%The hand gesture is not valid as the depth camera keeps a similar view as the color and can not see the whole hand. 
%
%How to improve the system in future: we could develop a special RGB-D camera in which the depth sensor looks down and the color cameras look straightly. 
Like most new technologies, the learning curve for the participants needs to be considered when interpreting the results of our user study. As each of the 7 medical participants did not have any prior knowledge with our user interface, it was not surprising to note that there was no significant difference in the time required to complete the interactions.  It was also expected that participants did not feel confident using the user interface for the first time as seen from the adapted SUS results. This effect translated to the cognitive workload NASA-TLX with highest scores to the following questions:\textit{(Performance) How successful were you in accomplishing what you were asked to do?} and \textit{(Effort) How hard did you have to work to accomplish your level of performance?}

Albeit these shortcomings, the vast majority of the results were positive. With consistent use of our UI, participants agreed that the potential impact of the proposed user interface compared to the traditional interfaces found in the operating room is clear. Our unique framework allows the surgeon to personally perform touchless interaction with the various medical systems at different locations and distances, switch effortlessly among them, all this without modifying the systems’ software and hardware. Comparing with the traditional methods our proposed framework is very general and can be adapted to multi-users and multi-displays. The objective of this section was to present the main user interface paradigm, and not evaluate the accuracy of the different gestures for displays A-B-C.

In 2014, the authors in \cite{Pederson2010} provided insights and guidelines when developing user interfaces for the operating room. A key consideration is the physical location of surgeons when they need to interact with different medical systems. The patient table can be a crowded environment, with surgeons often in close proximity to other members of the surgical team. Not only can this affect a system’s approach to tracking but also impose constraints on the kinds of movement available for gesture design. In sterile practice, hand movements must be restricted to the area extending forward from the surgeon's torso between the hips and chest level. Moreover, the patient table itself hides the lower half of the surgeon's body. Also, sterility restrictions in the operating theater make touchless interaction an interesting solution to give the surgeon direct control over the medical systems. Our proposed user interface addresses these concerns.

Currently, finger tracking is not perfect and the marker tracking is challenged since the wearable RGB-D sensor works at a rate of 30 frames per second. In future, with the advancement of algorithms and hardware these current limitations should be rectified.
One could also develop a special RGB-D camera in which the depth sensor looks downward and the color cameras look straight ahead to improve tracking and detection of gestures. Lastly, with the volumetric acquisition of scans, the data is increasingly visualized as a 3D volume. Hence, the volumes would be better exploited using a full 3D interaction technique. The tracking of hands and gestures in 3D space opens up a much richer set of possibilities on how surgeons manipulate and interact with images through the full six degrees of freedom \cite{Pederson2010}.

\subsection{Conclusion}
%In conclusion, we have presented a novel user interface paradigm that gives the surgeon direct control of the interaction of different medical systems while maintaining sterility within the operating room.
Via a user study, we proposed and evaluated a new UI paradigm that gives the surgeon direct control of the interaction of different medical systems. There is no need to modify the software/hardware of the existing medical systems, and our solution can integrate pointing with personal gesture commands, while enabling switching smoothly among the different medical systems and operators. Our system can also be used for multi-user multi-system platforms. In future work, we plan to design a full user study and compare the proposed method to the traditional interfaces.