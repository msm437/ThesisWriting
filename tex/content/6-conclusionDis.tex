\chapter{Conclusion} \label{chaptor:ConDis}
\section{Summary}
In this thesis, a magic mirror framework is first presented to create a MR environment with an \textit{in-situ} augmented reality view. The system can track the current user and finish the registration between the medical datasets and the user based on the personal information. It inherently allows the user to perform interactive registration and directly map the medical information onto their body. This opens interesting possibilities for personalized mixed reality for medical education and rehabilitation. 
Through the participation of 7 clinicians and 72 anatomy students, two user studies were designed to respectively assess the precision and acceptability of the magic mirror system for anatomy education in the classrooms of tomorrow. 
We also take the advantage of the interactive mixed reality to generate a personalized learning procedure. In addition, ``Organ Explosion'' and ``Self-control virtual view'' systems are designed and implemented for anatomy leaning. Serious games are also developed in the framework for health-care education and rehabilitation exercise.

Secondly, a method is proposed to calculate a virtual eye center as the origin for the \textit{eye-rooted} pointing ray without tracking the eye gaze in an egocentric setting. The user specific pointing geometry can then be recovered for interaction with ambient media and objects without visual feedback. 
The pointing ray starts from the virtual eye center and goes through the fingertip to the desired target information. 
During the initial calibration, the user is asked to perform pointing gestures towards several targets displayed within the field of view. A user specific virtual eye center is then estimated based on the detected fingertip in the coordinate system of the wearable device. Then the \textit{eye-rooted} pointing ray is recovered when the user is performing a pointing gesture. 
The proposed method is mathematically analyzed and demonstrated that the calibration is feasible with a measured accuracy of the recovered pointing ray below $0.9\degree$. The proposed method is also evaluated in a user study consisting of 10 participants performing pointing interaction with objects shown on the display while wearing a RGB-D sensor. The study shows that the calibration is independent of the environment and the pointing ray is recovered with an accuracy of $0.67 \pm 0.71\degree$ without any visual feedback. 
Based on the recovered pointing gesture, a novel user interface is introduced, allowing surgeons to {personally} perform touchless interaction with the {various} medical systems, and switch effortlessly among them, all of this without modifying the systems' software and hardware.

%natrual perception and one-multi user supervision and teaching, training 
Finally, in conjunction with the proposed magic mirror framework and recovered pointing gesture, a multi-user collaborative MR framework is introduced. The human-human communication is enhanced via pointing gesture and MR framework. 

\newpage
\section{Discussion \& Outlook}
\subsection{Personalized AR}
The magic mirror system does not require expensive hardware and can be set up with different display devices. Therefore, we can create a demo setup consisting only of a Kinect, a display device and a laptop. The system has been shown to the public at several occasions. It was shown during the open day of a hospital, in a school and during conferences. The feedback of the users, in particular from children has been very positive. While the MR \textit{in-situ} visualization attracts the attention of people, it turned out that most users would spend much more time to understand where different organs are and what they look like. During the demos, the system has been used by many people and it has shown to be robust. The calibration sometimes fails, in particular when people are wearing thicker coats or jackets. But if the position and angle of the Kinect is set well, calibration problems are reduced largely. 

It was originally suggested to us that our system would have much more of an impact for medical education if it were to be translated today in medical schools and anatomy classrooms. 
As such, with the help of our anatomy professor partners, we delivered the improved AR magic mirror to two anatomy classes within the Anatomy Department of the Ludwig Maximilian University (LMU) Medical School, Munich, Germany. 
The anatomy professors made it clear that the proposed system is exciting, but has to go beyond state-of-the-art technology to be truly useful for education. 
The user study participants stressed the importance of visualization of anatomy that can change dynamically resulting from the actions of the user moving the body. 
Furthermore, more advanced user interactions like the use of gaming elements would be required to make the use of the system for learning and rehabilitation tasks more interesting.

\subsection{Personalized user interface}
\subsubsection{Pointing gesture recovery in an egocentric setting}
%put the bared-hand calibration method here
%This work is different than the hand gesture recognition and gesture based interaction in the state of art. Its main difference lies within
In Section \ref{section:4-PAST}, we investigated whether our proposed method could recover a stable and precise pointing origin and corresponding pointing rays accurately, using the collected data via the wearable device. 
The interpupillary distances were calculated very accurately as the participants had a clear view of the finger when performing pointing with only one eye open  (see \figurename{\ref{fig:InterpupillaryDist}}). 
From \figurename{\ref{fig:PAST}}, we find the proposed method is contextually independent and can be used to calculate the eye position in the wearable device coordinate system. 
As seen in \figurename{\ref{fig:virtualEye}}, the virtual eye center calculated by the PAST calibration is user specific. When the eye positions are known, this method can be used to determine the position of each eye, or the average of both eyes, as the best origin for the pointing ray.

As demonstrated by our user study, the calibration of the virtual eye center is completed stably by the participants. The pointing ray is recovered in different scenarios with an accuracy of less than $1\degree$ in most cases.
The participants easily learned how to use this new interaction method.

Eye gaze tracking is an important technique in an egocentric setting, %such as \textit{ Microsoft HoloLens}, 
which allows users to navigate and explore holograms by looking at them. %\footnote{https://www.microsoft.com/microsoft-hololens/en-us/developers{\#}faq}
%\add{\\nassir's question: ``is this true? how do they recover the pointing reference?"\\}
In this case, the eye gaze geometry is employed to calculate where the user is looking at and the pointing gesture is used as a confirmation command. 
Compared to pointing techniques, there is only one available controlling point since the eyes can only focus on object of interest. In addition, it is very difficult to perform gestures using the eye-gaze, such as drawing a circle or line, and the visual feedback is necessary to combine the hand/finger movement for interaction, e.g. selecting a special area.

The PAST method can also be employed in other interesting scenarios such as: (a) \textbf{a wearable RGB-D sensor and an optical see through HMD}. A 3D virtual world can be presented using a HMD and contextual information of the virtual world in the HMD is known after calibration with the RGB-D sensor. The targets can be shown in the HMD during calibration and then the user can start interacting within a mixed reality world using pointing gestures. 
(b) \textbf{a wearable RGB-D sensor in a known environment}. The real world environment can be tracked through the color camera, thus contextual information of the entire real world is known. After calibration, the user can start interacting with elements in the real world using a pointing gesture without any visual feedback.
(c) \textbf{a near range and far range depth sensors}. The user and the computer can directly work together to collect information from the real world, such as 3D scene understanding.

A special RGB-D camera, in which the depth sensor looks at a different direction compared to the color camera, should be developed to acquire the depth image of the full hand to improve the fingertip tracking. 
Positioning $S_f$ and $S_c$ at different directions also enables users to perform a pointing gesture more naturally. 
The PAST calibration can be separately performed for each finger to improve the accuracy, allowing the user to use multi-fingers to perform a multi-control interaction in a mixed reality setting. Given the recent increased interest in using speech to interact with mobile devices, it is also a logical next step to support a user pointing at an object while stating a command or asking a question about it.

\subsubsection{Device and System Independent Personal Touchless User Interface for Multi-Display}
Like most new technologies, the learning curve for the participants needs to be considered when interpreting the results of our user study. As each of the 7 medical participants did not have any prior knowledge with our user interface, it was not surprising to note that there was no significant difference in the time required to complete the interactions.  It was also expected that participants did not feel confident using the user interface for the first time as seen from the adapted SUS results. This effect translated to the cognitive workload NASA-TLX with the highest scores to the following questions:\textit{(Performance) How successful were you in accomplishing what you were asked to do?} and \textit{(Effort) How hard did you have to work to accomplish your level of performance?}

Albeit these shortcomings, the vast majority of the results were positive. With consistent use of our user interface, participants agreed that the potential impact of the proposed solution compared to the traditional interfaces found in the operating room is clear. Our unique framework allows the surgeon to personally perform touchless interaction with the various medical systems at different locations and distances, switch effortlessly among them, all this without modifying the systems’ software and hardware. Comparing with the traditional methods our proposed framework is very general and can be adapted to multi-users and multi-displays. The objective of this section was to present the main user interface paradigm, and not evaluate the accuracy of the different gestures for displays A-B-C.

In 2014, the authors in \cite{Pederson2010} provided insights and guidelines when developing user interfaces for the operating room. A key consideration is the physical location of surgeons when they need to interact with different medical systems. The patient table can be a crowded environment, with surgeons often in close proximity to other members of the surgical team. Not only can this affect a system’s approach to tracking but also impose constraints on the kinds of movement available for gesture design. In sterile practice, hand movements must be restricted to the area extending forward from the surgeon's torso between the hips and chest level. Moreover, the patient table itself hides the lower half of the surgeon's body. Also, sterility restrictions in the operating theater make touchless interaction an interesting solution to give the surgeon direct control over the medical systems. Our proposed user interface addresses these concerns.

Currently, finger tracking is not perfect and the marker tracking is challenged since the wearable RGB-D sensor works at a rate of 30 frames per second. In the future, with the advancement of algorithms and hardware these current limitations should be rectified.
%One could also develop a special RGB-D camera in which the depth sensor looks downward and the color cameras look straight ahead to improve tracking and detection of gestures. 
Lastly, with the volumetric acquisition of scans, the data is increasingly visualized as a 3D volume. Hence, the volumes would be better exploited using a full 3D interaction technique. The tracking of hands and gestures in 3D space opens up a much richer set of possibilities on how surgeons manipulate and interact with images through the full six degrees of freedom \cite{Pederson2010}.

\section{Framework for Collaborated Mixed Reality}
Collaboration between people in MR environment becomes more and more common. In normal anatomy teaching or rehabilitation scenario, there is a teacher or doctor to teach and monitor the student/patient for the learning and exercise.
When we try to introduce the new technique for these scenarios, one question is to make sure that it supports the collaborative MR. 
From the real world to a MR environment, there are functional seams and cognitive seams for the user during the collaboration. 
In the proposed framework, the normal interaction methods are kept in the MR environment and the human-human communication is enhanced via interaction with AR view.
We proposed such a framework for only two scenarios. 
The next step is to develop some applications in medical teaching and rehabilitation exercise and perform user studies to evaluate the framework. 

