% !TEX root = ../thesis-example.tex

\chapter{History and State of the Art}
\section{Mixed Reality}

\section{Medical Education}
%Medical education is very difficult
In today's medical schools students are required to understand both function and spatial context of human anatomy. Traditional medical education learning is classified into three categories: cadaver, model, and book-based. The cadaver-based learning has seen decline due to practical and cost issues. As a result, anatomical education has shifted towards physical, diagram and image models. However, some drawbacks exist with these learning paradigms. For example, it is difficult to interpret the spatial and physical characteristics of anatomy by observing two-dimensional images, diagrams, or photographs. Many physical models also lack detail levels to fully understand the specific anatomy. 

In the advent of the plethora of exciting technological advancements there should be no reason not to include these for the creation of new education paradigms for medical learning. By combining computer models of anatomical structures with custom software we can showcase to students new ways of interacting with anatomy that could not be achieved during cadaveric dissections or in static images and diagrams for increasing their learning satisfaction \cite{Bacca2014,ma2013ismar,NMC2014}.

For education of medical students, the dissection of cadavers is still a very important practical training and it is paired with anatomical lessons to learn about the anatomical systems they are exploring each day. For obvious reasons, this is not a method that can be applied to any person interested in anatomy, nor should it be necessary to teach this level of understanding to the general public. However, digital methods find increasing popularity among medical teaching institutions, too [Arg13].
%todo: tranditional methodology to teaching, atlas and e-learning and more 
E-learning is commonly used today. Many case databases, atlases for medical experts, and atlases for non-experts exist. These resources are very valuable and are more interactive and interesting than textbooks; however with the disadvantage of having users still mentally map/link anatomy to their bodies. The personalization in E-learning systems has been the subject of much recent research and allows teachers to select personalization parameters and combine them flexibly to define different personalization strategies according to the specifics of the courses. 

%todo: VR for medical learning and teachinnd
Many commercially available systems use virtual reality (VR) for medical education and psychomotor skills training. These systems have indeed proven to be valid and are of use in clinical training \cite{Lewis2011,Thijssen2010,VanDongen2011}. In addition to the advantages of VR, AR systems also have the advantage that information can be embedded and/or superimposed upon reality. This allows for a more close-to-reality presentation of medical knowledge and offers opportunities for more interactive learning. The user can also spatially relate virtual objects to the reality. 

%todo: AR for medical education
AR in formal and informal learning environments with an emphasis on affordances, usually utilizes context-aware technologies, which enable participants to interact with digital information embedded within the physical environment \cite{Dunleavy2008}. The AR system presents digital media to learners after they point the camera at an object (e.g., QR code, 2D target) and creates immersive learning experiences with the physical environment, providing educators with a novel and potentially transformative tool for teaching and learning \cite{Dede2009,Johnson2011}. In medical, an AR system for patient-doctor communication has recently been presented \cite{Ni2011}. This system used a hand-held projector to project anatomy onto the skin of the patient. While this system is relatively inexpensive, it could only display anatomy which is close to the skin in correct perspective. Furthermore, it did not automatically track the position of the projector and the patient. The doctor must then figure out manually how to position the projector and the patient in such a way that the anatomy is augmented correctly.
Augmented reality systems for visualization of anatomy have been shown before. \cite{Davis2002} presented a system that augmented a 3D model of anatomical airways onto a patient phantom using a HMD. Another system that used a HMD to visualize human anatomy onto a phantom has been shown by \cite{Juan2008}. Their system allows students to open the abdomen of the phantom and visualizes different organs on the phantom.
\subsection{Rehabilitation using AR or MR}

\subsection{School education}
Although technology has advanced significantly in the last decades, classical school edu-cation still mostly uses the same methods to convey anatomical knowledge that have been used for a long time. Typically, the information is collected in printed books like anatomy atlases, displayed in the form of charts and diagrams like the one seen in Figure 2.1. Those diagrams provide a simple and well-known method to illustrate form and appearance of organs, having the advantage that the user is accustomed to such methods of display. However, there exist several downsides to this method: First of all, the view is limited to a selected few different angles the author chose to present. This may not be sufficient in some cases to fully convey how an organ is situated relative to its surroundings since oc-clusions limit the possibilities to visualize these spatial relations. Another problem is that often the organs are only depicted schematically by leaving out details or distorting tissue colors, thus giving only a coarse impression on how the organ actually looks like in reality.

In recent times, more and more digital methods evolve which are trying to improve on these methods. Many of them offer an interactive anatomy atlas, usable either as a on-line service or as a standalone application [Zyg13, HOW13]. There also exist organizations specifically offering teaching bundles for use in classrooms, making it possible to use alter-native teaching methods at different school levels [Nav13, Arg13] assisted by videos and interactive tools. Those interactive applications offer large improvements on the classi-cal, printed method: It is possible to view organs and structures from any desired angle, control magnification and often even select specific organs and systems to be displayed or hidden. Through all those interactive elements, the users can control the region of interest and hopefully better understand the information they are looking for.


\subsection{Games and Anatomy Education}

Using games to teach skills used in real life or to convey knowledge has in the last years become generally known under the term serious gaming. The goal is to teach the player something practical in an entertaining and ”fun” way.

In the context of anatomy education, for example the website Anatomy Arcade\footnote{\url{http://anatomyarcade.com/}} offers a large collection of anatomy-related arcade games. The games range from crosswords and jigsaw puzzles to more sophisticated ideas like ”Poke-A-Muscle” game where the user has to select the right muscles as quickly as possible. These games are accompanied with educational videos to provide a base knowledge that can be tested in the games. The games certainly are not suitable for educating medical students, but they are a fun way to learn basic human anatomy.

A recently successful game in the indie genre is the Surgeon Simulator 2013 [Bos13]. Here, players have to perform different surgeries from the viewpoint of the acting surgeon. In a 3D environment where organs and tools behave in a physically believable way, for instance they have to remove various organs to eventually perform a heart transplant. The most important gameplay mechanic is probably the unconventional input mechanic, making it very hard to successfully perform all necessary tasks. While the focus of this game is not on education but rather general entertainment, the players still implicitly learn positions and shape of the most important organs of the body.

Another free online service more tailored towards patient education is Surgery Squad\footnote{\url{http://www.surgerysquad.com/}}. Here, players can perform different types of surgery in the browser. They are guided by a virtual surgeon explaining the medical background to the surgery and every step involved. They interactively learn about the surgical procedure by listening to the in-structions the virtual surgeon gives and subsequently performing them, mostly by using the mouse clicking or dragging over certain areas. These interactive surgeries have been created to educate patients or other interested people on how the surgeries or other proce-dures are being performed, with a minimal baseline of anatomical knowledge.

\section{Natural Interaction}

\subsection{Eye-rooted Pointing gesture}
%Eye-rooted pointing has been used as a virtual pointer for object selection in virtual environment and pointing and clicking techniques with large displays from a distance.
A number of studies have demonstrated that pointing gestures perform quite effective selection in virtual environments \citep{Argelaguet2008} and interaction with very large, high resolution displays \citep{Vogel2005}.

%\textbf{Virtual Pointer} 
\paragraph{Virtual Pointer} The \textit{eye-rooted} pointing technique was developed as the virtual pointer for object selection in virtual environments.
% \citep{Argelaguet2008,Forsberg1996,Pierce1997}.  
The most simple approach, occlusion selection, used a selection ray which was defined as starting from the user's eye position and pointing to the user's hand position \citep{Forsberg1996,Pierce1997}.
\citet{Argelaguet2008} proposed a hybrid technique that the ray started from the user's eye but the direction was controlled by the hand orientation. 
\citet{Forsberg1996} presented that the origin point in this scenario was one of the positions of the user's two eyes or an average of them. 
The eye position in these papers was known as it was manually defined during scene rendering. 

%\textbf{Distant Pointing} 
\paragraph{Distant Pointing} Eye-rooted pointing allows users to perform interaction with a remote display by directly pointing at where they are looking at. The remote pointing position is also defined by the ray casting from the user's eye to their fingertip. 
\citet*{Liang1994} first presented a highly interactive 3D model system via a ray going from the eye through a hand-held bat while two tracking devices monitored the user's head and the bat.
\citet{Nickel2003} explored computer vision methods to recognize the body, hand, and finger positions in 2D and 3D space to calculate the pointing direction.
A vision-based human-computer interaction system was presented by \citet{Reale2011}, integrating components using multiple gestures, including eye gaze, head pose, hand pointing and mouth motions.
\citet*{Vogel2005} employed Vicon motion tracking system to analyze the hand-based raycasting and relative pointing with clutching.
\citet{Banerjee2012} developed \textit{MultiPoint}, a set of perspective-based remote pointing techniques that allowed users to perform remote manipulation of graphical objects on large displays while extra cameras and motion trackers were employed to detect the eye and finger.
\citet{Huang2014} developed \textit{Dart-it}, a lightweight system that enables perspective-based remote direct-pointing on any deployed remote display using only one RGB-D camera. 
Nonetheless, since the recovering pointing ray requires reliable eye and fingertip tracking, all the above works require extra camera and motion trackers for real-world deployment.

%\textbf{Interaction with an egocentric setup} 
\paragraph{Interaction with an egocentric setup} The interaction between the user and their device can start with a signal sent using their hands or eye-gaze through the camera.

Gesture interacting: Several works focused only on natural gesture interaction methodologies. In this case, sensors usually detect and track the pose of arm, hand or finger to recognize the gesture interaction.
\citet{DeCampos2006} proposed a framework combining a coarse-to-fine method for shape detection and 3D tracking method that can identify pointing gestures and estimate their direction in the wearable domain.
A gesture recognition algorithm was developed for an interactive museum using 5 different gestures \citep{Garcia-Rodriguez2011}. 
\citet{Colaco2013a} presented \textit{Mime}, a compact, low-power 3D sensor for unencumbered free-form, single-handed gestural interaction with HMDs. 

Relative pointing: In this scenario, usually a visual feedback is shown to hint where the potential action object would be. The user has to learn how to use the natural gesture movement to control the feedback cursor.
\citet{Mistry2009} proposed to bring information out into the tangible world by using a tiny projector and camera mounted on a hat, so the user could directly point to and manipulate the information in the real word.

Natural pointing: There are several methods to implement the natural pointing with an egocentric setup in different scenarios. 
(A) In the case where the object is reachable, the sensor could directly detect which object the user is pointing at without a valid origin. \textit{OmniTouch} \citep{Harrison2011} is a wearable depth-sensing and projection system that enabled interactive multi-touch applications on everyday reachable surfaces. 
SemanticPaint \citep{export:244725} allowed users to segment the scene simply by reaching out and touching any desired object or surface.
(B) \citet{Ha2014} developed a video see through AR system in head mounted display (HMD), allowing the user to manipulate virtual 3D objects with a bare hand. 
A bare hand-based interaction with reachable virtual objects in egocentric viewpoint was also developed by \citet{Jang2015}.
When the user's view was controlled by the computer, the user's eye position was defined during the rendering procedure. Hence, the origin of the pointing ray was manually defined.
(C) \textit{Pupile} \citep{Kassner2014} is an accessible, affordable, and extensible open source platform for mobile eye tracking and gaze-based interaction. In this case, the eye is tracked using an extra camera as the origin point and the pointing direction is also calculated from the eye tracking. 
(D) \citet{ma2015ismar} proposed to find a virtual eye center as the pointing origin to enable the interaction with ambient objects. The disadvantage of this method was that the user had to keep their head still and the calibration procedure was not natural and user friendly.

Usually the head, eye, hand, or finger are detected and tracked to recover the pointing ray. Hence, the interaction techniques are implemented with different features, such as with or without visual feedback, direct or relative interaction, dynamic or static scene, limited or unlimited distance in the work space, feasible in the real world or virtual world.
The objects to detect and track and the properties of the interaction the user can perform are compared among the above-mentioned works of eye-rooted pointing technologies and natural interaction in the egocentric setting (see \tablename{ \ref{tb:2-bg:pointing}}).
\input{content/tables/2-bg-pointing}
\subsection{Natural interaction in OR}
One approach is to insert a barrier between the sterile gloves and the non-sterile input device \cite{Ionescu2006}. This solution is very simple but involves certain inherent risks due to the potential damage to the barrier. Computer vision techniques are also being used to seek interaction methods in the operating room that avoid the need for contact with an input device.
A non-contact mouse was developed in \cite{Gratzel2004a} using a stereo camera and provided the surgeon standard mouse functions to control a single medical system. Even if they only developed simple gestures to control the cursor movement and clicking, the interactions were not performed smoothly. This also required each medical system to be augmented by a pair of stereo cameras which increases the hardware complexity in the OR. 
Washs \textit{et al.} \cite{Wachs2008} developed more sophisticated gestures for image browsing.
Quite a few touchless systems \cite{Strickland2013,Ebert2013,Tan2013} for the OR were developed after the Kinect sensor was released, as it has lower barriers and can be easily adapted.
Ebert \textit{et al.} \cite{Ebert2013} presented a system which allows the control of the open source Picture Archiving and Communication System (PACS) OsiriX by means of gesture and voice commands.   
The system in \cite{Strickland2013} helps navigate a predefined stack of MRI/CT images, using a simple constrained gesture set to move forward or backward through the image slices. 
The work in \cite{Tan2013} builds compound gestures that combine dominant and non-dominant hand, which were used for selecting, and other particular functions and modes. 
Once Leap Motion commercially appeared, a custom workstation was also set up in order to manipulate the imaging software by hand gesture \cite{Rosa2014}. 
All these systems have a similar setup as the depth sensor is placed next to the screen to perform outside-in tracking while observing the surgeon to detect the gestures which are subsequently mapped to different functions. 

A set of socio-technical issues, or human factors, arise when these systems are used in the context of an actual surgery, including collaboration, engagement and disengagement. 
Jalaliniya \textit{et al.} \cite{Jalaliniya2013} and Schwarz \textit{et al.} \cite{Schwarz2011a} presented user interfaces for gesture-based interaction with medical images via a wristband sensor or capacitive floor sensors. However, interaction with medical images or systems in a surgical setting often extends beyond simple navigation, requiring a much richer set of image manipulation options. 
Along with the larger gesture sets enabled by this expressiveness comes concern over the learnability of the system - natural user interfaces are not natural any more \cite{Norman2010a}. 
Several strategies have been also explored for touchless interacting with medical images, through speech recognition and egocentric interaction. 
Speech recognition is used in \cite{Ebert2012}, however voice-recognition software involves special challenges in noisy operating rooms and often are exhausting when repeated frequently during surgeries throughout the day. The egocentric interaction paradigm \cite{Pederson2010} redefines the relationship between the human, computer system, and world, and a wearable personal prototype for surgeons is developed based on Google Glass \cite{Jalaliniya2015}.