% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Argelaguet2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Argelaguet}{A.}%
     {F.}{F.}%
     {}{}%
     {}{}}%
    {{}%
     {Andujar}{A.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Trueba}{T.}%
     {R.}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{2006,2007,3d selection,been stud-,ied considering solely either,of
  partially-occluded objects,olwal and feiner 2003,peter et al,raycasting,the
  eye position or,the hand position,the occlusion problem has,vanacken et
  al,virtual pointer}
  \strng{namehash}{AF+1}
  \strng{fullhash}{AFACTR1}
  \field{labelalpha}{Arg+08}
  \field{sortinit}{A}
  \field{abstract}{%
  Most pointing techniques for 3D selection on virtual environments rely on a
  ray originating at the user's hand whose direction is controlled by the hand
  orientation. In this paper we study the potential mismatch between visible
  objects (those which appear unoccluded from the user's eye position) and
  selectable objects (those which appear unoccluded from the user's hand
  position). We study the impact of such eye-hand visibility mismatch on
  selection performance, and propose a new technique for ray control which
  attempts to overcome this problem. We present an experiment to compare our
  ray control technique with classic raycasting in selection tasks with complex
  3D scenes. Our user studies show promising results of our technique in terms
  of speed and accuracy.%
  }
  \verb{doi}
  \verb http://doi.acm.org/10.1145/1450579.1450588
  \endverb
  \field{isbn}{9781595939517}
  \field{pages}{43\bibrangedash 46}
  \field{title}{{Overcoming eye-hand visibility mismatch in 3D pointing
  selection}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?id=1450588
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Argelaguet, Andujar, Trueba/Proceedings of the 2
  \verb 008 ACM symposium on Virtual reality software and technology/Argelaguet
  \verb , Andujar, Trueba - 2008 - Overcoming eye-hand visibility mismatch in 3
  \verb D pointing selection.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 2008 ACM Symp. Virtual Real. Softw. Technol.}
  \field{year}{2008}
\endentry

\entry{Aubin2012}{article}{}
  \name{author}{7}{}{%
    {{}%
     {Aubin}{A.}%
     {Diane}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {King}{K.}%
     {Sharla}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Boechler}{B.}%
     {Patricia}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Burden}{B.}%
     {Michael}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Rockwell}{R.}%
     {Geoffrey}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Henry}{H.}%
     {Monica}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Gouglas}{G.}%
     {Sean}{S.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Informa Healthcare}%
  }
  \keyw{Education,Feedback,Games,Physicians,Safety,feedback,patient safety
  education,physicians,serious games}
  \strng{namehash}{AD+1}
  \strng{fullhash}{ADKSBPBMRGHMGS1}
  \field{labelalpha}{Aub+12}
  \field{sortinit}{A}
  \field{abstract}{%
  The purpose of this study is to pilot test an inexpensive prototype of a
  serious game with a group of interprofessional health students to determine
  if the game provides opportunities to learn about patient safety. The game
  was constructed around a series of scenarios created in consultation with a
  physician, enabling the player to explore patient safety-related learning
  objectives within a virtual hospital setting. Players enter into
  conversations with characters and choose what response to provide from a
  given selection. At the end of the game, students review their actions, and
  are provided feedback about their choices. Fourteen participants tested the
  prototype and filled out the questionnaire. The evaluation of the prototype
  demonstrated that there is potential for this tool to help students learn to
  overcome some of the barriers to communications and teamwork that can lead to
  improved patient safety. The participants enjoyed playing the game, learned
  something about team communications and thought this was a valid method for
  learning about patient safety. In addition, this game would be of great
  benefit to teaching through reflective practice. (PsycINFO Database Record
  (c) 2012 APA, all rights reserved)%
  }
  \verb{doi}
  \verb 10.3109/0142159X.2012.689448
  \endverb
  \field{issn}{0142-159X}
  \field{number}{8}
  \field{pages}{675}
  \field{title}{{Serious games for patient safety education.}}
  \verb{url}
  \verb http://search.ebscohost.com/login.aspx?direct=true{\&}db=psyh{\&}AN=201
  \verb 2-20359-019{\&}login.asp{\&}site=ehost-live{\&}scope=site$\backslash$nv
  \verb ist@ualberta.ca
  \endverb
  \field{volume}{34}
  \field{journaltitle}{Med. Teach.}
  \field{year}{2012}
\endentry

\entry{Bacca2014}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Bacca}{B.}%
     {Jorge}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Baldiris}{B.}%
     {Silvia}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Fabregat}{F.}%
     {Ramon}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Graf}{G.}%
     {Sabine}{S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Inclusive learning in augmented
  reality,Personalization,Systematic review,Trends of AR}
  \strng{namehash}{BJ+1}
  \strng{fullhash}{BJBSFRGS1}
  \field{labelalpha}{Bac+14}
  \field{sortinit}{B}
  \field{abstract}{%
  In recent years, there has been an increasing interest in applying Augmented
  Reality (AR) to create unique educational settings. So far, however, there is
  a lack of review studies with focus on investigating factors such as: the
  uses, advantages, limitations, effectiveness, challenges and features of
  augmented reality in educational settings. Personalization for promoting an
  inclusive learning using AR is also a growing area of interest. This paper
  reports a systematic review of literature on augmented reality in educational
  settings considering the factors mentioned before. In total, 32 studies
  published between 2003 and 2013 in 6 indexed journals were analyzed. The main
  findings from this review provide the current state of the art on research in
  AR in education. Furthermore, the paper discusses trends and the vision
  towards the future and opportunities for further research in augmented
  reality for educational settings.%
  }
  \field{issn}{1436-4522}
  \field{number}{4}
  \field{pages}{133\bibrangedash 149}
  \field{title}{{Augmented Reality Trends in Education: A Systematic Review of
  Research and Applications}}
  \verb{url}
  \verb http://www.ifets.info/others/abstract.php?art{\_}id=1521
  \endverb
  \field{volume}{17}
  \verb{file}
  \verb :E$\backslash$:/papers/Bacca et al/Educational Technology {\&} Society/
  \verb Bacca et al. - 2014 - Augmented Reality Trends in Education A Systemati
  \verb c Review of Research and Applications.pdf:pdf
  \endverb
  \field{journaltitle}{Educ. Technol. Soc.}
  \field{year}{2014}
\endentry

\entry{Banerjee2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Banerjee}{B.}%
     {Amartya}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Burstyn}{B.}%
     {Jesse}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Girouard}{G.}%
     {Audrey}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Vertegaal}{V.}%
     {Roel}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{Input device,Interaction technique,Large display,Multi-touch,Remote
  interaction}
  \strng{namehash}{BA+1}
  \strng{fullhash}{BABJGAVR1}
  \field{labelalpha}{Ban+12}
  \field{sortinit}{B}
  \field{abstract}{%
  We present MultiPoint, a set of perspective-based remote pointing techniques
  that allows users to perform bimanual and multi-finger remote manipulation of
  graphical objects on large displays. We conducted two empirical studies that
  compared remote pointing techniques performed using fingers and laser
  pointers, in single and multi-finger pointing interactions. We explored three
  types of manual selection gestures: squeeze, breach and trigger. The fastest
  and most preferred technique was the trigger gesture in the single point
  experiment and the unimanual breach gesture in the multi-finger pointing
  study. The laser pointer obtained mixed results: it is fast, but inaccurate
  in single point, and it obtained the lowest ranking and performance in the
  multipoint experiment. Our results suggest MultiPoint interaction techniques
  are superior in performance and accuracy to traditional laser pointers for
  interacting with graphical objects on a large display from a distance.%
  }
  \verb{doi}
  \verb 10.1016/j.ijhcs.2012.05.009
  \endverb
  \field{issn}{10715819}
  \field{number}{10}
  \field{pages}{690\bibrangedash 702}
  \field{title}{{MultiPoint: Comparing laser and manual pointing as remote
  input in large display interactions}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S1071581912000985
  \endverb
  \field{volume}{70}
  \field{journaltitle}{Int. J. Hum. Comput. Stud.}
  \field{year}{2012}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Bichlmeier2007}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Bichlmeier}{B.}%
     {Christoph}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Wimmer}{W.}%
     {Felix}{F.}%
     {}{}%
     {}{}}%
    {{}%
     {Heining}{H.}%
     {Sandro~Michael}{S.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Navab}{N.}%
     {Nassir}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{H.5.1 [Information Interfaces and Presentation]: M,H.5.2 [Information
  Interfaces and Presentation]: U,and virtual realities,augmented}
  \strng{namehash}{BC+1}
  \strng{fullhash}{BCWFHSMNN1}
  \field{labelalpha}{Bic+07}
  \field{sortinit}{B}
  \field{abstract}{%
  The need to improve medical diagnosis and reduce invasive surgery is
  dependent upon seeing into a living human system. The use of diverse types of
  medical imaging and endoscopic instruments has provided significant
  breakthroughs, but not without limiting the surgeon's natural, intuitive and
  direct 3D perception into the human body. This paper presents a method for
  the use of augmented reality (AR) for the convergence of improved perception
  of 3D medical imaging data (mimesis) in context to the patient's own anatomy
  (in-situ) incorporating the physician's intuitive multi- sensory interaction
  and integrating direct manipulation with endoscopic instruments. Transparency
  of the video images recorded by the color cameras of a video see-through,
  stereoscopic head- mounted-display (HMD) is adjusted according to the
  position and line of sight of the observer, the shape of the patient's skin
  and the location of the instrument. The modified video image of the real
  scene is then blended with the previously rendered virtual anatomy. The
  effectiveness has been demonstrated in a series of experiments at the
  Chirurgische Klinik in Munich, Germany with cadaver and in-vivo studies. The
  results can be applied for designing medical AR training and educational
  applications.%
  }
  \verb{doi}
  \verb 10.1109/ISMAR.2007.4538837
  \endverb
  \field{isbn}{9781424417506}
  \field{pages}{129\bibrangedash 138}
  \field{title}{{Contextual anatomic mimesis: Hybrid in-situ visualization
  method for improving multi-sensory depth perception in medical augmented
  reality}}
  \verb{file}
  \verb :E$\backslash$:/papers/Bichlmeier et al/2007 6th IEEE and ACM Internati
  \verb onal Symposium on Mixed and Augmented Reality, ISMAR/Bichlmeier et al.
  \verb - 2007 - Contextual anatomic mimesis Hybrid in-situ visualization metho
  \verb d for improving multi-sensory depth perception.pdf:pdf
  \endverb
  \field{journaltitle}{2007 6th IEEE ACM Int. Symp. Mix. Augment. Reality,
  ISMAR}
  \field{year}{2007}
\endentry

\entry{Blum2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Blum}{B.}%
     {Tobias}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Kleeberger}{K.}%
     {Valerie}{V.}%
     {}{}%
     {}{}}%
    {{}%
     {Bichlmeier}{B.}%
     {Christoph}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Navab}{N.}%
     {Nassir}{N.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Ieee}%
  }
  \keyw{H.5.1 [Information Interfaces and Presentation]: A}
  \strng{namehash}{BT+1}
  \strng{fullhash}{BTKVBCNN1}
  \field{labelalpha}{Blu+12}
  \field{sortinit}{B}
  \field{abstract}{%
  The mirracle system extends the concept of an Augmented Reality (AR) magic
  mirror to the visualization of human anatomy on the body of the user. Using a
  medical volume renderer a CT dataset is augmented onto the user. By a slice
  based user interface, slice from the CT and an additional photographic
  dataset can be selected.%
  }
  \verb{doi}
  \verb 10.1109/VR.2012.6180934
  \endverb
  \field{isbn}{9781467312462}
  \field{pages}{169\bibrangedash 170}
  \field{title}{{Mirracle: Augmented reality in-situ visualization of human
  anatomy using a magic mirror}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6180934
  \endverb
  \field{journaltitle}{Proc. - IEEE Virtual Real.}
  \field{year}{2012}
\endentry

\entry{Carpendale2010}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Carpendale}{C.}%
     {Jeremy I~M}{J.~I.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Carpendale}{C.}%
     {Ailidh~B.}{A.~B.}%
     {}{}%
     {}{}}%
  }
  \keyw{Gestures,Infancy,Pointing,Social development}
  \strng{namehash}{CJIMCAB1}
  \strng{fullhash}{CJIMCAB1}
  \field{labelalpha}{CC10}
  \field{sortinit}{C}
  \field{abstract}{%
  Although there is consensus about the importance of early communicative
  gestures such as pointing, there is an ongoing debate regarding how infants
  develop the ability to understand and produce pointing gestures. We review
  competing theories regarding this development and use observations from a
  diary study of infants social development, focusing primarily on one infant
  from 6 to 14 months to illustrate a currently neglected view of the
  development of pointing. According to this view, infants first use their
  extended index finger as a manifestation of their own attention that emerges
  from their tactile exploration of close-by objects. Their gesture gradually
  becomes social in its use as infants become aware of the meaning of their
  action for adults.%
  }
  \verb{doi}
  \verb 10.1159/000315168
  \endverb
  \field{isbn}{0018-716X}
  \field{issn}{0018716X}
  \field{number}{3}
  \field{pages}{110\bibrangedash 126}
  \field{title}{{The development of pointing: From personal directedness to
  interpersonal direction}}
  \verb{url}
  \verb http://www.karger.com/Article/Fulltext/315168
  \endverb
  \field{volume}{53}
  \field{journaltitle}{Hum. Dev.}
  \field{year}{2010}
\endentry

\entry{Council2009}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Council}{C.}%
     {General~Medical}{G.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {{General Medical Council}}{G.}%
     {}{}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CGMG1}
  \strng{fullhash}{CGMG1}
  \field{labelalpha}{CG09}
  \field{sortinit}{C}
  \field{abstract}{%
  The GMC sets the knowledge, skills and behaviours that medical students learn
  at UK medical schools: these are the outcomes that new UK graduates must be
  able to demonstrate. The GMC also sets standards for teaching, learning and
  assessment. These outcomes and standards are laid down in Tomorrow's Doctors
  (2009) (pdf)%
  }
  \verb{doi}
  \verb http://www.gmc-uk.or
  \endverb
  \field{isbn}{9780901458360}
  \field{issn}{0308-0110}
  \field{number}{1}
  \field{pages}{1\bibrangedash 108}
  \field{title}{{Tomorrow ’ s Doctors The duties of a doctor registered with
  the General Medical Council}}
  \verb{url}
  \verb http://www.gmc-uk.org/TomorrowsDoctors{\_}2009.pdf{\_}39260971.pdf
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Council, General Medical Council/2009/Council, G
  \verb eneral Medical Council - 2009 - Tomorrow ’ s Doctors The duties of a
  \verb doctor registered with the General Medical Council.pdf:pdf
  \endverb
  \field{journaltitle}{2009}
  \field{year}{2009}
\endentry

\entry{Chang2014}{article}{}
  \name{author}{6}{}{%
    {{}%
     {Chang}{C.}%
     {Kuo~En}{K.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Chia~Tzu}{C.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Hou}{H.}%
     {Huei~Tse}{H.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Sung}{S.}%
     {Yao~Ting}{Y.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Chao}{C.}%
     {Huei~Lin}{H.~L.}%
     {}{}%
     {}{}}%
    {{}%
     {Lee}{L.}%
     {Cheng~Ming}{C.~M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Applications in subject areas,Architectures for educational technology
  system,Interactive learning environments,Teaching/learning strategies}
  \strng{namehash}{CKE+1}
  \strng{fullhash}{CKECCTHHTSYTCHLLCM1}
  \field{labelalpha}{Cha+14}
  \field{sortinit}{C}
  \field{abstract}{%
  A mobile guide system that integrates art appreciation instruction with
  augmented reality (AR) was designed as an auxiliary tool for painting
  appreciation, and the learning performance of three groups of visiting
  participants was explored: AR-guided, audio-guided, and nonguided (i.e.,
  without carrying auxiliary devices). The participants were 135 college
  students, and a quasi-experimental research design was employed. Several
  learning performance factors of the museum visitors aided with different
  guided modes were evaluated, including their learning effectiveness, flow
  experience, the amount of time spent focusing on the paintings, behavioral
  patterns, and attitude of using the guide systems. The results showed that
  compared to the audio- and nonguided participants, the AR guide effectively
  enhanced visitors' learning effectiveness, promoted their flow experience,
  and extended the amount of time the visitors spent focusing on the paintings.
  In addition, the visitors' behavioral patterns were dependent upon the guided
  mode that they used; the visitors who were the most engaged in the gallery
  experience were those who were using the AR guide. Most of the visitors using
  the mobile AR-guide system elicited positive responses and acceptance
  attitudes. ?? 2013 Elsevier Ltd. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.compedu.2013.09.022
  \endverb
  \field{issn}{03601315}
  \field{number}{July 2015}
  \field{pages}{185\bibrangedash 197}
  \field{title}{{Development and behavioral pattern analysis of a mobile guide
  system with augmented reality for painting appreciation instruction in an art
  museum}}
  \field{volume}{71}
  \field{journaltitle}{Comput. Educ.}
  \field{year}{2014}
\endentry

\entry{Colaco2013a}{inproceedings}{}
  \name{author}{6}{}{%
    {{}%
     {Cola{\c{c}}o}{C.}%
     {Andrea}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Kirmani}{K.}%
     {Ahmed}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Yang}{Y.}%
     {Hye~Soo}{H.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Gong}{G.}%
     {Nan-Wei}{N.-W.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmandt}{S.}%
     {Chris}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Goyal}{G.}%
     {Vivek~K.}{V.~K.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \keyw{3d sensing,gesture sensing,glasses,hand tracking,head mounted
  displays,mobile,time-of- flight imaging}
  \strng{namehash}{CA+1}
  \strng{fullhash}{CAKAYHSGNWSCGVK1}
  \field{labelalpha}{Col+13}
  \field{sortinit}{C}
  \field{abstract}{%
  Computer techniques now emerging in the laboratory promise new capabilities
  for voice communication between man and machine. Three modes of interaction
  are of special interest: computer voice readout of stored information,
  automatic verification of a caller's identity by means of his voice signal,
  and automatic recognition of spoken commands. Applications extend to:
  voice-directed installation of telephone equipment, authentication by voice
  of a credit customer or of an individual requesting readout of privileged
  information, and voice-controlled services such as repertory dialing or
  automatic booking of travel reservations.%
  }
  \field{booktitle}{Proc. 26th Annu. ACM Symp. User interface Softw. Technol. -
  UIST '13}
  \verb{doi}
  \verb 10.1145/2501988.2502042
  \endverb
  \field{isbn}{9781450322683}
  \field{pages}{227\bibrangedash 236}
  \field{title}{{Mime}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2501988.2502042
  \endverb
  \list{location}{1}{%
    {New York, New York, USA}%
  }
  \verb{file}
  \verb :E$\backslash$:/papers/Cola{\c{c}}o et al/Proceedings of the 26th annua
  \verb l ACM symposium on User interface software and technology - UIST '13/Co
  \verb la{\c{c}}o et al. - 2013 - Mime.pdf:pdf
  \endverb
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Consultation2008}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Consultation}{C.}%
     {W~H O~Expert}{W.~H. O.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{CWHOE1}
  \strng{fullhash}{CWHOE1}
  \field{labelalpha}{Con08}
  \field{sortinit}{C}
  \field{abstract}{%
  The World Health Organization (WHO) Expert Consultation on Waist
  Circumference and Waist–Hip Ratio was held in Geneva, Switzerland on 8–11
  December 2008. The consultation was organized by WHO’s Department of
  Nutrition for Health and Development, in collaboration with the Department of
  Chronic Diseases and Health Promotion. It was opened by Dr Ala Alwan, WHO
  Assistant Director‐General for Noncommunicable Diseases and Mental Health.
  The consultation was convened as part of WHO's: • efforts in implementing
  the recommendations made at the WHO Consultation on Appropriate Body Mass
  Index for Asian Populations (WHO, 2004); • response to the emerging problem
  of obesity and related chronic diseases, in particular in low‐ and
  middle‐income countries.%
  }
  \verb{doi}
  \verb 10.1038/ejcn.2009.139
  \endverb
  \verb{eprint}
  \verb /www.nutrociencia.com.br/upload{\_}files/artigos{\_}download/Distribui{
  \verb \%}C3{\%}A7{\%}C3{\%}A3o{\%}20de{\%}20gordura{\%}20corporal.pdf
  \endverb
  \field{isbn}{1476-5640 (Electronic)$\backslash$n1476-5640 (Linking)}
  \field{issn}{1476-5640}
  \field{number}{December}
  \field{pages}{8\bibrangedash 11}
  \field{title}{{Waist Circumference and Waist-Hip Ratio Report of a WHO Expert
  Consultation}}
  \field{journaltitle}{World Health}
  \field{eprinttype}{arXiv}
  \field{eprintclass}{http:}
  \field{year}{2008}
\endentry

\entry{Davis2002}{article}{}
  \name{author}{11}{}{%
    {{}%
     {Davis}{D.}%
     {Larry}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Hamza-Lup}{H.-L.}%
     {Felix~G.}{F.~G.}%
     {}{}%
     {}{}}%
    {{}%
     {Daly}{D.}%
     {Jason}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Ha}{H.}%
     {Yonggang}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Frolich}{F.}%
     {Seth}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Meyer}{M.}%
     {Catherine}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Martin}{M.}%
     {Glenn}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Norfleet}{N.}%
     {Jack}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Lin}{L.}%
     {Kuo-Chi}{K.-C.}%
     {}{}%
     {}{}}%
    {{}%
     {Imielinska}{I.}%
     {Celina}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Rolland}{R.}%
     {Jannick~P.}{J.~P.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Spie}%
  }
  \keyw{augmented reality,head-mounted projective display,hmpd,human
  patient,medical visualization}
  \strng{namehash}{DL+1}
  \strng{fullhash}{DLHLFGDJHYFSMCMGNJLKCICRJP1}
  \field{labelalpha}{Dav+02}
  \field{sortinit}{D}
  \field{abstract}{%
  Visualizing information in three dimensions provides an increased
  understanding of the data presented. Furthermore, the ability to manipulate
  or interact with data visualized in three dimensions is superior. Within the
  medical community, augmented reality is being used for interactive,
  three-dimensional (3D) visualization. This type of visualization, which
  enhances the real world with computer generated information, requires a
  display device, a computer to generate the 3D data, and a system to track the
  user. In addition to these requirements, however, the hardware must be
  properly integrated to insure correct visualization. To this end, we present
  components of an integrated augmented reality system consisting of a novel
  head-mounted projective display, a Linux-based PC, and a commercially
  available optical tracking system. We demonstrate the system with the
  visualization of anatomical airways superimposed on a human patient
  simulator.%
  }
  \verb{doi}
  \verb 10.1117/12.478890
  \endverb
  \field{isbn}{0-8194-4461-8}
  \field{issn}{0277786X}
  \field{pages}{400\bibrangedash 405}
  \field{title}{{Application of Augmented Reality to Visualizing Anatomical
  Airways}}
  \verb{url}
  \verb http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=885
  \verb 156
  \endverb
  \field{volume}{4711}
  \field{journaltitle}{Proc. SPIE}
  \field{year}{2002}
\endentry

\entry{Dede2009}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Dede}{D.}%
     {C.}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{DC1}
  \strng{fullhash}{DC1}
  \field{labelalpha}{Ded09}
  \field{sortinit}{D}
  \field{abstract}{%
  Immersion is the subjective impression that one is participating in a
  comprehensive, realistic experience. Interactive media now enable various
  degrees of digital immersion. The more a virtual immersive experience is
  based on design strategies that combine actional, symbolic, and sensory
  factors, the greater the participant's suspension of disbelief that she or he
  is "inside" a digitally enhanced setting. Studies have shown that immersion
  in a digital environment can enhance education in at least three ways: by
  allowing multiple perspectives, situated learning, and transfer. Further
  studies are needed on the capabilities of immersive media for learning, on
  the instructional designs best suited to each type of immersive medium, and
  on the learning strengths and preferences these media develop in users.%
  }
  \verb{doi}
  \verb 10.1126/science.1167311
  \endverb
  \field{isbn}{1095-9203}
  \field{issn}{0036-8075}
  \field{number}{5910}
  \field{pages}{66\bibrangedash 69}
  \field{title}{{Immersive Interfaces for Engagement and Learning}}
  \verb{url}
  \verb http://www.sciencemag.org/cgi/doi/10.1126/science.1167311
  \endverb
  \field{volume}{323}
  \field{journaltitle}{Science (80-. ).}
  \field{year}{2009}
\endentry

\entry{Delp2007}{article}{}
  \name{author}{8}{}{%
    {{}%
     {Delp}{D.}%
     {Scott~L.}{S.~L.}%
     {}{}%
     {}{}}%
    {{}%
     {Anderson}{A.}%
     {Frank~C.}{F.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Arnold}{A.}%
     {Allison~S.}{A.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Loan}{L.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Habib}{H.}%
     {Ayman}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {John}{J.}%
     {Chand~T.}{C.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Guendelman}{G.}%
     {Eran}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Thelen}{T.}%
     {Darryl~G.}{D.~G.}%
     {}{}%
     {}{}}%
  }
  \keyw{Computed muscle control,Forward dynamic simulation,Musculoskeletal
  modeling,Open-source software}
  \strng{namehash}{DSL+1}
  \strng{fullhash}{DSLAFCAASLPHAJCTGETDG1}
  \field{labelalpha}{Del+07}
  \field{sortinit}{D}
  \field{abstract}{%
  Dynamic simulations of movement allow one to study neuromuscular
  coordination, analyze athletic performance, and estimate internal loading of
  the musculoskeletal system. Simulations can also be used to identify the
  sources of pathological movement and establish a scientific basis for
  treatment planning. We have developed a freely available, open-source
  software system (OpenSim) that lets users develop models of musculoskeletal
  structures and create dynamic simulations of a wide variety of movements. We
  are using this system to simulate the dynamics of individuals with
  pathological gait and to explore the biomechanical effects of treatments.
  OpenSim provides a platform on which the biomechanics community can build a
  library of simulations that can be exchanged, tested, analyzed, and improved
  through a multi-institutional collaboration. Developing software that enables
  a concerted effort from many investigators poses technical and sociological
  challenges. Meeting those challenges will accelerate the discovery of
  principles that govern movement control and improve treatments for
  individuals with movement pathologies.%
  }
  \verb{doi}
  \verb 10.1109/TBME.2007.901024
  \endverb
  \field{isbn}{0018-9294 (Print)$\backslash$n0018-9294 (Linking)}
  \field{issn}{0018-9294}
  \field{number}{11}
  \field{pages}{1940\bibrangedash 1950}
  \field{title}{{OpenSim: Open-Source Software to Create and Analyze Dynamic
  Simulations of Movement}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4352056
  \endverb
  \field{volume}{54}
  \field{journaltitle}{IEEE Trans. Biomed. Eng.}
  \field{year}{2007}
\endentry

\entry{VanDongen2011}{inproceedings}{}
  \name{author}{10}{}{%
    {{}%
     {Dongen}{D.}%
     {Koen~W}{K.~W.}%
     {van}{v.}%
     {}{}}%
    {{}%
     {Ahlberg}{A.}%
     {Gunnar}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Bonavina}{B.}%
     {Luigi}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Carter}{C.}%
     {Fiona~J}{F.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Grantcharov}{G.}%
     {Teodor~P}{T.~P.}%
     {}{}%
     {}{}}%
    {{}%
     {Hyltander}{H.}%
     {Anders}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Schijven}{S.}%
     {Marlies~P}{M.~P.}%
     {}{}%
     {}{}}%
    {{}%
     {Stefani}{S.}%
     {Alessandro}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Zee}{Z.}%
     {David~C}{D.~C.}%
     {van~der}{v.~d.}%
     {}{}}%
    {{}%
     {Broeders}{B.}%
     {Ivo A M~J}{I.~A. M.~J.}%
     {}{}%
     {}{}}%
  }
  \list{organization}{1}{%
    {University Medical Centre, Utrecht, The Netherlands.}%
  }
  \keyw{Clinical Competence,Computer Simulation,Computer-Assisted
  Instruction,Computer-Assisted Instruction: instrumentation,Computer-Assisted
  Instruction: methods,Educational Measurement,Endoscopy,Endoscopy:
  education,Equipment Design,Europe,Humans,Internship and
  Residency,Laparoscopy,Laparoscopy: education,Learning Curve,Motor
  Skills,Psychomotor Performance,User-Computer Interface}
  \strng{namehash}{DKWv+1}
  \strng{fullhash}{DKWvAGBLCFJGTPHASMPSAZDCvdBIAMJ1}
  \field{labelalpha}{Don+11}
  \field{sortinit}{D}
  \field{abstract}{%
  BACKGROUND: Virtual reality (VR) simulators have been demonstrated to improve
  basic psychomotor skills in endoscopic surgery. The exercise configuration
  settings used for validation in studies published so far are default settings
  or are based on the personal choice of the tutors. The purpose of this study
  was to establish consensus on exercise configurations and on a validated
  training program for a virtual reality simulator, based on the experience of
  international experts to set criterion levels to construct a
  proficiency-based training program.$\backslash$n$\backslash$nMETHODS: A
  consensus meeting was held with eight European teams, all extensively
  experienced in using the VR simulator. Construct validity of the training
  program was tested by 20 experts and 60 novices. The data were analyzed by
  using the t test for equality of means.$\backslash$n$\backslash$nRESULTS:
  Consensus was achieved on training designs, exercise configuration, and
  examination. Almost all exercises (7/8) showed construct validity. In total,
  50 of 94 parameters (53{\%}) showed significant
  difference.$\backslash$n$\backslash$nCONCLUSIONS: A European, multicenter,
  validated, training program was constructed according to the general
  consensus of a large international team with extended experience in virtual
  reality simulation. Therefore, a proficiency-based training program can be
  offered to training centers that use this simulator for training in basic
  psychomotor skills in endoscopic surgery.%
  }
  \field{booktitle}{Surg. Endosc.}
  \verb{doi}
  \verb 10.1007/s00464-010-1151-6
  \endverb
  \field{isbn}{1432-2218 (Electronic)$\backslash$r0930-2794 (Linking)}
  \field{issn}{1432-2218}
  \field{number}{1}
  \field{pages}{166\bibrangedash 71}
  \field{title}{{European consensus on a competency-based virtual reality
  training program for basic endoscopic surgical psychomotor skills.}}
  \verb{url}
  \verb http://www.scopus.com/inward/record.url?eid=2-s2.0-79251603641{\&}partn
  \verb erID=tZOtx3y1
  \endverb
  \field{volume}{25}
  \field{year}{2011}
\endentry

\entry{Dunleavy2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Dunleavy}{D.}%
     {Matt}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Dede}{D.}%
     {Chris}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Mitchell}{M.}%
     {Rebecca}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Classroom technology practices,GPS devices,Handheld
  devices,Immersive participatory simulations}
  \strng{namehash}{DM+1}
  \strng{fullhash}{DMDCMR1}
  \field{labelalpha}{Dun+09}
  \field{sortinit}{D}
  \field{abstract}{%
  Abstract The purpose of this study was to document how teachers and students
  describe and comprehend the ways in which participating in an augmented
  reality (AR) simulation aids or hinders teaching and learning. Like the
  multi-user virtual environment (MUVE) interface that underlies Internet
  games, AR is a good medium for immersive collaborative simulation, but has
  different strengths and limitations than MUVEs. Within a design-based
  research project, the researchers conducted multiple qualitative case studies
  across two middle schools (6th and 7th grade) and one high school (10th
  grade) in the northeastern United States to document the affordances and
  limitations of AR simulations from the student and teacher perspective. The
  researchers collected data through formal and informal interviews, direct
  observations, web site posts, and site documents. Teachers and students
  reported that the technology-mediated narrative and the interactive,
  situated, collaborative problem solving affordances of the AR simulation were
  highly engaging, especially among students who had previously presented
  behavioral and academic challenges for the teachers. However, while the AR
  simulation provided potentially transformative added value, it simultaneously
  presented unique technological, managerial, and cognitive challenges to
  teaching and learning.%
  }
  \verb{doi}
  \verb 10.1007/s10956-008-9119-1
  \endverb
  \field{isbn}{1059-0145}
  \field{issn}{1059-0145}
  \field{number}{1}
  \field{pages}{7\bibrangedash 22}
  \field{title}{{Affordances and Limitations of Immersive Participatory
  Augmented Reality Simulations for Teaching and Learning}}
  \verb{url}
  \verb http://link.springer.com/10.1007/s10956-008-9119-1
  \endverb
  \field{volume}{18}
  \verb{file}
  \verb :E$\backslash$:/papers/Dunleavy, Dede, Mitchell/Journal of Science Educ
  \verb ation and Technology/Dunleavy, Dede, Mitchell - 2009 - Affordances and
  \verb Limitations of Immersive Participatory Augmented Reality Simulations fo
  \verb r Teaching and.pdf:pdf
  \endverb
  \field{journaltitle}{J. Sci. Educ. Technol.}
  \field{year}{2009}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Ebert2012}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Ebert}{E.}%
     {L.~C.}{L.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Hatch}{H.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Ampanozi}{A.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Thali}{T.}%
     {M.~J.}{M.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Ross}{R.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{ELC+1}
  \strng{fullhash}{ELCHGAGTMJRS1}
  \field{labelalpha}{Ebe+12}
  \field{sortinit}{E}
  \field{abstract}{%
  Keyboards, mice, and touch screens are a potential source of infection or
  contamination in operating rooms, intensive care units, and autopsy suites.
  The authors present a low-cost prototype of a system, which allows for
  touch-free control of a medical image viewer. This touch-free navigation
  system consists of a computer system (IMac, OS X 10.6 Apple, USA) with a
  medical image viewer (OsiriX, OsiriX foundation, Switzerland) and a depth
  camera (Kinect, Microsoft, USA). They implemented software that translates
  the data delivered by the camera and a voice recognition software into
  keyboard and mouse commands, which are then passed to OsiriX. In this
  feasibility study, the authors introduced 10 medical professionals to the
  system and asked them to re-create 12 images from a CT data set. They
  evaluated response times and usability of the system compared with standard
  mouse/keyboard control. Users felt comfortable with the system after
  approximately 10 minutes. Response time was 120 ms. Users required 1.4 times
  more time to re-create an image with gesture control. Users with OsiriX
  experience were significantly faster using the mouse/keyboard and faster than
  users without prior experience. They rated the system 3.4 out of 5 for ease
  of use in comparison to the mouse/keyboard. The touch-free,
  gesture-controlled system performs favorably and removes a potential vector
  for infection, protecting both patients and staff. Because the camera can be
  quickly and easily integrated into existing systems, requires no calibration,
  and is low cost, the barriers to using this technology are low.%
  }
  \verb{doi}
  \verb 10.1177/1553350611425508
  \endverb
  \field{isbn}{1553-3514 (Electronic)$\backslash$r1553-3506 (Linking)}
  \field{issn}{1553-3506}
  \field{number}{3}
  \field{pages}{301\bibrangedash 307}
  \field{title}{{You Can't Touch This: Touch-free Navigation Through
  Radiological Images}}
  \field{volume}{19}
  \field{journaltitle}{Surg. Innov.}
  \field{year}{2012}
\endentry

\entry{Ebert2013}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Ebert}{E.}%
     {L.~C.}{L.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Hatch}{H.}%
     {G.}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Thali}{T.}%
     {M.~J.}{M.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Ross}{R.}%
     {S.}{S.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {English}%
  }
  \list{publisher}{1}{%
    {Elsevier}%
  }
  \keyw{Gesture detection,Kinect,OsiriX,PACS,Virtopsy}
  \strng{namehash}{ELC+1}
  \strng{fullhash}{ELCHGTMJRS1}
  \field{labelalpha}{Ebe+13}
  \field{sortinit}{E}
  \field{abstract}{%
  With the increasing use of imaging technologies during surgeries and
  autopsies, new control methods for computer systems are required to maintain
  sterility. Gesture controlled systems seem to be promising, since they allow
  for a touch-free control of computer systems. In a previous publication we
  presented a system which allows the control of the open source Picture
  Archiving and Communication System (PACS) OsiriX by means of gesture and
  voice commands. In order to overcome the limitations of this system, we
  developed a plug-in for OsiriX that allows for gesture control of the DICOM
  viewer of OsiriX with finger gestures. ?? 2012 Elsevier Ltd.%
  }
  \verb{doi}
  \verb 10.1016/j.jofri.2012.11.006
  \endverb
  \field{isbn}{22124780}
  \field{issn}{22124780}
  \field{number}{1}
  \field{pages}{10\bibrangedash 14}
  \field{title}{{Invisible touch-control of a DICOM viewer with finger gestures
  using the kinect depth camera}}
  \verb{url}
  \verb http://dx.doi.org/10.1016/j.jofri.2012.11.006
  \endverb
  \field{volume}{1}
  \verb{file}
  \verb ::
  \endverb
  \field{journaltitle}{J. Forensic Radiol. Imaging}
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Eisert2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Eisert}{E.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Fechteler}{F.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Rurainsky}{R.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{EP+1}
  \strng{fullhash}{EPFPRJ1}
  \field{labelalpha}{Eis+08}
  \field{sortinit}{E}
  \field{abstract}{%
  In this paper, augmented reality techniques are used in order
  to$\backslash$ncreate a Virtual Mirror for the real-time visualization of
  customized$\backslash$nsports shoes. Similar to looking into a mirror when
  trying on new$\backslash$nshoes in a shop, we create the same impression but
  for virtual shoes$\backslash$nthat the customer can design individually. For
  that purpose, we replace$\backslash$nthe real mirror by a large display that
  shows the mirrored input$\backslash$nof a camera capturing the legs and shoes
  of a person. 3-D Tracking$\backslash$nof both feet and exchanging the real
  shoes by computer graphics models$\backslash$ngives the impression of
  actually wearing the virtual shoes. The 3-D$\backslash$nmotion tracker
  presented in this paper, exploits mainly silhouette$\backslash$ninformation
  to achieve robust estimates for both shoes from a single$\backslash$ncamera
  view. The use of a hierarchical approach in an image
  pyramid$\backslash$nenables real-time estimation at frame rates of more than
  30 frames$\backslash$nper second.%
  }
  \verb{doi}
  \verb 10.1109/CVPR.2008.4587566
  \endverb
  \field{isbn}{978-1-4244-2242-5}
  \field{issn}{1063-6919}
  \field{pages}{1\bibrangedash 6}
  \field{title}{{3-D Tracking of shoes for Virtual Mirror applications}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587566
  \endverb
  \field{journaltitle}{2008 IEEE Conf. Comput. Vis. Pattern Recognit.}
  \field{year}{2008}
\endentry

\entry{Ehara2006}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Ehara}{E.}%
     {J}{J}%
     {}{}%
     {}{}}%
    {{}%
     {Saito}{S.}%
     {H}{H}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Deformable surface,Learning,PCA,Silhouette,Texture
  mapping,Virtual clothing}
  \strng{namehash}{EJSH1}
  \strng{fullhash}{EJSH1}
  \field{labelalpha}{ES07}
  \field{sortinit}{E}
  \field{abstract}{%
  In this paper, we propose a method for overlaying an arbitrary texture image
  onto a surface of a plain T-shirt worn by a user. For overlaying arbitrary
  textures onto the surface of the T-shirt, we need to know the deformation of
  the surface. For estimating the deformation of the surface from the input
  images, we use a two-phase process: learning and searching. In the learning
  phase, the system learns the relationship between the deformation of the
  surface and the silhouette of the T-shirt region in the image. A database of
  a number of training images in which a person wearing a T-shirt with markers
  moves through a variety of positions is used for this learning. Using the
  database, the system can learn the relationship between the shape of the
  silhouette and the surface deformation that is provided by the 2D positions
  of the markers on the surface of the T-shirt. In the searching phase, the
  silhouette of the user's T-shirt is extracted from the input image, and then,
  a search for a similar silhouette in the database is conducted in the
  subspace of the silhouette, which is computed using a PCA of the database. By
  using the proposed method for estimating the deformation of the surface of
  the T-shirt, we perform experiments for overlaying virtual clothing. ©2006
  IEEE.%
  }
  \verb{doi}
  \verb 10.1109/ISMAR.2006.297805
  \endverb
  \field{isbn}{1424406501}
  \field{pages}{139\bibrangedash 142}
  \field{title}{{Texture overlay for virtual clothing based on PCA of
  silhouettes}}
  \verb{url}
  \verb http://www.scopus.com/inward/record.url?eid=2-s2.0-45149115472{\&}partn
  \verb erID=40{\&}md5=af201d6c64cee704222d60e483e37008
  \endverb
  \field{journaltitle}{Proc. - ISMAR 2006 Fifth IEEE ACM Int. Symp. Mix.
  Augment. Real.}
  \field{year}{2007}
\endentry

\entry{Fathi2011}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Fathi}{F.}%
     {Alireza}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Farhadi}{F.}%
     {Ali}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Rehg}{R.}%
     {James~M.}{J.~M.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {IEEE}%
  }
  \strng{namehash}{FA+1}
  \strng{fullhash}{FAFARJM1}
  \field{labelalpha}{Fat+11}
  \field{sortinit}{F}
  \field{booktitle}{2011 Int. Conf. Comput. Vis.}
  \verb{doi}
  \verb 10.1109/ICCV.2011.6126269
  \endverb
  \field{isbn}{978-1-4577-1102-2}
  \field{pages}{407\bibrangedash 414}
  \field{title}{{Understanding egocentric activities}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=2355573.2356302
  \endverb
  \field{year}{2011}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Fiala2007}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Fiala}{F.}%
     {Mark}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{artag,augmented reality,magic mirror}
  \strng{namehash}{FM1}
  \strng{fullhash}{FM1}
  \field{labelalpha}{Fia07}
  \field{sortinit}{F}
  \field{abstract}{%
  A magic mirror paradigm is an augmented reality (AR) system where a camera
  and display device act as a mirror where one can see a reflection of oneself
  and virtual objects together. Fiducial markers mounted on a number of hand
  held and wearable objects allow them to be recognized by computer vision,
  different virtual objects can be rendered relative to the objects depending
  on the chosen theme. The experience can be enjoyed by many onlookers without
  special equipment, unlike other AR experiences such as with HMD's or tablet
  PC's. A series of theoretical and practical problems were overcome to produce
  a working system suitable for educational and entertainment for the public%
  }
  \verb{doi}
  \verb 10.1109/VR.2007.352493
  \endverb
  \field{isbn}{1-4244-0905-5}
  \field{pages}{251\bibrangedash 254}
  \field{title}{{Magic Mirror System with Hand-held and Wearable
  Augmentations}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4161035
  \endverb
  \field{journaltitle}{2007 IEEE Virtual Real. Conf.}
  \field{year}{2007}
\endentry

\entry{Forsberg1996}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Forsberg}{F.}%
     {Andrew}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Herndon}{H.}%
     {Kenneth}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Zeleznik}{Z.}%
     {Robert}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FA+2}
  \strng{fullhash}{FAHKZR1}
  \field{labelalpha}{For+96}
  \field{sortinit}{F}
  \field{abstract}{%
  We present two novel techniques for effectively selecting objects in
  immersive virtual environments using a single 6 DOF magnetic tracker. These
  techniques advance the state of the art in that they exploit the
  participant’s visual frame of reference and fully utilize the position and
  orientation data from the tracker to improve accuracy of the selection task.
  Preliminary results from pilot usability studies validate our designs.
  Finally, the two techniques combine to compensate for each other’s
  weaknesses.%
  }
  \verb{doi}
  \verb 10.1145/237091.237105
  \endverb
  \field{isbn}{0897917987}
  \field{pages}{95\bibrangedash 96}
  \field{title}{{Aperture based selection for immersive virtual environments}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=237105
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Forsberg, Herndon, Zeleznik/Proceedings of the 9
  \verb th annual ACM symposium on user interface software and technology/Forsb
  \verb erg, Herndon, Zeleznik - 1996 - Aperture based selection for immersive
  \verb virtual environments.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 9th Annu. ACM Symp. user interface Softw.
  Technol.}
  \field{year}{1996}
\endentry

\entry{Grosjean1999}{misc}{}
  \name{author}{2}{}{%
    {{}%
     {Grosjean}{G.}%
     {Jerome}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Coquillart}{C.}%
     {Sabine}{S.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{GJCS1}
  \strng{fullhash}{GJCS1}
  \field{labelalpha}{GC99}
  \field{sortinit}{G}
  \field{booktitle}{Proc. Spring Conf. Comput. Graph. (Budmerice}
  \field{pages}{125\bibrangedash 129}
  \field{title}{{The magic mirror: A metaphor for assisting the exploration of
  virtual worlds}}
  \verb{url}
  \verb http://libra.msra.cn/Publication/12702006/the-magic-mirror-a-metaphor-f
  \verb or-assisting-the-exploration-of-virtual-worlds
  \endverb
  \field{year}{1999}
  \field{urlday}{30}
  \field{urlmonth}{12}
  \field{urlyear}{2015}
\endentry

\entry{Garcia-Rodriguez2011}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Garc{\'{\i}}a-Rodr{\'{\i}}guez}{G.-R.}%
     {Jos{\'{e}}}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Garc{\'{\i}}a-Chamizo}{G.-C.}%
     {Juan~Manuel}{J.~M.}%
     {}{}%
     {}{}}%
  }
  \keyw{Growing Neural Gas,Human–computer interaction,Self-growing
  models,Surveillance systems,Topology preservation}
  \strng{namehash}{GRJGCJM1}
  \strng{fullhash}{GRJGCJM1}
  \field{labelalpha}{GRGC11}
  \field{sortinit}{G}
  \field{abstract}{%
  The aim of the work is to build self-growing based architectures to support
  visual surveillance and human–computer interaction systems. The objectives
  include: identifying and tracking persons or objects in the scene or the
  interpretation of user gestures for interaction with services, devices and
  systems implemented in the digital home. The system must address multiple
  vision tasks of various levels such as segmentation, representation or
  characterization, analysis and monitoring of the movement to allow the
  construction of a robust representation of their environment and interpret
  the elements of the scene. It is also necessary to integrate the vision
  module into a global system that operates in a complex environment by
  receiving images from acquisition devices at video frequency and offering
  results to higher level systems, monitors and take decisions in real time,
  and must accomplish a set of requirements such as: time constraints, high
  availability, robustness, high processing speed and re-configurability. Based
  on our previous work with neural models to represent objects, in particular
  the Growing Neural Gas (GNG) model and the study of the topology preservation
  as a function of the parameters election, it is proposed to extend the
  capabilities of this self-growing model to track objects and represent their
  motion in image sequences under temporal restrictions. These neural models
  have various interesting features such as: their ability to readjust to new
  input patterns without restarting the learning process, adaptability to
  represent deformable objects and even objects that are divided in different
  parts or the intrinsic resolution of the problem of matching features for the
  sequence analysis and monitoring of the movement. It is proposed to build an
  architecture based on the GNG that has been called GNG-Seq to represent and
  analyze the motion in image sequences. Several experiments are presented that
  demonstrate the validity of the architecture to solve problems of target
  tracking, motion analysis or human–computer interaction.%
  }
  \verb{doi}
  \verb 10.1016/j.asoc.2011.02.007
  \endverb
  \field{issn}{15684946}
  \field{number}{7}
  \field{pages}{4413\bibrangedash 4431}
  \field{title}{{Surveillance and human–computer interaction applications of
  self-growing models}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S1568494611000676
  \endverb
  \field{volume}{11}
  \field{journaltitle}{Appl. Soft Comput.}
  \field{year}{2011}
\endentry

\entry{Gratzel2004a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Gr{\"{a}}tzel}{G.}%
     {C}{C}%
     {}{}%
     {}{}}%
    {{}%
     {Fong}{F.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Grange}{G.}%
     {S}{S}%
     {}{}%
     {}{}}%
    {{}%
     {Baur}{B.}%
     {C}{C}%
     {}{}%
     {}{}}%
  }
  \keyw{Computer Peripherals,Finite Element Analysis,Gestures,Humans,Surgery,
  Computer-Assisted,Surgery, Computer-Assisted: instrumentation,User-Computer
  Interface}
  \strng{namehash}{GC+1}
  \strng{fullhash}{GCFTGSBC1}
  \field{labelalpha}{Gr{\"{a}}+04}
  \field{sortinit}{G}
  \field{abstract}{%
  We have developed a system that uses computer vision to replace standard
  computer mouse functions with hand gestures. The system is designed to enable
  non-contact human-computer interaction (HCI), so that surgeons will be able
  to make more effective use of computers during surgery. In this paper, we
  begin by discussing the need for non-contact computer interfaces in the
  operating room. We then describe the design of our non-contact mouse system,
  focusing on the techniques used for hand detection, tracking, and gesture
  recognition. Finally, we present preliminary results from testing and planned
  future work.%
  }
  \verb{doi}
  \verb 10.1.1.5.6631
  \endverb
  \field{issn}{0928-7329}
  \field{number}{3}
  \field{pages}{245\bibrangedash 57}
  \field{title}{{A non-contact mouse for surgeon-computer interaction.}}
  \verb{url}
  \verb http://www.ncbi.nlm.nih.gov/pubmed/15328453
  \endverb
  \field{volume}{12}
  \verb{file}
  \verb :E$\backslash$:/papers/Gr{\"{a}}tzel et al/Technology and health care o
  \verb fficial journal of the European Society for Engineering and Medicine/Gr
  \verb {\"{a}}tzel et al. - 2004 - A non-contact mouse for surgeon-computer in
  \verb teraction.pdf:pdf
  \endverb
  \field{journaltitle}{Technol. Health Care}
  \field{year}{2004}
\endentry

\entry{Ha2014}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Ha}{H.}%
     {Taejin}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Feiner}{F.}%
     {Steven}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Woo}{W.}%
     {Woontack}{W.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {IEEE}%
  }
  \keyw{3D User Interfaces,AR design,AR environment,AR games,AR information
  browsing,AR maintenance,Augmented Reality,Cameras,Equations,Hand
  Interaction,Image color analysis,Image resolution,RGB-D camera,Rendering
  (computer graphics),Three-dimensional displays,Virtual 3D Object
  Manipulation,Visualization,WeARHand,Wearable Computing,augmented
  reality,bare-hand user interface,cameras,depth perception,egocentric visual
  feedback,head-worn display,helmet mounted displays,red-green-blue-depth
  camera,semitransparent proxy hand,tethered tracking devices,user
  interfaces,wearable augmented reality}
  \strng{namehash}{HT+1}
  \strng{fullhash}{HTFSWW1}
  \field{labelalpha}{Ha+14}
  \field{sortinit}{H}
  \field{abstract}{%
  We introduce WeARHand, which allows a user to manipulate virtual 3D objects
  with a bare hand in a wearable augmented reality (AR) environment. Our method
  uses no environmentally tethered tracking devices and localizes a pair of
  near-range and far-range RGB-D cameras mounted on a head-worn display and a
  moving bare hand in 3D space by exploiting depth input data. Depth perception
  is enhanced through egocentric visual feedback, including a semi-transparent
  proxy hand. We implement a virtual hand interaction technique and feedback
  approaches, and evaluate their performance and usability. The proposed method
  can apply to many 3D interaction scenarios using hands in a wearable AR
  environment, such as AR information browsing, maintenance, design, and
  games.%
  }
  \field{booktitle}{2014 IEEE Int. Symp. Mix. Augment. Real.}
  \verb{doi}
  \verb 10.1109/ISMAR.2014.6948431
  \endverb
  \field{isbn}{978-1-4799-6184-9}
  \field{number}{Figure 1}
  \field{pages}{219\bibrangedash 228}
  \field{title}{{WeARHand: Head-worn, RGB-D camera-based, bare-hand user
  interface with visually enhanced depth perception}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6948431
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Ha, Feiner, Woo/2014 IEEE International Symposiu
  \verb m on Mixed and Augmented Reality (ISMAR)/Ha, Feiner, Woo - 2014 - WeARH
  \verb and Head-worn, RGB-D camera-based, bare-hand user interface with visual
  \verb ly enhanced depth perception.pdf:pdf
  \endverb
  \field{year}{2014}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Jonas2012}{misc}{}
  \name{author}{5}{}{%
    {{}%
     {Hannig}{H.}%
     {Andreas}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Kuth}{K.}%
     {Nicole}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {{\"{O}}zman}{{\"{O}}.}%
     {Monika}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Jonas}{J.}%
     {Stephan}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Spreckelsen}{S.}%
     {Cord}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{computer-assisted
  instruction,education,experimental,games,medical,methods,teaching,undergraduate,user-computer
  interface}
  \strng{namehash}{HA+1}
  \strng{fullhash}{HAKNOMJSSC1}
  \field{labelalpha}{Han+12}
  \field{sortinit}{H}
  \field{abstract}{%
  BACKGROUND: Preparing medical students for the takeover or the start-up of a
  medical practice is an important challenge in Germany today. Therefore, this
  paper presents a computer-aided serious game (eMedOffice) developed and
  currently in use at the RWTH Aachen University Medical School. The game is
  part of the attempt to teach medical students the organizational and
  conceptual basics of the medical practice of a general practitioner in a
  problem-based learning environment. This paper introduces methods and
  concepts used to develop the serious game and describes the results of an
  evaluation of the game's application in curricular courses at the Medical
  School.$\backslash$n$\backslash$nRESULTS: Results of the conducted evaluation
  gave evidence of a positive learning effect of the serious game. Educational
  supervisors observed strong collaboration among the players inspired by the
  competitive gaming aspects. In addition, an increase in willingness to learn
  and the exploration of new self-invented ideas were observed and valuable
  proposals for further prospective enhancements were elicited. A statistical
  analysis of the results of an evaluation provided a clear indication of the
  positive learning effect of the game. A usability questionnaire survey
  revealed a very good overall score of 4.07 (5=best,
  1=worst).$\backslash$n$\backslash$nCONCLUSIONS: We consider web-based,
  collaborative serious games to be a promising means of improving medical
  education. The insights gained by the implementation of eMedOffice will
  promote the future development of more effective serious games for
  integration into curricular courses of the RWTH Aachen University Medical
  School.%
  }
  \field{booktitle}{BMC Med. Educ.}
  \verb{doi}
  \verb 10.1186/1472-6920-12-104
  \endverb
  \field{isbn}{1472692012}
  \field{issn}{1472-6920}
  \field{number}{1}
  \field{pages}{104}
  \field{title}{{eMedOffice: A web-based collaborative serious game for
  teaching optimal design of a medical practice}}
  \field{volume}{12}
  \verb{file}
  \verb :E$\backslash$:/papers/Hannig et al/BMC Medical Education/Hannig et al.
  \verb  - 2012 - eMedOffice A web-based collaborative serious game for teachin
  \verb g optimal design of a medical practice.pdf:pdf
  \endverb
  \field{year}{2012}
\endentry

\entry{Harrison2011}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Harrison}{H.}%
     {Chris}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Benko}{B.}%
     {Hrvoje}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Wilson}{W.}%
     {Andrew~D.}{A.~D.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \keyw{appropriated surfaces,finger tracking,object classification,on-body
  computing,on-demand interfaces}
  \strng{namehash}{HC+1}
  \strng{fullhash}{HCBHWAD1}
  \field{labelalpha}{Har+11}
  \field{sortinit}{H}
  \field{abstract}{%
  Abstract OmniTouch is a wearable depth-sensing and projection system that
  enables interactive multitouch applications on everyday surfaces. Beyond the
  shoulder-worn system, there is no instrumentation of the user or environment.
  Foremost, the system allows the ... $\backslash$n%
  }
  \field{booktitle}{Proc. 24th Annu. ACM Symp. User interface Softw. Technol. -
  UIST '11}
  \verb{doi}
  \verb 10.1145/2047196.2047255
  \endverb
  \field{isbn}{9781450307161}
  \field{issn}{10974199}
  \field{pages}{441}
  \field{title}{{OmniTouch}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2047196.2047255
  \endverb
  \list{location}{1}{%
    {New York, New York, USA}%
  }
  \field{year}{2011}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Hart2006}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Hart}{H.}%
     {S.~G.}{S.~G.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {en}%
  }
  \list{publisher}{1}{%
    {SAGE Publications}%
  }
  \keyw{GS4 - Integrative Approaches to Understanding Comp}
  \strng{namehash}{HSG1}
  \strng{fullhash}{HSG1}
  \field{labelalpha}{Har06}
  \field{sortinit}{H}
  \field{abstract}{%
  NASA-TLX is a multi-dimensional scale designed to obtain workload estimates
  from one or more operators while they are performing a task or immediately
  afterwards. The years of research that preceded subscale selection and the
  weighted averaging approach resulted in a tool that has proven to be
  reasonably easy to use and reliably sensitive to experimentally important
  manipulations over the past 20 years. Its use has spread far beyond its
  original application (aviation), focus (crew complement), and language
  (English). This survey of 550 studies in which NASA-TLX was used or reviewed
  was undertaken to provide a resource for a new generation of users. The goal
  was to summarize the environments in which it has been applied, the types of
  activities the raters performed, other variables that were measured that did
  (or did not) covary, methodological issues, and lessons learned%
  }
  \verb{doi}
  \verb 10.1177/154193120605000909
  \endverb
  \field{isbn}{1071-1813}
  \field{issn}{1071-1813}
  \field{number}{9}
  \field{pages}{904\bibrangedash 908}
  \field{title}{{Nasa-Task Load Index (NASA-TLX); 20 Years Later}}
  \verb{url}
  \verb http://pro.sagepub.com/content/50/9/904.abstract
  \endverb
  \field{volume}{50}
  \field{journaltitle}{Proc. Hum. Factors Ergon. Soc. Annu. Meet.}
  \field{year}{2006}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Hilsmann2008}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Hilsmann}{H.}%
     {A.}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Eisert}{E.}%
     {P.}{P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Deformable Meshes,Garment Tracking,Optical Flow,Virtual Clothing}
  \strng{namehash}{HAEP1}
  \strng{fullhash}{HAEP1}
  \field{labelalpha}{HE08}
  \field{sortinit}{H}
  \field{abstract}{%
  In this paper, we present a method for tracking and retexturing of garments
  that exploits the entire image information using the optical flow constraint
  instead of working with distinct features. In a hierarchical framework we
  refine the motion model with every level. The motion model is used to
  regularize the optical flow field such that finding the best transformation
  amounts in minimizing an error function that can be solved in a least squares
  sense. Knowledge about the position and deformation of the garment in 2D
  allows us to erase the old texture and replace it by a new one with correct
  deformation and shading properties without 3D reconstruction. Additionally,
  it provides an estimation of the irradiance such that the new texture can be
  illuminated realistically.%
  }
  \verb{doi}
  \verb 10.1109/ICIP.2008.4711887
  \endverb
  \field{isbn}{978-1-4244-1765-0}
  \field{issn}{1522-4880}
  \field{title}{{Optical flow based tracking and retexturing of garments}}
  \field{journaltitle}{2008 15th IEEE Int. Conf. Image Process.}
  \field{year}{2008}
\endentry

\entry{Huang2014}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Huang}{H.}%
     {Chi-Chiang}{C.-C.}%
     {}{}%
     {}{}}%
    {{}%
     {Liang}{L.}%
     {Rong-Hao}{R.-H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chan}{C.}%
     {Liwei}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Chen}{C.}%
     {Bing-Yu}{B.-Y.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \strng{namehash}{HCC+1}
  \strng{fullhash}{HCCLRHCLCBY1}
  \field{labelalpha}{Hua+14}
  \field{sortinit}{H}
  \field{booktitle}{ACM SIGGRAPH 2014 Emerg. Technol. - SIGGRAPH '14}
  \verb{doi}
  \verb 10.1145/2614066.2614097
  \endverb
  \field{isbn}{9781450329613}
  \field{pages}{1\bibrangedash 1}
  \field{title}{{Dart-it}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2614066.2614097
  \endverb
  \list{location}{1}{%
    {New York, New York, USA}%
  }
  \field{year}{2014}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Ha2010}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Ha}{H.}%
     {Taejin}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Woo}{W.}%
     {Woontack}{W.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{HTWW1}
  \strng{fullhash}{HTWW1}
  \field{labelalpha}{HW10}
  \field{sortinit}{H}
  \field{abstract}{%
  In this paper, we present a Fitts’ law-based formal evaluation process and
  the corresponding results for 3D object manipulation techniques based on a
  virtual hand metaphor in a tangible augmented reality (TAR) environment.
  Specifically, we extend the design parameters of the 1D scale Fitts’ law to
  3D scale and then refine an evaluation model in order to bring generality and
  ease of adaptation to various TAR applications. Next, we implement and
  compare standard TAR manipulation techniques using a cup, a paddle, a cube,
  and a proposed extended paddle prop. Most manipulation techniques were
  well-modeled in terms of linear regression according to Fitts’ law, with a
  correlation coefficient value of over 0.9. Notably, the throughput by ISO
  9241-9 of the extended paddle technique peaked at around 1.39 to 2 times
  higher than in the other techniques, due to the instant 3D positioning of the
  3D objects. In the discussion, we subsequently examine the characteristics of
  the TAR manipulation techniques in terms of stability, speed, comfort, and
  understanding. As a result, our evaluation process, results, and analysis can
  be useful in guiding the design and implementation of future TAR interfaces.%
  }
  \field{booktitle}{2010 IEEE Symp. 3D User Interfaces}
  \verb{doi}
  \verb 10.1109/3DUI.2010.5444713
  \endverb
  \field{isbn}{978-1-4244-6846-1}
  \field{pages}{91\bibrangedash 98}
  \field{title}{{An empirical evaluation of virtual hand techniques for 3D
  object manipulation in a tangible augmented reality environment}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5444713
  \endverb
  \field{year}{2010}
\endentry

\entry{Ionescu2006}{misc}{}
  \name{author}{1}{}{%
    {{}%
     {Ionescu}{I.}%
     {Arna}{A.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{IA1}
  \strng{fullhash}{IA1}
  \field{labelalpha}{Ion06}
  \field{sortinit}{I}
  \field{booktitle}{Ambidextrous Mag.}
  \field{number}{4}
  \field{pages}{30\bibrangedash 32}
  \field{title}{{A Mouse in the O.R}}
  \verb{url}
  \verb https://www.ideo.com/images/uploads/news/pdfs/ambidextrous{\_}summer-20
  \verb 06.pdf https://www.google.de/search?q=a+mouse+in+the+O.R.+ambidextrous{
  \verb \&}rlz=1C1CHFX{\_}enUS596US596{\&}oq=a+mouse+in+the+O.R.+ambidextrous{\
  \verb &}aqs=chrome..69i57j69i64l2{\&}sourceid=chrome{\&}es{\_}sm=122{\&}ie=
  \endverb
  \field{year}{2006}
  \field{urlday}{03}
  \field{urlmonth}{11}
  \field{urlyear}{2015}
\endentry

\entry{Jalaliniya2013}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Jalaliniya}{J.}%
     {Shahram}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Smith}{S.}%
     {Jeremiah}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Sousa}{S.}%
     {Miguel}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {B{\"{u}}the}{B.}%
     {Lars}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Pederson}{P.}%
     {Thomas}{T.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \keyw{floor sensor,gesture-based interaction,touch-less interaction in
  hospital,wearable sensor}
  \strng{namehash}{JS+1}
  \strng{fullhash}{JSSJSMBLPT1}
  \field{labelalpha}{Jal+13}
  \field{sortinit}{J}
  \field{abstract}{%
  Sterility restrictions in surgical settings make touch-less interaction an
  interesting solution for surgeons to interact directly with digital images.
  The HCI community has already explored several methods for touch-less
  interaction including those based on camera-based gesture tracking and voice
  control. In this paper, we present a system for gesture-based interaction
  with medical images based on a single wristband sensor and capacitive floor
  sensors, allowing for hand and foot gesture input. The first limited
  evaluation of the system showed an acceptable level of accuracy for 12
  different hand {\&} foot gestures; also users found that our combined hand
  and foot based gestures are intuitive for providing input.%
  }
  \field{booktitle}{Proc. 2013 ACM Conf. Pervasive ubiquitous Comput. Adjun.
  Publ. - UbiComp '13 Adjun.}
  \verb{doi}
  \verb 10.1145/2494091.2497332
  \endverb
  \field{isbn}{9781450322157}
  \field{pages}{1265\bibrangedash 1274}
  \field{title}{{Touch-less interaction with medical images using hand {\&}
  foot gestures}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2494091.2497332
  \endverb
  \list{location}{1}{%
    {New York, New York, USA}%
  }
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Jang2015}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Jang}{J.}%
     {Youngkyoon}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Noh}{N.}%
     {Seung-Tak}{S.-T.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Hyung~Jin}{H.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Kim}{K.}%
     {Tae-Kyun}{T.-K.}%
     {}{}%
     {}{}}%
    {{}%
     {Woo}{W.}%
     {Woontack}{W.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {English}%
  }
  \list{publisher}{1}{%
    {IEEE}%
  }
  \keyw{2D image-based fingertip detection,3D finger CAPE,3D hand posture
  estimation,Estimation,Feature extraction,Hand
  tracking,Joints,Three-dimensional displays,Thumb,Vectors,arm reachable AR/VR
  space,augmented reality,bare hand-based interaction,clicking action
  detection,computer vision,egocentric viewed single-depth image
  sequences,egocentric-depth camera-attached HMD,fingertip position
  estimation,human computer interaction,image sequences,inference
  mechanisms,object detection,occluded fingertip position estimation,pose
  estimation,probabilistic inference,random processes,rotation invariant finger
  clicking action,selection,self-occlusion,self-occlusions,spatio-temporal
  forest,spatiotemporal random forest,translation invariant finger clicking
  action}
  \strng{namehash}{JY+1}
  \strng{fullhash}{JYNSTCHJKTKWW1}
  \field{labelalpha}{Jan+15}
  \field{sortinit}{J}
  \field{abstract}{%
  In this paper we present a novel framework for simultaneous detection of
  click action and estimation of occluded fingertip positions from egocentric
  viewed single-depth image sequences. For the detection and estimation, a
  novel probabilistic inference based on knowledge priors of clicking motion
  and clicked position is presented. Based on the detection and estimation
  results, we were able to achieve a fine resolution level of a bare hand-based
  interaction with virtual objects in egocentric viewpoint. Our contributions
  include: (i) a rotation and translation invariant finger clicking action and
  position estimation using the combination of 2D image-based fingertip
  detection with 3D hand posture estimation in egocentric viewpoint. (ii) a
  novel spatio-temporal random forest, which performs the detection and
  estimation efficiently in a single framework. We also present (iii) a
  selection process utilizing the proposed clicking action detection and
  position estimation in an arm reachable AR/VR space, which does not require
  any additional device. Experimental results show that the proposed method
  delivers promising performance under frequent self-occlusions in the process
  of selecting objects in AR/VR space whilst wearing an egocentric-depth
  camera-attached HMD.%
  }
  \verb{doi}
  \verb 10.1109/TVCG.2015.2391860
  \endverb
  \field{issn}{1077-2626}
  \field{number}{4}
  \field{pages}{501\bibrangedash 510}
  \field{title}{{3D Finger CAPE: Clicking Action and Position Estimation under
  Self-Occlusions in Egocentric Viewpoint}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7014300
  \endverb
  \field{volume}{21}
  \verb{file}
  \verb :E$\backslash$:/papers/Jang et al/IEEE Transactions on Visualization an
  \verb d Computer Graphics/Jang et al. - 2015 - 3D Finger CAPE Clicking Action
  \verb  and Position Estimation under Self-Occlusions in Egocentric Viewpoint.
  \verb pdf:pdf
  \endverb
  \field{journaltitle}{IEEE Trans. Vis. Comput. Graph.}
  \field{year}{2015}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Johnson2011a}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Johnson}{J.}%
     {Rose}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {O'Hara}{O.}%
     {Kenton}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Sellen}{S.}%
     {Abigail}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Cousins}{C.}%
     {Claire}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Criminisi}{C.}%
     {Antonio}{A.}%
     {}{}%
     {}{}}%
  }
  \keyw{ethnography,fieldwork,gesture input,image use,medical
  practice,touchless interaction}
  \strng{namehash}{JR+1}
  \strng{fullhash}{JROKSACCCA1}
  \field{labelalpha}{Joh+11}
  \field{sortinit}{J}
  \field{abstract}{%
  The growth of image-guided procedures in surgical settings has led to an
  increased need to interact with digital images under sterile conditions.
  Traditional touch-based interaction techniques present challenges for
  managing asepsis in these environments leading to suggestions that new
  touchless interaction techniques may provide a compelling set of
  alternatives. In this paper we explore the potential for touchless
  interaction in image-guided Interventional Radiology (IR) through an
  ethnographic study. The findings highlight how the distribution of labour and
  spatial practices of this work are organised with respect to concerns about
  asepsis and radiation exposure, the physical and cognitive demands of
  artefact manipulation, patient management, and the construction of
  "professional vision". We discuss the implications of these key features of
  the work for touchless interaction technologies within IR and suggest that
  such issues will be of central importance in considering new input techniques
  in other medical settings.%
  }
  \field{booktitle}{Proc. 2011 Annu. Conf. Hum. factors Comput. Syst. - CHI
  '11}
  \verb{doi}
  \verb 10.1145/1978942.1979436
  \endverb
  \field{isbn}{9781450302289}
  \field{pages}{3323\bibrangedash 3332}
  \field{title}{{Exploring the potential for touchless interaction in
  image-guided interventional radiology}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1978942.1979436
  \endverb
  \field{year}{2011}
\endentry

\entry{Jalaliniya2015}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Jalaliniya}{J.}%
     {S}{S}%
     {}{}%
     {}{}}%
    {{}%
     {Pederson}{P.}%
     {T}{T}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{JSPT1}
  \strng{fullhash}{JSPT1}
  \field{labelalpha}{JP15}
  \field{sortinit}{J}
  \field{title}{{Designing Wearable Personal Assistants for Surgeons: An
  Egocentric Approach}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7140695
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Yamada et al/Imaging science in dentistry/Yamada
  \verb  et al. - 2014 - Use of a gesture user interface as a touchless image n
  \verb avigation system in dental surgery Case series report(2).pdf:pdf
  \endverb
  \field{journaltitle}{Pervasive Comput. IEEE}
  \field{year}{2015}
\endentry

\entry{Juan2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Juan}{J.}%
     {Carmen}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Beatrice}{B.}%
     {Francesca}{F.}%
     {}{}%
     {}{}}%
    {{}%
     {Cano}{C.}%
     {Juan}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented Reality,human body,tangible interfaces}
  \strng{namehash}{JC+1}
  \strng{fullhash}{JCBFCJ1}
  \field{labelalpha}{Jua+08}
  \field{sortinit}{J}
  \field{abstract}{%
  Augmented Reality has been used for developing systems with learning
  purposes. In this paper, we present an Augmented Reality system for learning
  the interior of the human body. We have tested the system with children of
  the Summer School of the Technical University of Valencia. In this test we
  have analysed if the use of a Head-Mounted Display or a typical monitor
  influence in the experience of the children. Results do not offer statistical
  significant differences using both visualization systems and confirm that
  children enjoyed learning with the system and consider it as useful tool not
  only for learning the interior of the human body but also for learning other
  subjects.%
  }
  \verb{doi}
  \verb 10.1109/ICALT.2008.121
  \endverb
  \field{isbn}{9780769531670}
  \field{pages}{186\bibrangedash 188}
  \field{title}{{An Augmented Reality system for learning the interior of the
  human body}}
  \field{journaltitle}{Proc. - 8th IEEE Int. Conf. Adv. Learn. Technol. ICALT
  2008}
  \field{year}{2008}
\endentry

\entry{Kassner2014}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Kassner}{K.}%
     {Moritz}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Patera}{P.}%
     {William}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Bulling}{B.}%
     {Andreas}{A.}%
     {}{}%
     {}{}}%
  }
  \keyw{Eye Movement,Gaze-based Interaction,Mobile Eye Tracking,Wearable
  Computing}
  \strng{namehash}{KM+1}
  \strng{fullhash}{KMPWBA1}
  \field{labelalpha}{Kas+14}
  \field{sortinit}{K}
  \field{abstract}{%
  Commercial head-mounted eye trackers provide useful fea- tures to customers
  in industry and research but are expensive and rely on closed source hardware
  and software. This lim- its the application areas and use of mobile eye
  tracking to expert users and inhibits user-driven development, customisa-
  tion, and extension. In this paper we present Pupil – an acces- sible,
  affordable, and extensible open source platform for mo- bile eye tracking and
  gaze-based interaction. Pupil comprises 1) a light-weight headset with
  high-resolution cameras, 2) an open source software framework for mobile eye
  tracking, as well as 3) a graphical user interface (GUI) to playback and
  visualize video and gaze data. Pupil features high-resolution scene and eye
  cameras for monocular and binocular gaze esti- mation. The software and GUI
  are platform-independent and include state-of-the-art algorithms for
  real-time pupil detec- tion and tracking, calibration, and accurate gaze
  estimation. Results of a performance evaluation show that Pupil can pro- vide
  an average gaze estimation accuracy of 0.6 degree of visual angle (0.08
  degree precision) with a latency of the pro- cessing pipeline of only 0.045
  seconds.%
  }
  \verb{doi}
  \verb 10.1145/2638728.2641695
  \endverb
  \verb{eprint}
  \verb arXiv:1405.0006v1
  \endverb
  \field{isbn}{9781450330473}
  \field{pages}{1151\bibrangedash 1160}
  \field{title}{{Pupil}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2638728.2641695
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Kassner, Patera, Bulling/Proceedings of the 2014
  \verb  ACM International Joint Conference on Pervasive and Ubiquitous Computi
  \verb ng Adjunct Publication - UbiComp '14 Adjunct/Kassner, Patera, Bulling -
  \verb  2014 - Pupil.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous
  Comput. Adjun. Publ. - UbiComp '14 Adjun.}
  \field{eprinttype}{arXiv}
  \field{year}{2014}
\endentry

\entry{Klontz2013}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Klontz}{K.}%
     {Joshua~C.}{J.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Klare}{K.}%
     {Brendan~F.}{B.~F.}%
     {}{}%
     {}{}}%
    {{}%
     {Klum}{K.}%
     {Scott}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Jain}{J.}%
     {Anil~K.}{A.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Burge}{B.}%
     {Mark~J.}{M.~J.}%
     {}{}%
     {}{}}%
  }
  \keyw{Algorithm design and analysis,Face,Face recognition,Libraries,OpenBR
  software architecture,Software,Software algorithms,Vectors,collaborative
  software development,community-driven open source software,face
  recognition,groupware,open source biometric recognition,public domain
  software,software architecture,still-image frontal face recognition}
  \strng{namehash}{KJC+1}
  \strng{fullhash}{KJCKBFKSJAKBMJ1}
  \field{labelalpha}{Klo+13}
  \field{sortinit}{K}
  \field{abstract}{%
  The biometrics community enjoys an active research field that has produced
  algorithms for several modalities suitable for real-world applications.
  Despite these developments, there exist few open source implementations of
  complete algorithms that are maintained by the community or deployed outside
  a laboratory environment. In this paper we motivate the need for more
  community-driven open source software in the field of biometrics and present
  OpenBR as a candidate to address this deficiency. We overview the OpenBR
  software architecture and consider still-image frontal face recognition as a
  case study to illustrate its strengths and capabilities. All of our work is
  available at www.openbiometrics.org.%
  }
  \field{booktitle}{2013 IEEE Sixth Int. Conf. Biometrics Theory, Appl. Syst.}
  \verb{doi}
  \verb 10.1109/BTAS.2013.6712754
  \endverb
  \field{isbn}{978-1-4799-0527-0}
  \field{pages}{1\bibrangedash 8}
  \field{title}{{Open source biometric recognition}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6712754$\backsl
  \verb ash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6712
  \verb 754
  \endverb
  \field{year}{2013}
\endentry

\entry{Kurata2000}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Kurata}{K.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Okuma}{O.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Kourogi}{K.}%
     {M}{M}%
     {}{}%
     {}{}}%
    {{}%
     {Sakaue}{S.}%
     {K}{K}%
     {}{}%
     {}{}}%
  }
  \keyw{1 wearable augmented environments,human interface,real-time
  system,vironments,visual wearables,wearable augmented en-}
  \strng{namehash}{KT+1}
  \strng{fullhash}{KTOTKMSK1}
  \field{labelalpha}{Kur+00}
  \field{sortinit}{K}
  \field{abstract}{%
  This paper describes a wearable input interface the functions of which is
  similar to the usual mouse. The Hand-mouse is realized by classifying each
  pixel as a hand pixel or a background pixel based on approximation of a color
  histogram with the Gaussian mixture model and by tting the simple model of
  hand shapes into the classi ed hand pixels.%
  }
  \field{booktitle}{Symp. Mix. Real.}
  \field{pages}{188\bibrangedash 189}
  \field{title}{{The Hand-mouse: A Human Interface Suitable for Augmented
  Reality Environments Enabled by Visual Wearables}}
  \verb{url}
  \verb http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1{\&}type=pdf{\&}d
  \verb oi=10.1.1.141.5128
  \endverb
  \field{year}{2000}
\endentry

\entry{Lewis2011}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Lewis}{L.}%
     {T~M}{T.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Aggarwal}{A.}%
     {R}{R}%
     {}{}%
     {}{}}%
    {{}%
     {Rajaretnam}{R.}%
     {N}{N}%
     {}{}%
     {}{}}%
    {{}%
     {Grantcharov}{G.}%
     {T~P}{T.~P.}%
     {}{}%
     {}{}}%
    {{}%
     {Darzi}{D.}%
     {A}{A}%
     {}{}%
     {}{}}%
  }
  \keyw{Clinical Competence,Computer Simulation,Computer-Assisted Instruction/
  instrumentation,Fellowships and Scholarships/ standards,General Surgery/
  education,Humans,Medical Oncology/ education,Neoplasms/ surgery,Operating
  Rooms,Task Performance and Analysis}
  \strng{namehash}{LTM+1}
  \strng{fullhash}{LTMARRNGTPDA1}
  \field{labelalpha}{Lew+11}
  \field{sortinit}{L}
  \field{abstract}{%
  There have been dramatic changes in surgical training over the past two
  decades which have resulted in a number of concerns for the development of
  future surgeons. Changes in the structure of cancer services, working hour
  restrictions and a commitment to patient safety has led to a reduction in
  training opportunities that are available to the surgeon in training.
  Simulation and in particular virtual reality (VR) simulation has been
  heralded as an effective adjunct to surgical training. Advances in VR
  simulation has allowed trainees to practice realistic full length procedures
  in a safe and controlled environment, where mistakes are permitted and can be
  used as learning points. There is considerable evidence to demonstrate that
  the VR simulation can be used to enhance technical skills and improve
  operating room performance. Future work should focus on the cost
  effectiveness and predictive validity of VR simulation, which in turn would
  increase the uptake of simulation and enhance surgical training.%
  }
  \verb{doi}
  \verb 10.1016/j.suronc.2011.04.005
  \endverb
  \field{isbn}{1879-3320 (Electronic)$\backslash$r0960-7404 (Linking)}
  \field{issn}{09607404}
  \field{number}{3}
  \field{pages}{134\bibrangedash 139}
  \field{title}{{Training in surgical oncology - the role of VR simulation}}
  \verb{url}
  \verb http://www.sciencedirect.com/science/article/pii/S0960740411000363$\bac
  \verb kslash$nhttp://ac.els-cdn.com/S0960740411000363/1-s2.0-S096074041100036
  \verb 3-main.pdf?{\_}tid=a3e79a64-1cf4-11e5-adff-00000aacb360{\&}acdnat=14354
  \verb 27455{\_}e7c05d8a5d9bd990da57794c3461e2df
  \endverb
  \field{volume}{20}
  \list{institution}{2}{%
    {Department of Cancer}%
    {Surgery, Room 1029, 10th Floor, QEQM, St. Marys hospital Imperial College
  London, UK.}%
  }
  \field{journaltitle}{Surg Oncol}
  \field{year}{2011}
\endentry

\entry{Liang1994}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Liang}{L.}%
     {Jiandong}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Green}{G.}%
     {Mark}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{LJGM1}
  \strng{fullhash}{LJGM1}
  \field{labelalpha}{LG94}
  \field{sortinit}{L}
  \field{abstract}{%
  JDCAD: a highly interactive 3 D modeling system. JIANDONG , M
  GREEN$\backslash$nComputers {\{}{\&}{\}} graphics 18:44, 499-506, Elsevier
  Science, 1994.%
  }
  \verb{doi}
  \verb 10.1016/0097-8493(94)90062-0
  \endverb
  \field{issn}{00978493}
  \field{number}{4}
  \field{pages}{499\bibrangedash 506}
  \field{title}{{JDCAD: A highly interactive 3D modeling system}}
  \verb{url}
  \verb http://linkinghub.elsevier.com/retrieve/pii/0097849394900620
  \endverb
  \field{volume}{18}
  \field{journaltitle}{Comput. Graph.}
  \field{year}{1994}
\endentry

\entry{Li2015}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Li}{L.}%
     {Yin}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Ye}{Y.}%
     {Zhefan}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Rehg}{R.}%
     {James~M.}{J.~M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{LY+1}
  \strng{fullhash}{LYYZRJM1}
  \field{labelalpha}{Li+15}
  \field{sortinit}{L}
  \field{booktitle}{Proc. IEEE Conf. Comput. Vis. Pattern Recognit.}
  \field{pages}{287\bibrangedash 295}
  \field{title}{{Delving Into Egocentric Actions}}
  \verb{url}
  \verb http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Li
  \verb {\_}Delving{\_}Into{\_}Egocentric{\_}2015{\_}CVPR{\_}paper.html
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Li, Ye, Rehg/Proceedings of the IEEE Conference
  \verb on Computer Vision and Pattern Recognition/Li, Ye, Rehg - 2015 - Delvin
  \verb g Into Egocentric Actions.pdf:pdf
  \endverb
  \field{year}{2015}
\endentry

\entry{Luh2013}{misc}{}
  \name{author}{5}{}{%
    {{}%
     {Luh}{L.}%
     {Yuan-Ping}{Y.-P.}%
     {}{}%
     {}{}}%
    {{}%
     {Wang}{W.}%
     {Jeng-Bang}{J.-B.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Jin-Wan}{J.-W.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Shun-Ya}{S.-Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Chu}{C.}%
     {Chih-Hsing}{C.-H.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Mass customization,Product modularization,Shoe
  design}
  \strng{namehash}{LYP+1}
  \strng{fullhash}{LYPWJBCJWCSYCCH1}
  \field{labelalpha}{Luh+13}
  \field{sortinit}{L}
  \field{abstract}{%
  This paper presents a systematic framework for design customization of
  footwear for children and identifies three modules related to shoe styling:
  shoe surface, shoe bottom, and accessory. A new module, shoe cloth, is
  created to allow a quick change of shoe appearance. Consumers can specify
  various design attributes in each module, including color, texture,
  embroidery, and shape. A prototype design system is implemented using
  augmented reality and sensing technologies based on the framework. This
  system consists of novel functions that support customization, design
  evaluation, and pattern development. The user can virtually put a customized
  shoe model on his/her foot in a video stream. The proposed framework not only
  facilitates evaluating products that are highly interactive with users, but
  also helps engage them in the design process. This work realizes the concept
  of human-centric design for mass customization. © 2012 Springer
  Science+Business Media, LLC.%
  }
  \field{booktitle}{J. Intell. Manuf.}
  \verb{doi}
  \verb 10.1007/s10845-012-0642-9
  \endverb
  \field{isbn}{1084501206429}
  \field{issn}{0956-5515}
  \field{number}{5}
  \field{pages}{905\bibrangedash 917}
  \field{title}{{Augmented reality-based design customization of footwear for
  children}}
  \verb{url}
  \verb http://link.springer.com/10.1007/s10845-012-0642-9
  \endverb
  \field{volume}{24}
  \field{year}{2013}
\endentry

\entry{ma2015ismar}{inproceedings}{}
  \name{author}{4}{}{%
    {{}%
     {Ma}{M.}%
     {Meng}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Merckx}{M.}%
     {Kevin}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Fallavollita}{F.}%
     {Pascal}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Navab}{N.}%
     {Nassir}{N.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MM+1}
  \strng{fullhash}{MMMKFPNN1}
  \field{labelalpha}{Ma+15}
  \field{sortinit}{M}
  \field{booktitle}{2015 IEEE Int. Symp. Mix. Augment. Real.}
  \verb{doi}
  \verb 10.1109/ISMAR.2015.25
  \endverb
  \field{isbn}{978-1-4673-7660-0}
  \field{pages}{76\bibrangedash 79}
  \field{title}{{[POSTER] Natural User Interface for Ambient Objects}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7328065
  \endverb
  \list{location}{1}{%
    {Fukuoka, Japan}%
  }
  \field{year}{2015}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Matthews2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Matthews}{M.}%
     {Danielle}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Behne}{B.}%
     {Tanya}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Lieven}{L.}%
     {Elena}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Tomasello}{T.}%
     {Michael}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MD+1}
  \strng{fullhash}{MDBTLETM1}
  \field{labelalpha}{Mat+12}
  \field{sortinit}{M}
  \field{abstract}{%
  Despite its importance in the development of children's skills of social
  cognition and communication, very little is known about the ontogenetic
  origins of the pointing gesture. We report a training study in which mothers
  gave children one month of extra daily experience with pointing as compared
  with a control group who had extra experience with musical activities. One
  hundred and two infants of 9, 10, or 11 months of age were seen at the
  beginning, middle, and end of this one-month period and tested for
  declarative pointing and gaze following. Infants'ability to point with the
  index finger at the end of the study was not affected by the training but was
  instead predicted by infants' prior ability to follow the gaze direction of
  an adult. The frequency with which infants pointed indexically was also
  affected by infant gaze following ability and, in addition, by maternal
  pointing frequency in free play, but not by training. In contrast, infants'
  ability to monitor their partner's gaze when pointing, and the frequency with
  which they did so, was affected by both training and maternal pointing
  frequency in free play. These results suggest that prior social cognitive
  advances, rather than adult socialization of pointing per se, determine the
  developmental onset of indexical pointing, but socialization processes such
  as imitation and adult shaping subsequently affect both infants' ability to
  monitor their interlocutor's gaze while they point and how frequently infants
  choose to point.%
  }
  \verb{doi}
  \verb 10.1111/j.1467-7687.2012.01181.x
  \endverb
  \field{isbn}{1467-7687}
  \field{issn}{1363755X}
  \field{number}{6}
  \field{pages}{817\bibrangedash 829}
  \field{title}{{Origins of the human pointing gesture: a training study}}
  \verb{url}
  \verb http://doi.wiley.com/10.1111/j.1467-7687.2012.01181.x
  \endverb
  \field{volume}{15}
  \field{journaltitle}{Dev. Sci.}
  \field{year}{2012}
\endentry

\entry{McNeill2000}{article}{}
  \name{author}{1}{}{%
    {{}%
     {McNeill}{M.}%
     {D}{D}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MD1}
  \strng{fullhash}{MD1}
  \field{labelalpha}{McN00}
  \field{sortinit}{M}
  \field{title}{{15 Catchments and contexts: non-modular factors in speech and
  gesture production}}
  \verb{url}
  \verb https://books.google.de/books?hl=en{\&}lr={\&}id=DRBcMQuSrf8C{\&}oi=fnd
  \verb {\&}pg=PA312{\&}dq=catchments+and+context+non-modular+factors+in+speech
  \verb +and+gesture{\&}ots=jCCY5ztpmq{\&}sig=VsHbZojrjLuVgrTKiTpjqfZyBq4
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/MaMeng/AppData/Local/Mendeley Ltd./Mendeley Deskt
  \verb op/Downloaded/McNeill - 2000 - 15 Catchments and contexts non-modular f
  \verb actors in speech and gesture production.pdf:pdf
  \endverb
  \field{journaltitle}{Lang. gesture}
  \field{year}{2000}
\endentry

\entry{Mistry2009}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Mistry}{M.}%
     {Pranav}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Maes}{M.}%
     {Pattie}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Chang}{C.}%
     {Liyan}{L.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Gestural interaction,Object augmentation,Tangible
  computing,Wearable interface}
  \strng{namehash}{MP+1}
  \strng{fullhash}{MPMPCL1}
  \field{labelalpha}{Mis+09}
  \field{sortinit}{M}
  \field{abstract}{%
  Information is traditionally confined to paper or digitally to a screen. In
  this paper, we introduce WUW, a wearable gestural interface, which attempts
  to bring information out into the tangible world. By using a tiny projector
  and a camera mounted on a hat or coupled in a pendant like wearable device,
  WUW sees what the user sees and visually augments surfaces or physical
  objects the user is interacting with. WUW projects information onto surfaces,
  walls, and physical objects around us, and lets the user interact with the
  projected information through natural hand gestures, arm movements, or
  interaction with the object itself.%
  }
  \verb{doi}
  \verb 10.1145/1520340.1520626
  \endverb
  \field{isbn}{9781605582474}
  \field{issn}{9781605582467}
  \field{pages}{4111}
  \field{title}{{WUW - wear Ur world}}
  \verb{url}
  \verb http://www.scopus.com/inward/record.url?eid=2-s2.0-70349163742{\&}partn
  \verb erID=tZOtx3y1
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/MaMeng/AppData/Local/Mendeley Ltd./Mendeley Deskt
  \verb op/Downloaded/Mistry, Maes, Chang - 2009 - WUW-wear Ur world a wearable
  \verb  gestural interface.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 27th Int. Conf. Ext. Abstr. Hum. factors Comput.
  Syst. - CHI EA '09}
  \field{year}{2009}
\endentry

\entry{Moraes2012}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Moraes}{M.}%
     {Thiago~Franco}{T.~F.}%
     {}{}%
     {}{}}%
    {{}%
     {Amorim}{A.}%
     {Paulo Henrique~Junqueira}{P.~H.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Azevedo}{A.}%
     {F{\'{a}}bio~S}{F.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Silva}{S.}%
     {Jorge Vicente~Lopes}{J.~V.~L.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{MTF+1}
  \strng{fullhash}{MTFAPHJAFSSJVL1}
  \field{labelalpha}{Mor+12}
  \field{sortinit}{M}
  \field{abstract}{%
  This work presents InVesalius, an open-source imaging. Initially, the
  historical context and motivation for its development are discussed. Then,
  the software structure and some development aspects are commented, as well as
  InVesalius main tools and applications. At the end, the results of a survey
  realized within the user community are presented.%
  }
  \field{pages}{405\bibrangedash 408}
  \field{title}{{InVesalius - An open-source imaging application}}
  \field{journaltitle}{Comput. Vis. Med. Image Process.}
  \field{year}{2012}
\endentry

\entry{Nanayakkara2013a}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Nanayakkara}{N.}%
     {Suranga}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Shilkrot}{S.}%
     {Roy}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Yeo}{Y.}%
     {Kian~Peen}{K.~P.}%
     {}{}%
     {}{}}%
    {{}%
     {Maes}{M.}%
     {Pattie}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{NS+1}
  \strng{fullhash}{NSSRYKPMP1}
  \field{labelalpha}{Nan+13}
  \field{sortinit}{N}
  \field{abstract}{%
  Finger-worn interfaces remain a vastly unexplored space for user interfaces,
  despite the fact that our fingers and hands are naturally used for
  referencing and interacting with the environment. In this paper we present
  design guidelines and implementation of a finger-worn I/O device, the
  EyeRing, which leverages the universal and natural gesture of pointing. We
  present use cases of EyeRing for both visually impaired and sighted people.
  We discuss initial reactions from visually impaired users which suggest that
  EyeRing may indeed offer a more seamless solution for dealing with their
  immediate surroundings than the solutions they currently use. We also report
  on a user study that demonstrates how EyeRing reduces effort and disruption
  to a sighted user. We conclude that this highly promising form factor offers
  both audiences enhanced, seamless interaction with information related to
  objects in the environment.%
  }
  \verb{doi}
  \verb 10.1145/2459236.2459240
  \endverb
  \field{isbn}{9781450319041}
  \field{pages}{13\bibrangedash 20}
  \field{title}{{EyeRing}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2459236.2459240
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Nanayakkara et al/Proceedings of the 4th Augment
  \verb ed Human International Conference on - AH '13/Nanayakkara et al. - 2013
  \verb  - EyeRing.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 4th Augment. Hum. Int. Conf. - AH '13}
  \field{year}{2013}
\endentry

\entry{Navab2012a}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Navab}{N.}%
     {N}{N}%
     {}{}%
     {}{}}%
    {{}%
     {Blum}{B.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Lejing}{L.}%
     {Wang}{W.}%
     {}{}%
     {}{}}%
    {{}%
     {Okur}{O.}%
     {A}{A}%
     {}{}%
     {}{}}%
    {{}%
     {Wendler}{W.}%
     {T}{T}%
     {}{}%
     {}{}}%
  }
  \keyw{augmented reality,augmented reality visualization systems,first
  deployments,medical computing,medical intervention,operating rooms,patient
  information systems,user interfaces,user-friendly interfaces}
  \strng{namehash}{NN+1}
  \strng{fullhash}{NNBTLWOAWT1}
  \field{labelalpha}{Nav+12}
  \field{sortinit}{N}
  \field{abstract}{%
  Researchers are developing augmented reality visualization systems to provide
  accessible and user-friendly interfaces for medical intervention and patient
  information systems. A related video can be seen here:
  http://youtu.be/kifj0ZP4Mos. It shows how augmented reality visualization
  systems can provide accessible and user-friendly interfaces for medical
  intervention and patient information systems.%
  }
  \verb{doi}
  \verb 10.1109/mc.2012.75
  \endverb
  \field{isbn}{0018-9162}
  \field{issn}{0018-9162}
  \field{number}{7}
  \field{pages}{48\bibrangedash 55}
  \field{title}{{First Deployments of Augmented Reality in Operating Rooms}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/ielx5/2/6228562/06165241.pdf?tp={\&}arnumber
  \verb =6165241{\&}isnumber=6228562
  \endverb
  \field{volume}{45}
  \field{journaltitle}{Computer (Long. Beach. Calif).}
  \field{year}{2012}
\endentry

\entry{Ni2011}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Ni}{N.}%
     {Tao}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Karlson}{K.}%
     {Amy~K.}{A.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Wigdor}{W.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \strng{namehash}{NT+1}
  \strng{fullhash}{NTKAKWD1}
  \field{labelalpha}{Ni+11}
  \field{sortinit}{N}
  \field{abstract}{%
  In this paper, we explore the use of a projection-based handheld device to
  facilitate in-clinic doctor-patient communication. We present the
  user-centered design process used to understand the workflow of medical
  professionals and to identify challenges they currently face in communicating
  information to patients. Based on the lessons learned, we developed AnatOnMe,
  a prototype projection-based hand-held system for enhancing information
  exchange in the current practice of one medical sub-specialty, physical
  therapy. We then present the results of a controlled experiment to understand
  the desirability and learning tradeoffs of using AnatOnMe to teach medical
  concepts on three potential projection surfaces - wall, model, and patient
  body. Finally, we present results of two expert reviews of the system.%
  }
  \verb{doi}
  \verb 10.1145/1978942.1979437
  \endverb
  \field{isbn}{9781450302289}
  \field{pages}{3333}
  \field{series}{CHI '11}
  \field{title}{{AnatOnMe}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=1978942.1979437
  \endverb
  \verb{file}
  \verb :C$\backslash$:/Users/MaMeng/AppData/Local/Mendeley Ltd./Mendeley Deskt
  \verb op/Downloaded/Ni, Karlson, Wigdor - 2011 - AnatOnMe facilitating doctor
  \verb -patient communication using a projection-based handheld device.pdf:pdf
  \endverb
  \field{journaltitle}{Proc. 2011 Annu. Conf. Hum. factors Comput. Syst. - CHI
  '11}
  \field{year}{2011}
\endentry

\entry{Johnson2011}{report}{}
  \name{author}{3}{}{%
    {{}%
     {North}{N.}%
     {Michael}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Omedes-Pujol}{O.-P.}%
     {Marta}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Williamson}{W.}%
     {Courtney}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Asymmetric catalysis,Cyanohydrin,Hammett analysis,Lewis acids,Lewis
  bases}
  \strng{namehash}{NM+1}
  \strng{fullhash}{NMOPMWC1}
  \field{labelalpha}{Nor+10}
  \field{sortinit}{N}
  \field{abstract}{%
  The asymmetric addition of trimethylsilyl cyanide to aldehydes can be
  catalysed by Lewis acids and/or Lewis bases, which activate the aldehyde and
  trimethylsilyl cyanide, respectively. It is not always apparent from the
  structure of the catalyst whether Lewis acid or Lewis base catalysis
  predominates. To investigate this in the context of using salen complexes of
  titanium, vanadium and aluminium as catalysts, a Hammett analysis of
  asymmetric cyanohydrin synthesis was undertaken. When Lewis acid catalysis is
  dominant, a significantly positive reaction constant is observed, whereas
  reactions dominated by Lewis base catalysis give much smaller reaction
  constants. [{\{}Ti(salen)O{\}}(2)] was found to show the highest degree of
  Lewis acid catalysis, whereas two [VO(salen)X] (X=EtOSO(3) or NCS) complexes
  both displayed lower degrees of Lewis acid catalysis. In the case of
  reactions catalysed by [{\{}Al(salen){\}}(2)O] and triphenylphosphine oxide,
  a non-linear Hammett plot was observed, which is indicative of a change in
  mechanism with increasing Lewis base catalysis as the carbonyl compound
  becomes more electron-deficient. These results suggested that the aluminium
  complex/triphenylphosphine oxide catalyst system should also catalyse the
  asymmetric addition of trimethylsilyl cyanide to ketones and this was found
  to be the case.%
  }
  \field{booktitle}{Chem. - A Eur. J.}
  \verb{doi}
  \verb 10.1002/chem.201001078
  \endverb
  \field{isbn}{9780982829059}
  \field{issn}{09476539}
  \field{pages}{11367\bibrangedash 11375}
  \field{title}{{Investigation of lewis acid versus lewis base catalysis in
  asymmetric cyanohydrin synthesis}}
  \verb{url}
  \verb http://wp.nmc.org/horizon2011/
  \endverb
  \field{volume}{16}
  \field{type}{techreport}
  \field{year}{2010}
\endentry

\entry{Norman2010a}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Norman}{N.}%
     {Donald~a.}{D.~a.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM}%
  }
  \strng{namehash}{NDa1}
  \strng{fullhash}{NDa1}
  \field{labelalpha}{Nor10}
  \field{sortinit}{N}
  \field{abstract}{%
  Gestural interfaces which have been a part of the interface scene are
  discussed. Gestural systems are no different from any other form of
  interaction. Some systems are trying to develop a gestural language,
  sometimes with the number of touch points as a meta-signal about the scope of
  the movement. Gesture and touch-based systems are already so well accepted
  that people make gestures to systems that do not understand them such as
  tapping the screens of non-touch sensitive displays, pinching and expanding
  the fingers or sliding the finger across the screen on systems that do not
  support these actions, and for that matter and waving hands in front of sinks
  that use old-fashioned handles. Gestural systems are indeed one of the
  important future paths for a more holistic, human interaction of people with
  technology.%
  }
  \verb{doi}
  \verb 10.1145/1744161.1744163
  \endverb
  \field{isbn}{9780596518394}
  \field{issn}{10725520}
  \field{number}{3}
  \field{pages}{6}
  \field{title}{{Natural user interfaces are not natural}}
  \verb{url}
  \verb http://dl.acm.org/ft{\_}gateway.cfm?id=1744163{\&}type=html
  \endverb
  \field{volume}{17}
  \field{journaltitle}{Interactions}
  \field{year}{2010}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Nickel2003}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Nickel}{N.}%
     {Kai}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Stiefelhagen}{S.}%
     {Rainer}{R.}%
     {}{}%
     {}{}}%
  }
  \keyw{com-,gesture recognition,person tracking,pointing gestures}
  \strng{namehash}{NKSR1}
  \strng{fullhash}{NKSR1}
  \field{labelalpha}{NS03}
  \field{sortinit}{N}
  \field{abstract}{%
  In this paper, we present a system capable of visually detecting pointing
  gestures and estimating the 3D pointing direction in real-time. In order to
  acquire input features for gesture recognition, we track the positions of a
  person's face and hands on image sequences provided by a stereo-camera.
  Hidden Markov Models (HMMs), trained on different phases of sample pointing
  gestures, are used to classify the 3D-trajectories in order to detect the
  occurrence of a gesture. When analyzing sample pointing gestures, we noticed
  that humans tend to look at the pointing target while performing the gesture.
  In order to utilize this behavior, we additionally measured head orientation
  by means of a magnetic sensor in a similar scenario. By using head
  orientation as an additional feature, we observed significant gains in both
  recall and precision of pointing gestures. Moreover, the percentage of
  correctly identified pointing targets improved significantly from 65{\%} to
  83{\%}. For estimating the pointing direction, we comparatively used three
  approaches: 1) The line of sight between head and hand, 2) the forearm
  orientation, and 3) the head orientation.%
  }
  \verb{doi}
  \verb 10.1145/958432.958460
  \endverb
  \field{isbn}{1581136218}
  \field{issn}{02628856}
  \field{pages}{140\bibrangedash 146}
  \field{title}{{Pointing Gesture Recognition based on 3D-Tracking of Face ,
  Hands and Head Orientation Categories and Subject Descriptors}}
  \field{journaltitle}{Proc. 5th Int. Conf. Multimodal interfaces}
  \field{year}{2003}
\endentry

\entry{OHara2014a}{article}{}
  \name{author}{11}{}{%
    {{}%
     {O'Hara}{O.}%
     {Kenton}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Dastur}{D.}%
     {Neville}{N.}%
     {}{}%
     {}{}}%
    {{}%
     {Carrell}{C.}%
     {Tom}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Gonzalez}{G.}%
     {Gerardo}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Sellen}{S.}%
     {Abigail}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Penney}{P.}%
     {Graeme}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Varnavas}{V.}%
     {Andreas}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Mentis}{M.}%
     {Helena}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Criminisi}{C.}%
     {Antonio}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Corish}{C.}%
     {Robert}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Rouncefield}{R.}%
     {Mark}{M.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM}%
  }
  \strng{namehash}{OK+1}
  \strng{fullhash}{OKDNCTGGSAPGVAMHCACRRM1}
  \field{labelalpha}{O'H+14}
  \field{sortinit}{O}
  \field{abstract}{%
  除证明非接触式交互系统的技术可行
  性外，还应将外科手术中的这类系统进
  行合理设计，让其能在手术室操作环境 内工作。
  手势设计不仅应考虑与医学影像的个
  体交互，还应考虑如何在协作讨论环境 下使用这些影像。
  与单手和双手相关的手势设计应能满
  足表达丰富度的要求，以及手术医生双
  手操作的要求，同时还要受到了手术小
  组成员靠近程度及无菌操作所产生的动 作限制的限制。%
  }
  \verb{doi}
  \verb 10.1145/2541883.2541899
  \endverb
  \field{isbn}{0001-0782}
  \field{issn}{00010782}
  \field{number}{1}
  \field{pages}{70\bibrangedash 77}
  \field{title}{{Touchless interaction in surgery}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?doid=2541883.2541899
  \endverb
  \field{volume}{57}
  \verb{file}
  \verb :E$\backslash$:/papers/O'Hara et al/Communications of the ACM/O'Hara et
  \verb  al. - 2014 - Touchless interaction in surgery(2).pdf:pdf
  \endverb
  \field{journaltitle}{Commun. ACM}
  \field{year}{2014}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{export:244725}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Pamplona}{P.}%
     {Vitor~F.}{V.~F.}%
     {}{}%
     {}{}}%
    {{}%
     {Oliveira}{O.}%
     {Manuel~M.}{M.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Baranoski}{B.}%
     {Gladimir V.~G.}{G.~V.~G.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM – Association for Computing Machinery}%
  }
  \strng{namehash}{PVF+1}
  \strng{fullhash}{PVFOMMBGVG1}
  \field{labelalpha}{Pam+09}
  \field{sortinit}{P}
  \field{abstract}{%
  Muscles provide physiological functions to drive body movement and
  anatomically characterize body shape, making them a crucial component of
  modeling animated human figures. Substantial efforts have been ex- pended on
  developing computational models of muscles for the purpose of increasing
  realism and accuracy in a broad range of applications, includ- ing computer
  graphics and biomechanics. We survey various approaches that have been
  employed to model and simulate muscles both morpholog- ically and
  functionally. Modeling the realistic morphology of muscle re- quires that
  muscle deformations be accurately depicted. To this end, sev- eral
  methodologies have been presented, including geometrically-based,
  physically-based, and data-driven approaches. On the other hand, the simu-
  lation of physiological muscle functions aims to identify the biomechanical
  controls responsible for realistic human motion. Estimating these muscle
  controls has been pursued through static and dynamic simulations. We re- view
  and discuss all these approaches, and conclude with suggestions for future
  research.%
  }
  \verb{doi}
  \verb 10.1145/1559755.1559763
  \endverb
  \verb{eprint}
  \verb 1006.4903
  \endverb
  \field{isbn}{0103660054}
  \field{issn}{07300301}
  \field{number}{4}
  \field{pages}{1\bibrangedash 12}
  \field{title}{{Photorealistic models for pupil light reflex and iridal
  pattern deformation}}
  \verb{url}
  \verb http://research.microsoft.com/apps/pubs/default.aspx?id=244725
  \endverb
  \field{volume}{28}
  \field{journaltitle}{ACM Trans. Graph.}
  \field{eprinttype}{arXiv}
  \field{year}{2009}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Park2006}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Park}{P.}%
     {Jin~Seo}{J.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Chung}{C.}%
     {Min~Suk}{M.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Hwang}{H.}%
     {Sung~Bae}{S.~B.}%
     {}{}%
     {}{}}%
    {{}%
     {Shin}{S.}%
     {Byeong-Seok}{B.-S.}%
     {}{}%
     {}{}}%
    {{}%
     {Park}{P.}%
     {Hyung~Seon}{H.~S.}%
     {}{}%
     {}{}}%
  }
  \keyw{Anatomy,Anatomy: education,Anatomy: methods,Asian Continental Ancestry
  Group,Computer-Assisted Instruction,Education, Medical,
  Undergraduate,Humans,Imaging, Three-Dimensional,Korea,Male,Visible Human
  Projects}
  \strng{namehash}{PJS+1}
  \strng{fullhash}{PJSCMSHSBSBSPHS1}
  \field{labelalpha}{Par+06}
  \field{sortinit}{P}
  \field{abstract}{%
  Three recent studies have offered an unprecedented view of the human body.
  The Visible Human Project, the Visible Korean Human (VKH), and the Chinese
  Visible Human have featured the serial sectioning of whole cadavers,
  producing cross-sectional images that methodically catalogue gross human
  anatomy. By volumetric reconstruction, these cross-sectional images can be
  transformed into three-dimensional (3D) images of anatomic structures.
  Compiling these 3D images would create an invaluable library for medical
  education and research. The goal of this report is to promote the expansion
  of such a library of 3D anatomic images and to help users fully understand
  and utilize the serially sectioned images. To do this, we will discuss the
  fundamental techniques and equipment used in the VKH and its preliminary
  experiments. We will also address new applications of the VKH, including
  virtual brain surgery, virtual endoscopy, and virtual cardiopulmonary
  resuscitation via the development of virtual dissection software.%
  }
  \verb{doi}
  \verb 10.1002/ca.20275
  \endverb
  \field{isbn}{0897-3806}
  \field{issn}{0897-3806}
  \field{number}{3}
  \field{pages}{216\bibrangedash 24}
  \field{title}{{Visible Korean Human: its techniques and applications.}}
  \verb{url}
  \verb http://www.ncbi.nlm.nih.gov/pubmed/16506204
  \endverb
  \field{volume}{19}
  \verb{file}
  \verb :E$\backslash$:/papers/Park et al/Clinical anatomy (New York, N.Y.)/Par
  \verb k et al. - 2006 - Visible Korean Human its techniques and applications.
  \verb pdf:pdf
  \endverb
  \field{journaltitle}{Clin. Anat.}
  \field{year}{2006}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Pederson2010}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Pederson}{P.}%
     {Thomas}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Janlert}{J.}%
     {Lars-Erik}{L.-E.}%
     {}{}%
     {}{}}%
    {{}%
     {Surie}{S.}%
     {Dipak}{D.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM Press}%
  }
  \keyw{interaction paradigm,user interface design}
  \strng{namehash}{PT+1}
  \strng{fullhash}{PTJLESD1}
  \field{labelalpha}{Ped+10}
  \field{sortinit}{P}
  \field{booktitle}{Proc. 6th Nord. Conf. Human-Computer Interact. Extending
  Boundaries - Nord. '10}
  \verb{doi}
  \verb 10.1145/1868914.1869022
  \endverb
  \field{isbn}{9781605589343}
  \field{pages}{755}
  \field{title}{{Towards a model for egocentric interaction with physical and
  virtual objects}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=1868914.1869022
  \endverb
  \list{location}{1}{%
    {New York, New York, USA}%
  }
  \field{year}{2010}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Pierce1997}{article}{}
  \name{author}{6}{}{%
    {{}%
     {Pierce}{P.}%
     {Jeffrey~S}{J.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Forsberg}{F.}%
     {Andrew~S}{A.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Conway}{C.}%
     {Matthew~J}{M.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Hong}{H.}%
     {Seung}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Zeleznik}{Z.}%
     {Robert~C}{R.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Mine}{M.}%
     {Mark~R}{M.~R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{PJS+2}
  \strng{fullhash}{PJSFASCMJHSZRCMMR1}
  \field{labelalpha}{Pie+97}
  \field{sortinit}{P}
  \field{abstract}{%
  An abstract is not available.%
  }
  \verb{doi}
  \verb 10.1145/253284.253303
  \endverb
  \field{isbn}{0897918843}
  \field{issn}{2000-6764}
  \field{pages}{39\bibrangedash ff}
  \field{title}{{Image plane interaction techniques in 3D immersive
  environments}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=253284.253303
  \endverb
  \field{journaltitle}{ACM i3D}
  \field{year}{1997}
\endentry

\entry{Rosa2014}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Rosa}{R.}%
     {Guillermo~M.}{G.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Elizondo}{E.}%
     {Mar{\'{\i}}a~L.}{M.~L.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {English}%
  }
  \keyw{Diagnostic imaging,Medical informatics
  computing,Oral,Surgery,User-computer interface}
  \strng{namehash}{RGMEML1}
  \strng{fullhash}{RGMEML1}
  \field{labelalpha}{RE14}
  \field{sortinit}{R}
  \field{abstract}{%
  PURPOSE: The purposes of this study were to develop a workstation computer
  that allowed intraoperative touchless control of diagnostic and surgical
  images by a surgeon, and to report the preliminary experience with the use of
  the system in a series of cases in which dental surgery was
  performed.$\backslash$n$\backslash$nMATERIALS AND METHODS: A custom
  workstation with a new motion sensing input device (Leap Motion) was set up
  in order to use a natural user interface (NUI) to manipulate the imaging
  software by hand gestures. The system allowed intraoperative touchless
  control of the surgical images.$\backslash$n$\backslash$nRESULTS: For the
  first time in the literature, an NUI system was used for a pilot study during
  11 dental surgery procedures including tooth extractions, dental implant
  placements, and guided bone regeneration. No complications were reported. The
  system performed very well and was very
  useful.$\backslash$n$\backslash$nCONCLUSION: The proposed system fulfilled
  the objective of providing touchless access and control of the system of
  images and a three-dimensional surgical plan, thus allowing the maintenance
  of sterile conditions. The interaction between surgical staff, under sterile
  conditions, and computer equipment has been a key issue. The solution with an
  NUI with touchless control of the images seems to be closer to an ideal. The
  cost of the sensor system is quite low; this could facilitate its
  incorporation into the practice of routine dental surgery. This technology
  has enormous potential in dental surgery and other healthcare specialties.%
  }
  \verb{doi}
  \verb 10.5624/isd.2014.44.2.155
  \endverb
  \field{isbn}{978-1-4577-1322-4}
  \field{issn}{22337830}
  \field{number}{2}
  \field{pages}{155\bibrangedash 160}
  \field{title}{{Use of a gesture user interface as a touchless image
  navigation system in dental surgery: Case series report}}
  \verb{url}
  \verb http://dx.doi.org/10.5624/isd.2014.44.2.155
  \endverb
  \field{volume}{44}
  \verb{file}
  \verb :E$\backslash$:/papers/Yamada et al/Imaging science in dentistry/Yamada
  \verb  et al. - 2014 - Use of a gesture user interface as a touchless image n
  \verb avigation system in dental surgery Case series report.pdf:pdf
  \endverb
  \field{journaltitle}{Imaging Sci. Dent.}
  \field{year}{2014}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Reale2011}{inproceedings}{}
  \name{author}{5}{}{%
    {{}%
     {Reale}{R.}%
     {Michael~J.}{M.~J.}%
     {}{}%
     {}{}}%
    {{}%
     {Canavan}{C.}%
     {Shaun}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Yin}{Y.}%
     {Lijun}{L.}%
     {}{}%
     {}{}}%
    {{}%
     {Hu}{H.}%
     {Kaoning}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Hung}{H.}%
     {Terry}{T.}%
     {}{}%
     {}{}}%
  }
  \keyw{3D eyeball location,3D hand pointing,3D iris disk model,Adaptive
  optics,Calibration,Cameras,Estimation,Face,Gaze estimation,Iris,Three
  dimensional displays,active appearance model,cameras,data visualisation,eye
  detector,eye gaze gesture,eyeball radius,fovea position,gaze
  estimation,gesture recognition,hand pointing gesture,hand tracking,head pose
  gesture,human computer interaction,human–computer interaction
  (HCI),information visualization,iris center,iris contour points,iris
  recognition,mouth motions gesture,multigesture interaction
  system,pan-tilt-zoom camera,point-of-regard tracking,solid
  modelling,two-camera system,vision-based human-computer interaction system}
  \strng{namehash}{RMJ+1}
  \strng{fullhash}{RMJCSYLHKHT1}
  \field{labelalpha}{Rea+11}
  \field{sortinit}{R}
  \field{abstract}{%
  In this paper, we present a vision-based human-computer interaction system,
  which integrates control components using multiple gestures, including eye
  gaze, head pose, hand pointing, and mouth motions. To track head, eye, and
  mouth movements, we present a two-camera system that detects the face from a
  fixed, wide-angle camera, estimates a rough location for the eye region using
  an eye detector based on topographic features, and directs another active
  pan-tilt-zoom camera to focus in on this eye region. We also propose a novel
  eye gaze estimation approach for point-of-regard (POR) tracking on a viewing
  screen. To allow for greater head pose freedom, we developed a new
  calibration approach to find the 3-D eyeball location, eyeball radius, and
  fovea position. Moreover, in order to get the optical axis, we create a 3-D
  iris disk by mapping both the iris center and iris contour points to the
  eyeball sphere. We then rotate the fovea accordingly and compute the final,
  visual axis gaze direction. This part of the system permits natural,
  non-intrusive, pose-invariant POR estimation from a distance without
  resorting to infrared or complex hardware setups. We also propose and
  integrate a two-camera hand pointing estimation algorithm for hand gesture
  tracking in 3-D from a distance. The algorithms of gaze pointing and hand
  finger pointing are evaluated individually, and the feasibility of the entire
  system is validated through two interactive information visualization
  applications.%
  }
  \field{booktitle}{IEEE Trans. Multimed.}
  \verb{doi}
  \verb 10.1109/TMM.2011.2120600
  \endverb
  \field{isbn}{1520-9210}
  \field{issn}{1520-9210}
  \field{number}{3}
  \field{pages}{474\bibrangedash 486}
  \field{shorttitle}{Multimedia, IEEE Transactions on}
  \field{title}{{A Multi-Gesture Interaction System Using a 3-D Iris Disk Model
  for Gaze Estimation and an Active Appearance Model for 3-D Hand Pointing}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5720546
  \endverb
  \field{volume}{13}
  \verb{file}
  \verb :E$\backslash$:/papers/Reale et al/IEEE Transactions on Multimedia/Real
  \verb e et al. - 2011 - A Multi-Gesture Interaction System Using a 3-D Iris D
  \verb isk Model for Gaze Estimation and an Active Appearance Model.pdf:pdf
  \endverb
  \field{year}{2011}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Suzuki1985}{misc}{}
  \name{author}{2}{}{%
    {{}%
     {Suzuki}{S.}%
     {Satoshi}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Abe}{A.}%
     {Keiichi}{K.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SSAK1}
  \strng{fullhash}{SSAK1}
  \field{labelalpha}{SA85}
  \field{sortinit}{S}
  \field{abstract}{%
  Two border following algorithms are proposed for the topological analysis of
  digitized binary images. The first one determines the surroundness relations
  among the borders of a binary image. Since the outer borders and the hole
  borders have a one-to-one correspondence to the connected components of
  l-pixels and to the holes, respectively, the proposed algorithm yields a
  representation of a binary image, from which one can extract some sort of
  features without reconstructing the image. The second algorithm, which is a
  modified version of the first, follows only the outermost borders (i.e., the
  outer borders which are not surrounded by holes). These algorithms can be
  effectively used in component counting, shrinking, and topological structural
  analysis of binary images, when a sequential digital computer is used%
  }
  \field{booktitle}{Comput. Vision, Graph. Image Process.}
  \verb{doi}
  \verb 10.1016/0734-189X(85)90136-7
  \endverb
  \field{isbn}{0734-189X}
  \field{issn}{0734189X}
  \field{number}{3}
  \field{pages}{396}
  \field{title}{{Topological structural analysis of digitized binary images by
  border following}}
  \field{volume}{29}
  \field{year}{1985}
\endentry

\entry{Schwarz2011a}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Schwarz}{S.}%
     {Loren~Arthur}{L.~A.}%
     {}{}%
     {}{}}%
    {{}%
     {Bigdelou}{B.}%
     {Ali}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Navab}{N.}%
     {Nassir}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{Algorithms,Computer Systems,Computer-Assisted,Computer-Assisted:
  instrumentation,Computer-Assisted: methods,Equipment
  Design,Gestures,Humans,Learning,Movement,Operating Rooms,Operative,Operative:
  methods,Surgery,Surgical Procedures,Task Performance and
  Analysis,User-Computer Interface}
  \strng{namehash}{SLA+1}
  \strng{fullhash}{SLABANN1}
  \field{labelalpha}{Sch+11}
  \field{sortinit}{S}
  \field{abstract}{%
  Interaction with computer-based medical devices in the operating room is
  often challenging for surgeons due to sterility requirements and the
  complexity of interventional procedures. Typical solutions, such as
  delegating the interaction task to an assistant, can be inefficient. We
  propose a method for gesture-based interaction in the operating room that
  surgeons can customize to personal requirements and interventional workflow.
  Given training examples for each desired gesture, our system learns
  low-dimensional manifold models that enable recognizing gestures and tracking
  particular poses for fine-grained control. By capturing the surgeon's
  movements with a few wireless body-worn inertial sensors, we avoid issues of
  camera-based systems, such as sensitivity to illumination and occlusions.
  Using a component-based framework implementation, our method can easily be
  connected to different medical devices. Our experiments show that the
  approach is able to robustly recognize learned gestures and to distinguish
  these from other movements.%
  }
  \verb{doi}
  \verb 10.1007/978-3-642-23623-5{\_}17
  \endverb
  \field{isbn}{9783642236228}
  \field{issn}{03029743}
  \field{number}{PART 1}
  \field{pages}{129\bibrangedash 136}
  \field{title}{{Learning gestures for customizable human-computer interaction
  in the operating room}}
  \verb{url}
  \verb http://link.springer.com/chapter/10.1007/978-3-642-23623-5{\_}17 http:/
  \verb /www.ncbi.nlm.nih.gov/pubmed/22003609
  \endverb
  \field{volume}{6891 LNCS}
  \field{journaltitle}{Lect. Notes Comput. Sci. (including Subser. Lect. Notes
  Artif. Intell. Lect. Notes Bioinformatics)}
  \field{year}{2011}
\endentry

\entry{Strickland2013}{article}{}
  \name{author}{4}{}{%
    {{}%
     {Strickland}{S.}%
     {Matt}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Tremaine}{T.}%
     {Jamie}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Brigley}{B.}%
     {Greg}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Law}{L.}%
     {Calvin}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{Adrenalectomy,Adrenalectomy:
  instrumentation,Aged,Computer-Assisted,Computer-Assisted:
  instrumentation,Equipment Design,Female,Gestures,Hepatectomy,Hepatectomy:
  instrumentation,Humans,Imaging,Intraoperative,Intraoperative:
  instrumentation,Laparoscopy,Male,Middle
  Aged,Monitoring,Pancreatectomy,Pancreatectomy: instrumentation,Pilot
  Projects,Surgery,Three-Dimensional,Three-Dimensional:
  instrumentation,User-Computer Interface}
  \strng{namehash}{SM+1}
  \strng{fullhash}{SMTJBGLC1}
  \field{labelalpha}{Str+13}
  \field{sortinit}{S}
  \field{abstract}{%
  BACKGROUND: As surgical procedures become increasingly dependent on equipment
  and imaging, the need for sterile members of the surgical team to have
  unimpeded access to the nonsterile technology in their operating room (OR) is
  of growing importance. To our knowledge, our team is the first to use an
  inexpensive infrared depthsensing camera (a component of the Microsoft
  Kinect) and software developed inhouse to give surgeons a touchless, gestural
  interface with which to navigate their picture archiving and communication
  systems intraoperatively.$\backslash$n$\backslash$nMETHODS: The system was
  designed and developed with feedback from surgeons and OR personnel and with
  consideration of the principles of aseptic technique and gestural controls in
  mind. Simulation was used for basic validation before trialing in a pilot
  series of 6 hepatobiliary-pancreatic
  surgeries.$\backslash$n$\backslash$nRESULTS: The interface was used
  extensively in 2 laparoscopic and 4 open procedures. Surgeons primarily used
  the system for anatomic correlation, real-time comparison of intraoperative
  ultrasound with preoperative computed tomography and magnetic resonance
  imaging scans and for teaching residents and
  fellows.$\backslash$n$\backslash$nCONCLUSION: The system worked well in a
  wide range of lighting conditions and procedures. It led to a perceived
  increase in the use of intraoperative image consultation. Further research
  should be focused on investigating the usefulness of touchless gestural
  interfaces in different types of surgical procedures and its effects on
  operative time.$\backslash$n$\backslash$nAbstract available from the
  publisher.%
  }
  \verb{doi}
  \verb 10.1503/cjs.035311
  \endverb
  \field{isbn}{1488-2310}
  \field{issn}{0008428X}
  \field{number}{3}
  \field{pages}{1\bibrangedash 6}
  \field{title}{{Using a depth-sensing infrared camera system to access and
  manipulate medical imaging from within the sterile operating field}}
  \verb{url}
  \verb http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3672422/?tool=pmcentrez
  \endverb
  \field{volume}{56}
  \verb{file}
  \verb ::
  \endverb
  \field{journaltitle}{Can. J. Surg.}
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Sun2013}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Sun}{S.}%
     {Jing}{J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SJ1}
  \strng{fullhash}{SJ1}
  \field{labelalpha}{Sun13}
  \field{sortinit}{S}
  \field{title}{{Motion Learning with Biomechanics Principles}}
  \verb{url}
  \verb http://scholarworks.sjsu.edu/etd{\_}projects/313/
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Sun/Unknown/Sun - 2013 - Motion Learning with Bi
  \verb omechanics Principles.pdf:pdf
  \endverb
  \field{year}{2013}
\endentry

\entry{Swindells2002}{thesis}{}
  \name{author}{1}{}{%
    {{}%
     {Swindells}{S.}%
     {Colin~Edward}{C.~E.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SCE1}
  \strng{fullhash}{SCE1}
  \field{labelalpha}{Swi02}
  \field{sortinit}{S}
  \field{abstract}{%
  Computing devices within current work and play environments are relatively
  static. As the number of 'networked' devices grows, and as people and their
  devices become more dynamic, situations will commonly arise where users will
  wish to use 'that device there' instead of navigating through traditional
  user interface widgets such as lists. This paper describes a process for
  identifying devices through a pointing gesture using custom tags and a custom
  stylus called the gesturePen. Implementation details for this system are
  provided along with qualitative and quantitative results from a formal user
  study. As ubiquitous computing environments become more pervasive, people
  will rapidly switch their focus between many computing devices. The results
  of our work demonstrate that our gesturePen method can improve the user
  experience in ubiquitous environments by facilitating significantly faster
  interactions between computing devices.%
  }
  \field{booktitle}{Thes}
  \verb{doi}
  \verb 10.1145/571985.572007
  \endverb
  \field{isbn}{1581134886}
  \field{number}{2}
  \field{pages}{151}
  \field{title}{{That one there! Pointing to establish device identity}}
  \verb{url}
  \verb http://portal.acm.org/citation.cfm?doid=571985.572007
  \endverb
  \field{volume}{4}
  \verb{file}
  \verb :C$\backslash$:/Users/MaMeng/AppData/Local/Mendeley Ltd./Mendeley Deskt
  \verb op/Downloaded/Swindells - 2002 - That one there! Pointing to establish
  \verb device identity.pdf:pdf
  \endverb
  \field{type}{phdthesis}
  \field{year}{2002}
\endentry

\entry{Tangcharoen2011}{misc}{}
  \name{author}{9}{}{%
    {{}%
     {Tangcharoen}{T.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Bell}{B.}%
     {A}{A}%
     {}{}%
     {}{}}%
    {{}%
     {Hegde}{H.}%
     {S}{S}%
     {}{}%
     {}{}}%
    {{}%
     {Hussain}{H.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Beerbaum}{B.}%
     {P}{P}%
     {}{}%
     {}{}}%
    {{}%
     {Schaeffter}{S.}%
     {T}{T}%
     {}{}%
     {}{}}%
    {{}%
     {Razavi}{R.}%
     {R}{R}%
     {}{}%
     {}{}}%
    {{}%
     {Botnar}{B.}%
     {R~M}{R.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Greil}{G.}%
     {G~F}{G.~F.}%
     {}{}%
     {}{}}%
  }
  \keyw{Child,Congenital/pathology,Coronary Vessel
  Anomalies/*pathology,Coronary Vessels/*pathology,Female,Heart
  Defects,Humans,Infant,Magnetic Resonance
  Angiography/*methods,Male,Preschool,Reproducibility of Results,Sensitivity
  and Specificity}
  \strng{namehash}{TT+1}
  \strng{fullhash}{TTBAHSHTBPSTRRBRMGGF1}
  \field{labelalpha}{Tan+11}
  \field{sortinit}{T}
  \field{abstract}{%
  PURPOSE: To evaluate the feasibility and accuracy of magnetic resonance (MR)
  coronary angiography for the detection of coronary artery anomalies in
  infants and children by using surgical findings as a reference. MATERIALS AND
  METHODS: The data analysis was approved by the institutional review board.
  One hundred children with congenital heart disease underwent MR coronary
  angiography while under general anesthesia (mean age +/- standard deviation,
  3.9 years +/- 3; age range, 0.2-11 years). A navigator-gated, T2-prepared,
  three-dimensional steady-state free precession whole-heart protocol
  (isotropic voxel size, 1.0-1.3 mm(3); mean imaging time, 4.6 minutes +/- 1.2;
  mean navigator efficiency, 70{\%}; 3-mm gating window) was used after
  injection of gadopentetate dimeglumine. The cardiac rest period (end systole
  or middiastole) and acquisition window were prospectively assessed for each
  patient. Coronary artery image quality (score of 0 [nondiagnostic] to 4
  [excellent]), vessel sharpness, and coronary artery anomalies were assessed
  by two observers. Surgery was performed in 58 patients, and those findings
  were used to define accuracy. Variables were assessed between age groups by
  using either analysis of variance or Kruskal-Wallis tests. RESULTS:
  Diagnostic image quality (score, >/=1 for all coronary artery segments) was
  obtained in 46 of the 58 patients (79{\%}) who underwent surgery. The origin
  and course of the coronary artery anatomy depicted with MR imaging was
  confirmed at surgery in all 46 patients-including the four (9{\%}) with
  substantial coronary artery anomalies. Diagnostic-quality images were
  obtained in 84 of the 100 patients. The rate of success improved
  significantly when patients were older than 4 months (88{\%} for patients >4
  months vs 17{\%} for patients </=4 months, P < .001). CONCLUSION: Improved
  whole-heart MR coronary angiography enables accurate detection of abnormal
  origin and course of the coronary artery system even in very young patients
  with congenital heart disease.%
  }
  \field{booktitle}{Radiology}
  \verb{doi}
  \verb radiol.10100828 [pii] 10.1148/radiol.10100828
  \endverb
  \field{isbn}{1527-1315 (Electronic) 0033-8419 (Linking)}
  \field{issn}{1527-1315}
  \field{number}{1}
  \field{pages}{240\bibrangedash 247}
  \field{title}{{Detection of coronary artery anomalies in infants and young
  children with congenital heart disease by using MR imaging}}
  \verb{url}
  \verb http://www.ncbi.nlm.nih.gov/pubmed/21325034
  \endverb
  \field{volume}{259}
  \field{year}{2011}
\endentry

\entry{Tan2013}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Tan}{T.}%
     {Justin~H}{J.~H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chao}{C.}%
     {Cherng}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Zawaideh}{Z.}%
     {Mazen}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Roberts}{R.}%
     {Anne~C}{A.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Kinney}{K.}%
     {Thomas~B}{T.~B.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {en}%
  }
  \list{publisher}{1}{%
    {Radiological Society of North America}%
  }
  \keyw{Computer-Assisted,Computer-Assisted: instrumentation,Equipment
  Design,Equipment Failure
  Analysis,Gestures,Humans,Interventional,Interventional:
  instrumentation,Radiographic Image
  Interpretation,Radiography,Surgery,Touch,User-Computer Interface,Video Games}
  \strng{namehash}{TJH+1}
  \strng{fullhash}{TJHCCZMRACKTB1}
  \field{labelalpha}{Tan+13}
  \field{sortinit}{T}
  \field{abstract}{%
  Review of prior and real-time patient images is critical during an
  interventional radiology procedure; however, it often poses the challenge of
  efficiently reviewing images while maintaining a sterile field. Although
  interventional radiologists can "scrub out" of the procedure, use sterile
  console covers, or verbally relay directions to an assistant, the ability of
  the interventionalist to directly control the images without having to touch
  the console could offer potential gains in terms of sterility, procedure
  efficiency, and radiation reduction. The authors investigated a potential
  solution with a low-cost, touch-free motion-tracking device that was
  originally designed as a video game controller. The device tracks a person's
  skeletal frame and its motions, a capacity that was adapted to allow
  manipulation of medical images by means of hand gestures. A custom software
  program called the Touchless Radiology Imaging Control System translates
  motion information obtained with the motion-tracking device into commands to
  review images on a workstation. To evaluate this system, 29 radiologists at
  the authors' institution were asked to perform a set of standardized tasks
  during a routine abdominal computed tomographic study. Participants evaluated
  the device for its efficacy as well as its possible advantages and
  disadvantages. The majority (69{\%}) of those surveyed believed that the
  device could be useful in an interventional radiology practice and did not
  foresee problems with maintaining a sterile field. This proof-of-concept
  prototype and study demonstrate the potential utility of the motion-tracking
  device for enhancing imaging-guided treatment in the interventional radiology
  suite while maintaining a sterile field. Supplemental material available at
  http://radiographics.rsna.org/lookup/suppl/doi:10.1148/rg.332125101/-/DC1.%
  }
  \verb{doi}
  \verb 10.1148/rg.332125101
  \endverb
  \field{isbn}{1527-1323}
  \field{issn}{1527-1323}
  \field{number}{2}
  \field{pages}{E61\bibrangedash 70}
  \field{title}{{Informatics in Radiology: developing a touchless user
  interface for intraoperative image control during interventional radiology
  procedures.}}
  \verb{url}
  \verb http://www.ncbi.nlm.nih.gov/pubmed/23264282
  \endverb
  \field{volume}{33}
  \field{journaltitle}{Radiographics}
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Thijssen2010}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Thijssen}{T.}%
     {Anthony~S.}{A.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Schijven}{S.}%
     {Marlies~P.}{M.~P.}%
     {}{}%
     {}{}}%
  }
  \keyw{Education,Laparoscopy,Skills assessment,Virtual reality}
  \strng{namehash}{TASSMP1}
  \strng{fullhash}{TASSMP1}
  \field{labelalpha}{TS10}
  \field{sortinit}{T}
  \field{abstract}{%
  Background: A demand for safe, efficient laparoscopic training tools has
  prompted the introduction of virtual reality (VR) laparoscopic simulators,
  which might be used for performance assessment. The purpose of this review is
  to determine the value of VR metrics in laparoscopic skills assessment. Data
  sources: An exhaustive search of the MEDLINE and EMBASE databases was
  performed to identify publications concerning construct, concurrent and
  predictive validation of VR simulators. Of 643 publications found, 42 were
  included in this review. Studies into all 3 types of validation showed a
  large heterogeneity in study design. Although concurrence of VR metrics with
  box trainer metrics, mental aptitude tests, and in vivo surgical performance
  was generally weak, several metrics demonstrated construct validity in
  selected simulators. Conclusions: Using the right simulator, tasks, and
  metrics, trainees' and experts' laparoscopic skills can reliably be compared.
  However, VR simulators cannot yet predict levels of real life surgical
  skills. ?? 2010 Elsevier Inc. All rights reserved.%
  }
  \verb{doi}
  \verb 10.1016/j.amjsurg.2009.04.015
  \endverb
  \field{isbn}{1879-1883 (Electronic)$\backslash$r0002-9610 (Linking)}
  \field{issn}{00029610}
  \field{number}{4}
  \field{pages}{529\bibrangedash 541}
  \field{title}{{Contemporary virtual reality laparoscopy simulators: quicksand
  or solid grounds for assessing surgical trainees?}}
  \verb{url}
  \verb http://linkinghub.elsevier.com/retrieve/pii/S0002961009004310
  \endverb
  \field{volume}{199}
  \list{institution}{1}{%
    {Department of Surgery, University Medical Center Utrecht, Utrecht, The
  Netherlands.}%
  }
  \field{journaltitle}{Am. J. Surg.}
  \field{year}{2010}
\endentry

\entry{Tozeren2000}{book}{}
  \name{author}{1}{}{%
    {{}%
     {T{\"{o}}zeren}{T.}%
     {A}{A}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{TA1}
  \strng{fullhash}{TA1}
  \field{labelalpha}{T{\"{o}}z00}
  \field{sortinit}{T}
  \field{title}{{Human body dynamics: classical mechanics and human movement}}
  \verb{url}
  \verb https://books.google.com/books?hl=en{\&}lr={\&}id=Bs7ndEsyth0C{\&}oi=fn
  \verb d{\&}pg=PR7{\&}dq=Human+body+dynamics:+classical+mechanics+and+human+mo
  \verb ve-ment{\&}ots=kcfcAmV8Oz{\&}sig=5QokzkdHLbiY-wNzGSDZihfJxAs
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/T{\"{o}}zeren/Unknown/T{\"{o}}zeren - 2000 - Hum
  \verb an body dynamics classical mechanics and human movement.pdf:pdf
  \endverb
  \field{year}{2000}
\endentry

\entry{Vogel2005}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Vogel}{V.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Balakrishnan}{B.}%
     {Ravin}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{VDBR1}
  \strng{fullhash}{VDBR1}
  \field{labelalpha}{VB05}
  \field{sortinit}{V}
  \field{abstract}{%
  We explore the design space of freehand pointing and clicking interaction
  with very large high resolution displays from a distance. Three techniques
  for gestural pointing and two for clicking are developed and evaluated. In
  addition, we present subtle auditory and visual feedback techniques to
  compensate for the lack of kinesthetic feedback in freehand interaction, and
  to promote learning and use of appropriate postures.%
  }
  \verb{doi}
  \verb 10.1145/1095034.1095041
  \endverb
  \field{isbn}{159593023X}
  \field{pages}{33\bibrangedash 42}
  \field{title}{{Distant freehand pointing and clicking on very large, high
  resolution displays}}
  \verb{url}
  \verb http://dl.acm.org/citation.cfm?id=1095041
  \endverb
  \field{journaltitle}{Proc. 18th Annu. ACM Symp. User Interface Softw.
  Technol.}
  \field{year}{2005}
\endentry

\entry{Wachs2008}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Wachs}{W.}%
     {J}{J}%
     {}{}%
     {}{}}%
    {{}%
     {Stern}{S.}%
     {H}{H}%
     {}{}%
     {}{}}%
    {{}%
     {Edan}{E.}%
     {Y}{Y}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WJ+1}
  \strng{fullhash}{WJSHEY1}
  \field{labelalpha}{Wac+08}
  \field{sortinit}{W}
  \field{title}{{Real-time hand gesture interface for browsing medical images}}
  \verb{url}
  \verb http://www.tandfonline.com/doi/abs/10.1080/1931308X.2008.10644149
  \endverb
  \verb{file}
  \verb :E$\backslash$:/papers/Yamada et al/Imaging science in dentistry/Yamada
  \verb  et al. - 2014 - Use of a gesture user interface as a touchless image n
  \verb avigation system in dental surgery Case series report.pdf:pdf
  \endverb
  \field{journaltitle}{{\ldots} J. Intell. {\ldots}}
  \field{year}{2008}
\endentry

\entry{Wagner2007a}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Wagner}{W.}%
     {Daniel}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmalstieg}{S.}%
     {Dieter}{D.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{WDSD1}
  \strng{fullhash}{WDSD1}
  \field{labelalpha}{WS07}
  \field{sortinit}{W}
  \field{abstract}{%
  In this paper we present ARToolKitPlus, a successor to the popular ARToolKit
  pose tracking library. ARToolKitPlus has been optimized and extended for the
  usage on mobile devices such as smartphones, PDAs and Ultra Mobile PCs
  (UMPCs). We explain the need and specific requirements of pose tracking on
  mobile devices and how we met those requirements. To prove the applicability
  we performed an extensive benchmark series on a braod range of off-the-shelf
  handhelds.%
  }
  \field{booktitle}{Proc. 12th Comput. Vis. Winter Work. CVWW07}
  \verb{doi}
  \verb 10.1.1.157.1879
  \endverb
  \field{pages}{139\bibrangedash 146}
  \field{title}{{ARToolKitPlus for Pose Tracking on Mobile Devices}}
  \verb{url}
  \verb http://www.icg.tu-graz.ac.at/Members/daniel/ARToolKitPlusMobilePoseTrac
  \verb king
  \endverb
  \field{year}{2007}
\endentry

\entry{Zhang2000}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Zhang}{Z.}%
     {Z.}{Z.}%
     {}{}%
     {}{}}%
  }
  \keyw{2d pattern,Absolute conic,Calibration from planes,Camera
  calibration,Closed-form solution,Flexible plane-based calibration,Flexible
  setup,Lens distortion,Maximum likelihood estimation,Projective mapping}
  \strng{namehash}{ZZ1}
  \strng{fullhash}{ZZ1}
  \field{labelalpha}{Zha00}
  \field{sortinit}{Z}
  \field{abstract}{%
  We propose a flexible technique to easily calibrate a camera. It only
  requires the camera to observe a planar pattern shown at a few (at least two)
  different orientations. Either the camera or the planar pattern can be freely
  moved. The motion need not be known. Radial lens distortion is modeled. The
  proposed procedure consists of a closed-form solution, followed by a
  nonlinear refinement based on the maximum likelihood criterion. Both computer
  simulation and real data have been used to test the proposed technique and
  very good results have been obtained. Compared with classical techniques
  which use expensive equipment such as two or three orthogonal planes, the
  proposed technique is easy to use and flexible. It advances 3D computer
  vision one more step from laboratory environments to real world use.%
  }
  \verb{doi}
  \verb 10.1109/34.888718
  \endverb
  \field{isbn}{0162-8828}
  \field{issn}{01628828}
  \field{number}{11}
  \field{pages}{1330\bibrangedash 1334}
  \field{title}{{A flexible new technique for camera calibration}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=888718
  \endverb
  \field{volume}{22}
  \field{journaltitle}{IEEE Trans. Pattern Anal. Mach. Intell.}
  \field{year}{2000}
\endentry

\entry{DeCampos2006}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {{De Campos}}{D.}%
     {Te{\'{o}}filo~E.}{T.~E.}%
     {}{}%
     {}{}}%
    {{}%
     {Cuevas}{C.}%
     {Walterio W~Mayol}{W.~W.~M.}%
     {}{}%
     {}{}}%
    {{}%
     {Murray}{M.}%
     {David~W.}{D.~W.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {English}%
  }
  \list{publisher}{1}{%
    {IEEE}%
  }
  \keyw{3D tracking method,Cameras,Computer
  science,Focusing,Humans,Keyboards,Magnetic heads,Robot vision
  systems,Scholarships,Shape,Wearable sensors,cameras,coarse-to-fine
  method,computational complexity,gesture recognition,hand gesture
  recognition,object detection,shape detection,wearable camera,wearable
  computers,wearable visual sensor}
  \strng{namehash}{DTE+1}
  \strng{fullhash}{DTECWWMMDW1}
  \field{labelalpha}{{De }+06}
  \field{sortinit}{{D}}
  \field{abstract}{%
  Wearable visual sensors provide views of the environment which are rich in
  information about the wearer's location, interactions and intentions. In the
  wearable domain, hand gesture recognition is the natural replacement for
  keyboard input. We describe a framework combining a coarse-to-fine method for
  shape detection and a 3D tracking method that can identify pointing gestures
  and estimate their direction. The low computational complexity of both
  methods allows a real-time implementation that is applied to estimate the
  user's focus of attention and to control fast redirections of gaze of a
  wearable active camera. Experiments have demonstrated a level of robustness
  of this system in long and noisy image sequences%
  }
  \field{booktitle}{Brazilian Symp. Comput. Graph. Image Process.}
  \verb{doi}
  \verb 10.1109/SIBGRAPI.2006.13
  \endverb
  \field{isbn}{0769526861}
  \field{issn}{15301834}
  \field{pages}{179\bibrangedash 186}
  \field{title}{{Directing the attention of a wearable camera by pointing
  gestures}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4027066
  \endverb
  \field{year}{2006}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{DiSerio2013}{article}{}
  \name{author}{3}{}{%
    {{}%
     {{Di Serio}}{D.}%
     {{\'{A}}ngela}{{\'{A}}.}%
     {}{}%
     {}{}}%
    {{}%
     {Ib{\'{a}}{\~{n}}ez}{I.}%
     {Mar{\'{\i}}a~Blanca}{M.~B.}%
     {}{}%
     {}{}}%
    {{}%
     {Kloos}{K.}%
     {Carlos~Delgado}{C.~D.}%
     {}{}%
     {}{}}%
  }
  \keyw{Augmented reality,Middle-school education,Motivation}
  \strng{namehash}{DA+1}
  \strng{fullhash}{DAIMBKCD1}
  \field{labelalpha}{{Di }+13}
  \field{sortinit}{{D}}
  \field{abstract}{%
  In this paper, the authors show that augmented reality technology has a
  positive impact on the motivation of middle-school students. The
  Instructional Materials Motivation Survey (IMMS) (Keller, 2010) based on the
  ARCS motivation model (Keller, 1987a) was used to gather information; it
  considers four motivational factors: attention, relevance, confidence, and
  satisfaction. Motivational factors of attention and satisfaction in an
  augmented-reality-based learning environment were better rated than those
  obtained in a slides-based learning environment. When the impact of the
  augmented reality system was analyzed in isolation, the attention and
  confidence factors were the best rated. The usability study showed that
  although this technology is not mature enough to be used massively in
  education, enthusiasm of middle-school students diminished most of the
  barriers found. © 2012 Published by Elsevier Ltd.%
  }
  \verb{doi}
  \verb 10.1016/j.compedu.2012.03.002
  \endverb
  \field{isbn}{0360-1315}
  \field{issn}{03601315}
  \field{pages}{586\bibrangedash 596}
  \field{title}{{Impact of an augmented reality system on students' motivation
  for a visual art course}}
  \verb{url}
  \verb http://linkinghub.elsevier.com/retrieve/pii/S0360131512000590
  \endverb
  \field{volume}{68}
  \field{journaltitle}{Comput. Educ.}
  \field{year}{2013}
\endentry

\entry{NMC2014}{book}{}
  \name{author}{4}{}{%
    {{}%
     {{Johnson L.}}{J.}%
     {}{}%
     {}{}%
     {}{}}%
    {{}%
     {Becker}{B.}%
     {S~Adams}{S.~A.}%
     {}{}%
     {}{}}%
    {{}%
     {{Estrada V.}}{E.}%
     {}{}%
     {}{}%
     {}{}}%
    {{}%
     {{Freeman A.}}{F.}%
     {}{}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{J+1}
  \strng{fullhash}{JBSAEF1}
  \field{labelalpha}{{Joh}+14}
  \field{sortinit}{{J}}
  \field{abstract}{%
  Vorschau auf Horizon Report 2014 und gute Zusammenfassung von Anne Thillosen,
  e-teaching.org: Umsetzungszeitraum ein Jahr oder weniger: Flipped Classroom
  {\&} Learning Analytics. Umsetzungszeitraum zwei bis drei Jahre: 3D Printing
  {\&} Games and Gamification. Umsetzungszeitraum vier bis f{\"{u}}nf Jahre:
  Quantified Self {\&} Virtual Assistants.%
  }
  \field{booktitle}{Resume}
  \field{isbn}{9780989733595}
  \field{pages}{55\bibrangedash 57}
  \field{title}{{NMC Horizon Report: 2014 Higher Education}}
  \field{year}{2014}
\endentry

\entry{ma2013ismar}{inproceedings}{}
  \name{author}{8}{}{%
    {{}%
     {{Ma Meng}}{M.}%
     {}{}%
     {}{}%
     {}{}}%
    {{}%
     {Fallavollita}{F.}%
     {Pascal}{P.}%
     {}{}%
     {}{}}%
    {{}%
     {Blum}{B.}%
     {Tobias}{T.}%
     {}{}%
     {}{}}%
    {{}%
     {Eck}{E.}%
     {Ulrich}{U.}%
     {}{}%
     {}{}}%
    {{}%
     {Sandor}{S.}%
     {Christian}{C.}%
     {}{}%
     {}{}}%
    {{}%
     {Weidert}{W.}%
     {Simon}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Waschke}{W.}%
     {Jens}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Navab}{N.}%
     {Nassir}{N.}%
     {}{}%
     {}{}}%
  }
  \keyw{Anatomy Learning,Augmented Reality,Kinect}
  \strng{namehash}{M+1}
  \strng{fullhash}{MFPBTEUSCWSWJNN1}
  \field{labelalpha}{{Ma }+13}
  \field{sortinit}{{M}}
  \field{abstract}{%
  Education of anatomy is a challenging but crucial element in educating
  medical professionals, but also for general education of pupils. Our research
  group has previously developed a prototype of an Augmented Reality (AR) magic
  mirror which allows intuitive visualization of realistic anatomical
  information on the user. However, the current overlay is imprecise as the
  magic mirror depends on the skeleton output from Kinect. These imprecisions
  affect the quality of education and learning. Hence, together with clinicians
  we have defined bone landmarks which users can touch easily on their body
  while standing in front of the sensor. We demonstrate that these landmarks
  allow the proper deformation of medical data within the magic mirror and onto
  the human body, resulting in a more precise augmentation.%
  }
  \field{booktitle}{2013 IEEE Int. Symp. Mix. Augment. Real.}
  \verb{doi}
  \verb 10.1109/ISMAR.2013.6671803
  \endverb
  \field{isbn}{978-1-4799-2869-9}
  \field{pages}{277\bibrangedash 278}
  \field{title}{{Kinect for interactive AR anatomy learning}}
  \verb{url}
  \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6671803
  \endverb
  \list{location}{1}{%
    {Adelaide, Australia}%
  }
  \field{year}{2013}
  \warn{\item Invalid format of field 'month'}
\endentry

\lossort
\endlossort

\endinput
